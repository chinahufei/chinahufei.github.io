<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>SparkSql | chinahufei</title><meta name="description" content="SparkSql"><meta name="keywords" content="Jerry,JerryC,blog,安卓博客,安卓,程序员,个人博客,安卓開發,安卓博客,程序員,安卓開發,個人博客,Android"><meta name="author" content="hufei"><meta name="copyright" content="hufei"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://chinahufei.coding.me/bigdata/SparkSql"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="SparkSql"><meta name="twitter:description" content="SparkSql"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png"><meta property="og:type" content="website"><meta property="og:title" content="SparkSql"><meta property="og:url" content="http://chinahufei.coding.me/bigdata/SparkSql"><meta property="og:site_name" content="chinahufei"><meta property="og:description" content="SparkSql"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preload" href="https://i.loli.net/2019/05/22/5ce53eb6dc82757840.jpg" as="image"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"http://chinahufei.com","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: undefined,
  copy_copyright_js: false
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><div id="header"> <div id="page-header"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">chinahufei</a></span><i class="fa fa-bars fa-fw toggle-menu pull-right close" aria-hidden="true"></i><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw fa fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">24</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">14</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw fa fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></div></div><div id="body-wrap"><nav class="not_index_bg" id="nav"><div class="nav_bg" style="background-image: url(https://i.loli.net/2019/05/22/5ce53eb6dc82757840.jpg)"></div><div id="page_site-info"><div id="site-title"><span class="blogtitle">SparkSql</span></div></div></nav><div id="content-outer"><div class="layout_page" id="content-inner"><article id="page"><h1>SparkSql</h1><div class="article-container"><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ul>
<li>DataSet: </li>
<li>DataFrame: 等同于数据库中的一张表</li>
<li>SparkSession: 底层还是SparkContext</li>
</ul>
<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p><img alt data-src="./images/sql_layout.jpg" class="lozad"></p>
<h1 id="SQL执行流程"><a href="#SQL执行流程" class="headerlink" title="SQL执行流程"></a>SQL执行流程</h1><p>–files/–jars 不能清空，需要改Spark Core Spark SQL源码<br>SQL字符串 ==&gt; SQL parser 解析 ==&gt; 生成逻辑执行计划 ==&gt; 优化之后的逻辑执行计划 ==&gt; 物理执行计划 ==&gt; 优化之后的物理执行计划</p>
<ul>
<li>spark shell使用sql 需要使用–jars传入mysql jdbc包<br>spark-shell –master local[2] –jars mysql-jdbc.jar</li>
<li>spark sql使用需要多传入一个–driver-class-path 引入mysql jdbc包,其他数据库也是一样<br>spark-sql –jars mysql-jdbc.jar –driver-class-path mysql-jdbc.jar</li>
<li>RDD中的cache是lazy的，Spark SQL中的cache是eager的</li>
</ul>
<h1 id="SparkSql读取数据"><a href="#SparkSql读取数据" class="headerlink" title="SparkSql读取数据"></a>SparkSql读取数据</h1><ul>
<li>spark.read.format(“json).load(“path”)</li>
<li>spark.read.textFile(“path”) // 结果是DataSet类型</li>
<li>spark.read.json(“path”)</li>
<li>df.show() 默认是20条数据，可以传数量。.show(flase) 对于数据不截取，默认长度是20</li>
</ul>
<h3 id="读取数据的三种方式：DataFrameReader"><a href="#读取数据的三种方式：DataFrameReader" class="headerlink" title="读取数据的三种方式：DataFrameReader"></a>读取数据的三种方式：DataFrameReader</h3><p>df.show()<br>df.show(100)<br>df.show(100, false)<br>df.select(df(“name”), df(“age”), df(“gender”))<br>df.select($”name”, $”age”).show()  // 需要引入隐式转换 import spark.implicits._</p>
<h3 id="读取复杂数据类型"><a href="#读取复杂数据类型" class="headerlink" title="读取复杂数据类型"></a>读取复杂数据类型</h3><p>df.select($”name”, $”info.desc”, $”info.sex”.as(“gender”))</p>
<h3 id="RDD和DF、DS之间的转换"><a href="#RDD和DF、DS之间的转换" class="headerlink" title="RDD和DF、DS之间的转换"></a>RDD和DF、DS之间的转换</h3><p>df.rdd<br>ds.toDF</p>
<h3 id="过滤"><a href="#过滤" class="headerlink" title="过滤"></a>过滤</h3><ul>
<li>filter<br>df.filter(“name=’hufei’”).show()<br>df.filter(df(“name”) === “hufei”).show()<br>df.filter(‘name === “hufei”).show()</li>
<li>where  进去还是filter</li>
</ul>
<h3 id="写出去-DataFrameWriter"><a href="#写出去-DataFrameWriter" class="headerlink" title="写出去: DataFrameWriter"></a>写出去: DataFrameWriter</h3><p>SaveMode // 枚举类型，Append, Overwrite<br>df.write.format(“json”).save(“path”)</p>
<h1 id="不同数据源读写"><a href="#不同数据源读写" class="headerlink" title="不同数据源读写"></a>不同数据源读写</h1><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">json</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"ruozedata-spark-sql/data/access.json"</span>)</span><br><span class="line">    val df2: DataFrame = spark.read.json(<span class="string">"ruozedata-spark-sql/data/access.json"</span>)</span><br><span class="line">    <span class="comment">//    df.printSchema()</span></span><br><span class="line">    <span class="comment">// select a,b,c from xxx</span></span><br><span class="line">    <span class="comment">//    df.select("appId","platform","traffic","user").show(false)</span></span><br><span class="line">    <span class="comment">//    df.select(df("appId"),df("platform"),df("traffic"),df("user")).show(false)</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df3 = df.select($<span class="string">"appId"</span>,$<span class="string">"platform"</span>,$<span class="string">"traffic"</span>,$<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">//    df3.show(false)</span></span><br><span class="line">    <span class="comment">//    df3.filter(df("user") === "ruozedata62").show()</span></span><br><span class="line">    <span class="comment">//    df3.filter('user === "ruozedata62").show()</span></span><br><span class="line">    val df4 = df3.filter(<span class="string">"user = 'ruozedata62'"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//df4.write.format("json").mode(SaveMode.Overwrite).save("out")</span></span><br><span class="line"></span><br><span class="line">    val df5 = spark.read.json(<span class="string">"ruozedata-spark-sql/data/per.json"</span>)</span><br><span class="line">    df5.printSchema()</span><br><span class="line">    df5.select($<span class="string">"name"</span>,$<span class="string">"age"</span>, $<span class="string">"info.home"</span>.as(<span class="string">"info-home"</span>), $<span class="string">"info.work"</span>.as(<span class="string">"info-work"</span>)).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">text</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df: DataFrame = spark.read.format(<span class="string">"text"</span>).load(<span class="string">"ruozedata-spark-sql/data/people.txt"</span>)</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(<span class="string">"ruozedata-spark-sql/data/people.txt"</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    val ds2 = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(<span class="string">","</span>)</span><br><span class="line">      (splits(<span class="number">0</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ds2.write.format(<span class="string">"text"</span>)</span><br><span class="line">      .option(<span class="string">"compression"</span>,<span class="string">"gzip"</span>)</span><br><span class="line">      .mode(<span class="string">"overwrite"</span>).save(<span class="string">"out"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//    df.map(x =&gt; &#123;</span></span><br><span class="line"><span class="comment">//      val splits = x.getString(0).split(",")</span></span><br><span class="line"><span class="comment">//      (splits(0), splits(1))</span></span><br><span class="line"><span class="comment">//    &#125;).show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    df.rdd.map(x =&gt; &#123;</span></span><br><span class="line"><span class="comment">//      val splits = x.getString(0).split(",")</span></span><br><span class="line"><span class="comment">//      (splits(0), splits(1))</span></span><br><span class="line"><span class="comment">//    &#125;).foreach(println)</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">csv</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>,<span class="string">";"</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">      .load(<span class="string">"ruozedata-spark-sql/data/people.csv"</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><ul>
<li>硬编码<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">jdbc</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://ruozedata001:3306"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"offsets.offsets_storage"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"ruozedata"</span>)</span><br><span class="line">      .load()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.filter(<span class="string">'partitions === 0)</span></span><br><span class="line"><span class="string">      .write.format("jdbc")</span></span><br><span class="line"><span class="string">      .option("url", "jdbc:mysql://ruozedata001:3306")</span></span><br><span class="line"><span class="string">      .option("dbtable", "offsets.offsets_storage_2")</span></span><br><span class="line"><span class="string">      .option("user", "root")</span></span><br><span class="line"><span class="string">      .option("password", "ruozedata")</span></span><br><span class="line"><span class="string">      .save()</span></span><br><span class="line"><span class="string">  &#125;</span></span><br></pre></td></tr></table></figure></li>
<li>写入配置文件<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">jdbc02</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val config = ConfigFactory.load()</span><br><span class="line">    val url = config.getString(<span class="string">"db.default.url"</span>)</span><br><span class="line">    val user = config.getString(<span class="string">"db.default.user"</span>)</span><br><span class="line">    val password = config.getString(<span class="string">"db.default.password"</span>)</span><br><span class="line">    val srctable = config.getString(<span class="string">"db.default.srctable"</span>)</span><br><span class="line">    val targettable = config.getString(<span class="string">"db.default.targettable"</span>)</span><br><span class="line">    val driver = config.getString(<span class="string">"db.default.driver"</span>)</span><br><span class="line">    val database = config.getString(<span class="string">"db.default.database"</span>)</span><br><span class="line">    val df = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, url)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, database+<span class="string">"."</span>+srctable)</span><br><span class="line">      .option(<span class="string">"user"</span>, user)</span><br><span class="line">      .option(<span class="string">"password"</span>, password)</span><br><span class="line">      .option(<span class="string">"driver"</span>, driver)</span><br><span class="line">      .load()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.filter(<span class="string">'partitions === 0)</span></span><br><span class="line"><span class="string">      .write.format("jdbc")</span></span><br><span class="line"><span class="string">      .option("url", url)</span></span><br><span class="line"><span class="string">      .option("dbtable", database+"."+targettable)</span></span><br><span class="line"><span class="string">      .option("user", user)</span></span><br><span class="line"><span class="string">      .option("password", password)</span></span><br><span class="line"><span class="string">      .option("driver", driver)</span></span><br><span class="line"><span class="string">      .save()</span></span><br><span class="line"><span class="string">  &#125;</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="SparkSql做统计分析"><a href="#SparkSql做统计分析" class="headerlink" title="SparkSql做统计分析"></a>SparkSql做统计分析</h1><h3 id="写sql"><a href="#写sql" class="headerlink" title="写sql"></a>写sql</h3><h3 id="API实现"><a href="#API实现" class="headerlink" title="API实现"></a>API实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .enableHiveSupport() <span class="comment">//开启HiveContext</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df = spark.read.textFile(<span class="string">"ruozedata-spark-sql/data/access.log"</span>)</span><br><span class="line">        .map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(<span class="string">"\t"</span>)</span><br><span class="line">          val platform = splits(<span class="number">1</span>)</span><br><span class="line">          val traffic = splits(<span class="number">6</span>).toLong</span><br><span class="line">          val province = splits(<span class="number">8</span>)</span><br><span class="line">          val city = splits(<span class="number">9</span>)</span><br><span class="line">          val isp = splits(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// TODO... 清洗操作</span></span><br><span class="line">          (platform,traffic,province,city,isp)</span><br><span class="line">        &#125;).toDF(<span class="string">"platform"</span>,<span class="string">"traffic"</span>,<span class="string">"province"</span>,<span class="string">"city"</span>,<span class="string">"isp"</span>)</span><br><span class="line"></span><br><span class="line">    df.write.format(<span class="string">"orc"</span>)</span><br><span class="line">      .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">        .save(<span class="string">"out"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//    df.createOrReplaceTempView("log")</span></span><br><span class="line"><span class="comment">//    spark.sql("select platform,province,city,sum(traffic) as traffics from log group by platform,province,city order by traffics desc")</span></span><br><span class="line"><span class="comment">//        .show(false)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    import org.apache.spark.sql.functions._</span></span><br><span class="line"><span class="comment">//    df.groupBy("platform","province","city")</span></span><br><span class="line"><span class="comment">//        .agg(sum("traffic").as("traffics"))</span></span><br><span class="line"><span class="comment">//        .sort('traffics.desc)</span></span><br><span class="line"><span class="comment">//        .show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO... 按照platform分组，province访问次数最多的TopN</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    val topNSQL =</span></span><br><span class="line"><span class="comment">//      """</span></span><br><span class="line"><span class="comment">//        |select * from</span></span><br><span class="line"><span class="comment">//        |(</span></span><br><span class="line"><span class="comment">//        |select t.*,row_number() over(partition by platform order by cnt desc) as r</span></span><br><span class="line"><span class="comment">//        |from</span></span><br><span class="line"><span class="comment">//        |(select platform,province,city,count(1) cnt from log group by platform,province,city) t</span></span><br><span class="line"><span class="comment">//        |) a where a.r&lt;=3</span></span><br><span class="line"><span class="comment">//        |""".stripMargin</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql(topNSQL).show()</span></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>


<h1 id="spark-压缩方式"><a href="#spark-压缩方式" class="headerlink" title="spark 压缩方式"></a>spark 压缩方式</h1><h1 id="工作中文本处理方式"><a href="#工作中文本处理方式" class="headerlink" title="工作中文本处理方式"></a>工作中文本处理方式</h1><h3 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h3><p>DF ==&gt; rdd     spark.sparkContext.textFile(“”)<br>rdd ==&gt; DF     rdd.toDF    // import spark.imlicits._  导入隐式转换<br>案例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       reflection(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用反射方式将RDD转成DF</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">reflection</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">// RDD=&gt;DF时需要的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建RDD</span></span><br><span class="line">    val rdd = spark.sparkContext.textFile(<span class="string">"ruozedata-spark-sql/data/info.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD[String] ==&gt; case class</span></span><br><span class="line">    val infoDF = rdd.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(<span class="string">","</span>)</span><br><span class="line">      val id = splits(<span class="number">0</span>).trim.toInt</span><br><span class="line">      val name = splits(<span class="number">1</span>).trim</span><br><span class="line">      val age = splits(<span class="number">2</span>).trim.toInt</span><br><span class="line">      Info(id, name, age)</span><br><span class="line">    &#125;).toDF() <span class="comment">// 最终转成DF</span></span><br><span class="line"></span><br><span class="line">    infoDF.printSchema()</span><br><span class="line">    infoDF.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Info</span><span class="params">(id: Int, name: String, age: Int)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="Programmatically-如果要和外部数据源对接，需要用这种"><a href="#Programmatically-如果要和外部数据源对接，需要用这种" class="headerlink" title="Programmatically(如果要和外部数据源对接，需要用这种)"></a>Programmatically(如果要和外部数据源对接，需要用这种)</h3><ul>
<li>Create an RDD of Rows from the original RDD;</li>
<li>Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.</li>
<li>Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.<br>案例：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    programmatically(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用编程方式</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">programmatically</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">// RDD=&gt;DF时需要的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 创建RDD</span></span><br><span class="line">    val rdd = spark.sparkContext.textFile(<span class="string">"ruozedata-spark-sql/data/info.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP1: RDD[String] ==&gt; RDD[Row]</span></span><br><span class="line">    val infoRDD: RDD[Row] = rdd.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(<span class="string">","</span>)</span><br><span class="line">      val id = splits(<span class="number">0</span>).trim.toInt</span><br><span class="line">      val name = splits(<span class="number">1</span>).trim</span><br><span class="line">      val age = splits(<span class="number">2</span>).trim.toInt</span><br><span class="line">      Row(id, name, age)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP2: schema</span></span><br><span class="line">    val schema = StructType(</span><br><span class="line">      StructField(<span class="string">"id"</span>, IntegerType, <span class="keyword">true</span>) ::</span><br><span class="line">        StructField(<span class="string">"name"</span>, StringType, <span class="keyword">false</span>) ::</span><br><span class="line">        StructField(<span class="string">"age"</span>, IntegerType, <span class="keyword">false</span>) :: Nil)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// STEP3: createDataFrame</span></span><br><span class="line">    val df = spark.createDataFrame(infoRDD, schema)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Info</span><span class="params">(id: Int, name: String, age: Int)</span></span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="spark-read-json-使用sql方式"><a href="#spark-read-json-使用sql方式" class="headerlink" title="spark.read.json()使用sql方式"></a>spark.read.json()使用sql方式</h1><p>查看官网处的sql</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">VIEW</span> jsonTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.json</span><br><span class="line">OPTIONS (</span><br><span class="line">  <span class="keyword">path</span> <span class="string">"file:///home/hadoop/app/spark-2.4.5-bin-2.6.0-cdh5.16.2/examples/src/main/resources/people.json"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> jsonTable</span><br></pre></td></tr></table></figure>

<h1 id="跨数据源做join"><a href="#跨数据源做join" class="headerlink" title="跨数据源做join"></a>跨数据源做join</h1><p>hive: tab1DF<br>mysql: tab2DF<br>tab1DF.join(tab2DF, tab1DF.get(“id”) === table2DF.get(“2”))</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val mysqlDF = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost:3306"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"sqoop.dept"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"ruozedata"</span>)</span><br><span class="line">.load().show</span><br><span class="line">val hiveDF = spark.sql(<span class="string">"select * from ruozedata_hive.emp"</span>)</span><br><span class="line">mysqlDF.join(hiveDF).show()</span><br></pre></td></tr></table></figure>

<h1 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h1><ul>
<li>导入数据，默认是向上兼容，比如整数和double会取Double</li>
<li>导入数据，Json里有个Mode：PermissiveMode(接受), DroMalformedMode(丢掉), FailFasterMode(报错), logWarning(警告)<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .load(<span class="string">"ruozedata-spark-sql/data/sales.csv"</span>)</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[Sales] = df.as[Sales]</span><br><span class="line"></span><br><span class="line">    val selectDF = df.select(<span class="string">"customerId"</span>)</span><br><span class="line">    val selectDS = ds.map(_.customerId)</span><br><span class="line">println(selectDF.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line">println(<span class="string">"....................."</span>)</span><br><span class="line">    println(selectDS.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Sales</span><span class="params">(transactionId:Int, customerId:Int, itemId:Int, amountPaid:Double)</span></span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="spark-catalog"><a href="#spark-catalog" class="headerlink" title="spark catalog"></a>spark catalog</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val catalog = spark.catalog</span><br><span class="line"></span><br><span class="line">    catalog.listDatabases().show(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">    catalog.listDatabases().map(_.name).show</span><br><span class="line">    catalog.listTables(<span class="string">"ruozedata_hive"</span>).filter(<span class="string">'name.contains("emp")).show</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    catalog.listTables("ruozedata_hive").printSchema()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    catalog.isCached("ruozedata_hive.emp")</span></span><br><span class="line"><span class="string">    catalog.cacheTable("ruozedata_hive.emp")</span></span><br><span class="line"><span class="string">    catalog.uncacheTable("ruozedata_hive.emp")</span></span><br><span class="line"><span class="string">    catalog.listFunctions().filter('</span>name === <span class="string">"str_length"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">"str_length"</span>,(str:String)=&gt;str.length)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="UDF、UDAF、UDTF"><a href="#UDF、UDAF、UDTF" class="headerlink" title="UDF、UDAF、UDTF"></a>UDF、UDAF、UDTF</h1><h3 id="UDF-一进一出"><a href="#UDF-一进一出" class="headerlink" title="UDF 一进一出"></a>UDF 一进一出</h3><p>案例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession.builder</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val df = spark.sparkContext.textFile(<span class="string">"ruozedata-spark-sql/data/likes.txt"</span>)</span><br><span class="line">      .map(_.split(<span class="string">"\t"</span>))</span><br><span class="line">      .map(x =&gt; Likes(x(<span class="number">0</span>), x(<span class="number">1</span>)))</span><br><span class="line">      .toDF</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"teams"</span>)</span><br><span class="line"></span><br><span class="line">    val teamsLengthUDF = spark.udf.register(<span class="string">"teams_length"</span>,(input:String) =&gt;&#123;</span><br><span class="line">      input.split(<span class="string">","</span>).length</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//spark.sql("select name,teams,teams_length(teams) as teams_length from teams").show(false)</span></span><br><span class="line"></span><br><span class="line">    df.select($<span class="string">"name"</span>,$<span class="string">"teams"</span>,teamsLengthUDF($<span class="string">"teams"</span>)).show(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Likes</span><span class="params">(name: String, teams: String)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="UDAF-多进一出"><a href="#UDAF-多进一出" class="headerlink" title="UDAF 多进一出"></a>UDAF 多进一出</h3><p>extends UserDefinedAggrageFunction{}  // 实现这个函数<br>案例：</p>
<h3 id="UDTF-一进多出"><a href="#UDTF-一进多出" class="headerlink" title="UDTF 一进多出"></a>UDTF 一进多出</h3><p>案例：</p>
<h3 id="自定义数据源-1"><a href="#自定义数据源-1" class="headerlink" title="自定义数据源"></a>自定义数据源</h3></div><nav id="pagination"><div class="pagination"></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'Yc8acyRz91wT0MAUadFgxTfR-gzGzoHsz',
  appKey:'hawGYUTQCFtkdVF1Uu8toBWU',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></article><div class="aside_content" id="aside_content"><div class="card_widget card-author"><div class="card-content"><div class="post_data"><div class="data-item is_center"><img class="lozad avatar_img" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"><p class="author-info__name is_center">hufei</p><p class="author-info__description is_center">Amazing Site</p></div></div><div class="post_data data_config"><div class="data-item is_center"><div class="data_link"><a href="/archives/"><p class="headline">文章</p><p class="length_num">24</p></a></div></div><div class="data-item is_center">      <div class="data_link"><a href="/tags/"><p class="headline">标签</p><p class="length_num">16</p></a></div></div><div class="data-item is_center">     <div class="data_link"><a href="/categories/"><p class="headline">分类</p><p class="length_num">14</p></a></div></div></div><div class="post_data is_center"><a class="data-item bookmark bookmarke--primary bookmark--animated" id="bookmark-it" href="javascript:;" title="加入书签"><i class="fa fa-bookmark" aria-hidden="true"></i><span>加入书签</span></a></div><div class="post_data data_config"><div id="aside-social-icons"> <a class="social-icon data-item" href="https://gitee.com/chinahufei" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon data-item" href="/atom.xml" target="_blank"><i class="fa fa-envelope-o"></i></a></div></div></div></div><div class="card_widget card-announcement"><div class="card-content"><div class="item_headline"><i class="fa fa-bullhorn card-announcement-animation" aria-hidden="true"></i><span>公告</span></div><div class="announcement_content">欢迎光临本站，若喜欢请收藏。</div></div></div><div class="card_widget card-recent-post"><div class="card-content"><div class="item_headline"><i class="fa fa-history" aria-hidden="true"></i><span>最新文章</span></div><div class="aside_recent_item"><div class="aside_recent_post"><a href="/2020/03/12/Spark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"><div class="aside_post_cover"><img class="aside_post_bg lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" title="Spark源码编译"></div><div id="aside_title"><div class="aside_post_title" href="/2020/03/12/Spark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/" title="Spark源码编译">Spark源码编译</div><time class="aside_post_meta post-meta__date">2020-03-12</time></div></a></div><div class="aside_recent_post"><a href="/2020/03/11/HDFS%E6%9E%B6%E6%9E%84%E5%92%8CYarn%E6%9E%B6%E6%9E%84/"><div class="aside_post_cover"><img class="aside_post_bg lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" title="HDFS架构和Yarn架构"></div><div id="aside_title"><div class="aside_post_title" href="/2020/03/11/HDFS%E6%9E%B6%E6%9E%84%E5%92%8CYarn%E6%9E%B6%E6%9E%84/" title="HDFS架构和Yarn架构">HDFS架构和Yarn架构</div><time class="aside_post_meta post-meta__date">2020-03-11</time></div></a></div><div class="aside_recent_post"><a href="/2020/03/09/HUE%E9%9B%86%E6%88%90Hadoop%E3%80%81Hive%E3%80%81Yarn/"><div class="aside_post_cover"><img class="aside_post_bg lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" title="HUE集成Hadoop、Hive、Yarn"></div><div id="aside_title"><div class="aside_post_title" href="/2020/03/09/HUE%E9%9B%86%E6%88%90Hadoop%E3%80%81Hive%E3%80%81Yarn/" title="HUE集成Hadoop、Hive、Yarn">HUE集成Hadoop、Hive、Yarn</div><time class="aside_post_meta post-meta__date">2020-03-09</time></div></a></div><div class="aside_recent_post"><a href="/2020/03/09/Sqoop%E4%BD%BF%E7%94%A8/"><div class="aside_post_cover"><img class="aside_post_bg lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" title="Sqoop使用"></div><div id="aside_title"><div class="aside_post_title" href="/2020/03/09/Sqoop%E4%BD%BF%E7%94%A8/" title="Sqoop使用">Sqoop使用</div><time class="aside_post_meta post-meta__date">2020-03-09</time></div></a></div><div class="aside_recent_post"><a href="/2020/03/07/Linux%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/"><div class="aside_post_cover"><img class="aside_post_bg lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" title="Linux大数据开发环境及工具配置"></div><div id="aside_title"><div class="aside_post_title" href="/2020/03/07/Linux%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/" title="Linux大数据开发环境及工具配置">Linux大数据开发环境及工具配置</div><time class="aside_post_meta post-meta__date">2020-03-07</time></div></a></div></div></div></div><div class="card_widget card-categories"><div class="card-content"><div class="item_headline"><i class="fa fa-folder-open" aria-hidden="true"></i><span>分类</span></div><ul class="aside_category_item">       <li class="aside_category_list"><a class="aside_category_list_link" href="/categories/CDH/"><span class="aside_category_list_name">CDH</span><span class="aside_category_list_length">4</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Flink/"><span class="aside_category_list_name">Flink</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Flume/"><span class="aside_category_list_name">Flume</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/HUE/"><span class="aside_category_list_name">HUE</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Hadoop/"><span class="aside_category_list_name">Hadoop</span><span class="aside_category_list_length">3</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Hive/"><span class="aside_category_list_name">Hive</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Java/"><span class="aside_category_list_name">Java</span><span class="aside_category_list_length">2</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Linux-Shell/"><span class="aside_category_list_name">Linux&amp;Shell</span><span class="aside_category_list_length">2</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/MapReduce/"><span class="aside_category_list_name">MapReduce</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/MySQL/"><span class="aside_category_list_name">MySQL</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/PHP/"><span class="aside_category_list_name">PHP</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Spark/"><span class="aside_category_list_name">Spark</span><span class="aside_category_list_length">2</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/Sqoop/"><span class="aside_category_list_name">Sqoop</span><span class="aside_category_list_length">1</span></a></li><li class="aside_category_list"><a class="aside_category_list_link" href="/categories/%E5%B7%A5%E5%85%B7/"><span class="aside_category_list_name">工具</span><span class="aside_category_list_length">3</span></a></li></ul></div></div><div class="card_widget card-tags"><div class="card-content"><div class="item_headline"><i class="fa fa-tags" aria-hidden="true"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/CDH/" style="font-size: 24px; color: #000">CDH</a> <a href="/tags/Flink/" style="font-size: 16px; color: #999">Flink</a> <a href="/tags/Flume/" style="font-size: 16px; color: #999">Flume</a> <a href="/tags/HUE/" style="font-size: 16px; color: #999">HUE</a> <a href="/tags/Hadoop/" style="font-size: 21.33px; color: #333">Hadoop</a> <a href="/tags/Hive/" style="font-size: 16px; color: #999">Hive</a> <a href="/tags/Java/" style="font-size: 18.67px; color: #666">Java</a> <a href="/tags/Linux/" style="font-size: 16px; color: #999">Linux</a> <a href="/tags/MapReduce/" style="font-size: 16px; color: #999">MapReduce</a> <a href="/tags/MySQL/" style="font-size: 16px; color: #999">MySQL</a> <a href="/tags/PHP/" style="font-size: 16px; color: #999">PHP</a> <a href="/tags/Shell/" style="font-size: 16px; color: #999">Shell</a> <a href="/tags/Spark/" style="font-size: 18.67px; color: #666">Spark</a> <a href="/tags/Sqoop/" style="font-size: 16px; color: #999">Sqoop</a> <a href="/tags/shell/" style="font-size: 16px; color: #999">shell</a> <a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 21.33px; color: #333">工具</a></div></div></div><div class="card_widget card-archives"><div class="card-content"><div class="item_headline"><i class="fa fa-archive" aria-hidden="true"></i><span>归档</span></div><div class="archives_item"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">2020年03月<span class="archive-list-count">5</span></a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">2020年02月<span class="archive-list-count">3</span></a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">2019年12月<span class="archive-list-count">11</span></a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">2019年11月<span class="archive-list-count">1</span></a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">2019年01月<span class="archive-list-count">4</span></a></li></ul></div></div></div><div class="card_widget card-webinfo"><div class="card-content"><div class="item_headline"><i class="fa fa-line-chart" aria-hidden="true"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo_item"><div class="webinfo_article_name">文章数目 :</div><div class="webinfo_article_count">24</div></div><div class="webinfo_item"><div class="webinfo_runtime_name">已运行时间 :</div><div class="webinfo_runtime_count" id="webinfo_runtime_count"></div><script id="runtionshow" src="/js/runtimeshow.js" start_date="10/10/2016 00:00:00">      </script></div><div class="webinfo_item">      <div class="webinfo_site_uv_name">本站访客数 :</div><div class="webinfo_site_uv_count" id="busuanzi_value_site_uv"></div></div><div class="webinfo_item"><div class="webinfo_site_name">本站总访问量 :</div><div class="webinfo_site_pv_count" id="busuanzi_value_site_pv"></div></div></div></div></div></div></div></div><footer><div id="footer"><div class="copyright">&copy;2016 - 2020 By hufei</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script async src="/js/search/local-search.js"></script><script src="/js/nightshift.js"></script><script color="0,0,255" opacity="0.7" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/js/canvas-nest.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();
</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>