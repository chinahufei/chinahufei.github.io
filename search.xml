<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark源码编译</title>
      <link href="/2020/03/12/Spark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
      <url>/2020/03/12/Spark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文讲述如何编译 hadoop-2.6.0-cdh5.12.0.tar.gz, hadoop版本为2.6.0, cdh版本为5.12.0, spark版本为2.4.5</p></blockquote><h1 id="官网下载2-4-5的Source-Code"><a href="#官网下载2-4-5的Source-Code" class="headerlink" title="官网下载2.4.5的Source Code"></a>官网下载2.4.5的Source Code</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;spark.apache.org&#x2F;downloads.html</span><br></pre></td></tr></table></figure><h1 id="解压源码包"><a href="#解压源码包" class="headerlink" title="解压源码包"></a>解压源码包</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf tar -zxvf spark-2.4.5.tgz</span><br><span class="line">http:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;building-spark.html</span><br></pre></td></tr></table></figure><h1 id="配置编译参数"><a href="#配置编译参数" class="headerlink" title="配置编译参数"></a>配置编译参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;dev&#x2F;make-distribution.sh --name 2.6.0-cdh5.12.0 --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-2.12 -Phadoop-2.6 -Dhadoop.version&#x3D;2.6.0-cdh5.12.0</span><br><span class="line"></span><br><span class="line">--name 2.6.0-cdh5.12.0 编译后的包名后缀</span><br><span class="line">--tgz 使用压缩</span><br><span class="line">-Pyarn -Phive -Phive-thriftserver 使用yarn,hive,hive-thriftserver</span><br><span class="line">-Pscala-2.12 指定scala版本</span><br><span class="line">-Phadoop-2.6 指定hadoop</span><br><span class="line">-Dhadoop.version&#x3D;2.6.0-cdh5.12.0 指定hadoop版本</span><br></pre></td></tr></table></figure><h1 id="修改部分make-distribution-sh代码"><a href="#修改部分make-distribution-sh代码" class="headerlink" title="修改部分make-distribution.sh代码"></a>修改部分make-distribution.sh代码</h1><ul><li>指定版本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">VERSION&#x3D;2.4.5</span><br><span class="line">SCALA_VERSION&#x3D;2.12.0</span><br><span class="line">SPARK_HADOOP_VERSION&#x3D;2.6.0-cdh5.12.0</span><br><span class="line">SPARK_HIVE&#x3D;1.1.0</span><br></pre></td></tr></table></figure></li><li>修改maven仓库地址，因为是cdh版本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;repository&gt;</span><br><span class="line">    &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;repository&gt;</span><br></pre></td></tr></table></figure><h1 id="执行上述shell命令，编译后结果在dist目录"><a href="#执行上述shell命令，编译后结果在dist目录" class="headerlink" title="执行上述shell命令，编译后结果在dist目录"></a>执行上述shell命令，编译后结果在dist目录</h1></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构和Yarn架构</title>
      <link href="/2020/03/11/HDFS%E6%9E%B6%E6%9E%84%E5%92%8CYarn%E6%9E%B6%E6%9E%84/"/>
      <url>/2020/03/11/HDFS%E6%9E%B6%E6%9E%84%E5%92%8CYarn%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h1><h1 id="Yarn架构"><a href="#Yarn架构" class="headerlink" title="Yarn架构"></a>Yarn架构</h1>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUE集成Hadoop、Hive、Yarn</title>
      <link href="/2020/03/09/HUE%E9%9B%86%E6%88%90Hadoop%E3%80%81Hive%E3%80%81Yarn/"/>
      <url>/2020/03/09/HUE%E9%9B%86%E6%88%90Hadoop%E3%80%81Hive%E3%80%81Yarn/</url>
      
        <content type="html"><![CDATA[<h1 id="修改hadoopde的hdfs-site-xml文件"><a href="#修改hadoopde的hdfs-site-xml文件" class="headerlink" title="修改hadoopde的hdfs-site.xml文件"></a>修改hadoopde的hdfs-site.xml文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.permissions.enabled&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h1 id="修改hadoopde的core-site-xml文件"><a href="#修改hadoopde的core-site-xml文件" class="headerlink" title="修改hadoopde的core-site.xml文件"></a>修改hadoopde的core-site.xml文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;hadoop.proxyuser.hue.groups&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h1 id="修改hadoopde的httpfs-site-xml文件"><a href="#修改hadoopde的httpfs-site-xml文件" class="headerlink" title="修改hadoopde的httpfs-site.xml文件"></a>修改hadoopde的httpfs-site.xml文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;httpfs.proxyuser.hue.hosts&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;httpfs.proxyuser.hue.groups&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h1 id="修改hadoopde的yarn-site-xml文件"><a href="#修改hadoopde的yarn-site-xml文件" class="headerlink" title="修改hadoopde的yarn-site.xml文件"></a>修改hadoopde的yarn-site.xml文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">         &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;432000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h1 id="修改hue的pseudo-distributed-ini文件集成hdfs"><a href="#修改hue的pseudo-distributed-ini文件集成hdfs" class="headerlink" title="修改hue的pseudo-distributed.ini文件集成hdfs"></a>修改hue的pseudo-distributed.ini文件集成hdfs</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改hue的&#x2F;desktop&#x2F;conf&#x2F;目录下的pseudo-distributed.ini文件，对hadoop集群的hdfs配置如下：</span><br><span class="line">fs_defaultfs&#x3D;hdfs:&#x2F;&#x2F;hdpc01:9000</span><br><span class="line">webhdfs_url&#x3D;http:&#x2F;&#x2F;hdpc01:50070&#x2F;webhdfs&#x2F;v1</span><br><span class="line">hadoop_conf_dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.8.2&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure><h1 id="修改hue的pseudo-distributed-ini文件集成yarn"><a href="#修改hue的pseudo-distributed-ini文件集成yarn" class="headerlink" title="修改hue的pseudo-distributed.ini文件集成yarn"></a>修改hue的pseudo-distributed.ini文件集成yarn</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">修改hue的&#x2F;desktop&#x2F;conf&#x2F;目录下的pseudo-distributed.ini文件，对hadoop集群的yarn配置如下：  </span><br><span class="line">resourcemanager_host&#x3D;hdpc01</span><br><span class="line">resourcemanager_port&#x3D;8032</span><br><span class="line">submit_to&#x3D;True</span><br><span class="line">resourcemanager_api_url&#x3D;http:&#x2F;&#x2F;hdpc01:8088</span><br><span class="line">proxy_api_url&#x3D;http:&#x2F;&#x2F;hdpc01:8088</span><br><span class="line">history_server_api_url&#x3D;http:&#x2F;&#x2F;hdpc01:19888</span><br></pre></td></tr></table></figure><h1 id="启动hadoop集群"><a href="#启动hadoop集群" class="headerlink" title="启动hadoop集群"></a>启动hadoop集群</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在主节点上启动hadoop集群start-all.sh</span><br></pre></td></tr></table></figure><h1 id="启动Hue服务"><a href="#启动Hue服务" class="headerlink" title="启动Hue服务"></a>启动Hue服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在hue的&#x2F;bulid&#x2F;env&#x2F;bin&#x2F;目录下 .&#x2F;supervisor 启动hue服务</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HUE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HUE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop使用</title>
      <link href="/2020/03/09/Sqoop%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/03/09/Sqoop%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="测试连接"><a href="#测试连接" class="headerlink" title="测试连接"></a>测试连接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;sqoop list-databases --connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F; --username root --password yourpwd</span><br></pre></td></tr></table></figure><h1 id="全部导入-mysql–-gt-hdfs"><a href="#全部导入-mysql–-gt-hdfs" class="headerlink" title="全部导入(mysql–&gt;hdfs)"></a>全部导入(mysql–&gt;hdfs)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;hadoop \</span><br><span class="line">--username root \</span><br><span class="line">--password yourpwd \</span><br><span class="line">--table student \</span><br><span class="line">--target-dir &#x2F;user&#x2F;hufei&#x2F;sqoop01 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure><h1 id="查询导入"><a href="#查询导入" class="headerlink" title="查询导入"></a>查询导入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;hadoop \</span><br><span class="line">--username root \</span><br><span class="line">--password yourpwd \</span><br><span class="line">--target-dir &#x2F;user&#x2F;hufei&#x2F;sqoop02 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--query &#39;select sid,sname,sage,ssex from student where sid &#x3D; &quot;08&quot; and $CONDITIONS;&#39;</span><br></pre></td></tr></table></figure><ul><li>must contain ‘$CONDITIONS’ in WHERE clause.</li><li>如果 query 后使用的是双引号，则$CONDITIONS 前必须加转移符，防止 shell 识别为自己的变量。</li></ul><h1 id="导入指定列"><a href="#导入指定列" class="headerlink" title="导入指定列"></a>导入指定列</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;hadoop \</span><br><span class="line">--username root \</span><br><span class="line">--password yourpwd \</span><br><span class="line">--target-dir &#x2F;user&#x2F;hufei&#x2F;sqoop03 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--columns sid,sname \</span><br><span class="line">--table student</span><br></pre></td></tr></table></figure><ul><li>columns 中如果涉及到多列，用逗号分隔，分隔时不要添加空格</li></ul><h1 id="关键字筛选查询导入数据"><a href="#关键字筛选查询导入数据" class="headerlink" title="关键字筛选查询导入数据"></a>关键字筛选查询导入数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir &#x2F;user&#x2F;emp2  \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--table dept \</span><br><span class="line">--where &quot;id&#x3D;1&quot;</span><br></pre></td></tr></table></figure><h1 id="RDBMS到HIVE"><a href="#RDBMS到HIVE" class="headerlink" title="RDBMS到HIVE"></a>RDBMS到HIVE</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table dept \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table dept_hive</span><br></pre></td></tr></table></figure><ul><li>执行这个操作之前需要把将hive/lib中的hive-common-2.3.3.jar拷贝到sqoop的lib目录中</li></ul><h1 id="从RDBMS增量导入数据到HIVE中（根据主键）"><a href="#从RDBMS增量导入数据到HIVE中（根据主键）" class="headerlink" title="从RDBMS增量导入数据到HIVE中（根据主键）"></a>从RDBMS增量导入数据到HIVE中（根据主键）</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table emp \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--target-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;taff_hive1 \</span><br><span class="line">--check-column eid \</span><br><span class="line">--incremental append \</span><br><span class="line">--last-value 3</span><br></pre></td></tr></table></figure><ul><li>append不能与–hive-的参数同时使用，并且RDBMS导入到HIVE的过程是：先把RDBMS数据导入到HDFS本地目录下，然后再转移到HIVE中。</li></ul><h1 id="从RDBMS增量导入数据到HIVE中（根据实时时间字段）"><a href="#从RDBMS增量导入数据到HIVE中（根据实时时间字段）" class="headerlink" title="从RDBMS增量导入数据到HIVE中（根据实时时间字段）"></a>从RDBMS增量导入数据到HIVE中（根据实时时间字段）</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table staff_timestamp \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--check-column last_modified \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2018-11-8 21:46:30&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--append</span><br></pre></td></tr></table></figure><h1 id="RDBMS导入数据到Hbase"><a href="#RDBMS导入数据到Hbase" class="headerlink" title="RDBMS导入数据到Hbase"></a>RDBMS导入数据到Hbase</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table company \</span><br><span class="line">--columns &quot;id,name,sex&quot; \</span><br><span class="line">--column-family &quot;info&quot; \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-row-key &quot;id&quot; \</span><br><span class="line">--hbase-table &quot;hbase_company&quot; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--split-by id</span><br></pre></td></tr></table></figure><ul><li>sqoop1.4.6 只支持 HBase1.0.1 之前的版本的自动创建 HBase 表的功能</li></ul><h1 id="HDFS到RDBMS"><a href="#HDFS到RDBMS" class="headerlink" title="HDFS到RDBMS"></a>HDFS到RDBMS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table dept \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;t1&#x2F;part-m-00001 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39;</span><br></pre></td></tr></table></figure><ul><li>一定要清楚以什么为分割导出，不然会导致导出失败</li></ul><h1 id="HIVE到RDBMS"><a href="#HIVE到RDBMS" class="headerlink" title="HIVE到RDBMS"></a>HIVE到RDBMS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306&#x2F;bigdata11 \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table dept \</span><br><span class="line">--hive-table hive_dept</span><br></pre></td></tr></table></figure><ul><li>Mysql 中如果表不存在，不会自动创建</li></ul><h1 id="Sqoop创建job，自动增量导入"><a href="#Sqoop创建job，自动增量导入" class="headerlink" title="Sqoop创建job，自动增量导入"></a>Sqoop创建job，自动增量导入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sqoop job --create users -- import --connect jdbc:mysql:&#x2F;&#x2F;hadoop002:3306 \</span><br><span class="line">--username user \</span><br><span class="line">--password 123456  \</span><br><span class="line">--query &quot;select user_name ,user_id,identype from users where \$CONDITIONS&quot; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-database haibian_odbc \</span><br><span class="line">--hive-table users \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &#39;\01&#39; \</span><br><span class="line">--lines-terminated-by &#39;\n&#39; \</span><br><span class="line">--target-dir &#x2F;user&#x2F;hive&#x2F;tmp&#x2F;users \</span><br><span class="line">--hive-delims-replacement &#39; &#39; </span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 0</span><br></pre></td></tr></table></figure><ul><li>执行命令: sqoop job –exec users, 在借用调度工具定时执行</li></ul><h1 id="编写sqoop脚本"><a href="#编写sqoop脚本" class="headerlink" title="编写sqoop脚本"></a>编写sqoop脚本</h1><ul><li>vi opt/job_HDFS2RDBMS.opt<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company</span><br><span class="line">--username root</span><br><span class="line">--password 000000</span><br><span class="line">--table staff</span><br><span class="line">--num-mappers 1</span><br><span class="line">--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;staff_hive</span><br><span class="line">--input-fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure></li><li>执行: sqoop –options-file opt/job_HDFS2RDBMS.opt, 再借用调度工具定时执行</li></ul>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux大数据开发环境及工具配置</title>
      <link href="/2020/03/07/Linux%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/03/07/Linux%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h1><ul><li>下载地址: hk服务器/package/windows<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME ： C:\Program Files (x86)\Java\jdk1.8.0_60（此目录为jdk主目录）</span><br><span class="line">Path ：%JAVA_HOME%\bin</span><br><span class="line">classPath ： .;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar（别错过前面的点）</span><br><span class="line">Idea中添加JDK : File--&gt;Project Structure--&gt;SDKS--&gt;添加</span><br></pre></td></tr></table></figure></li></ul><h1 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h1><ul><li>下载地址: hk服务器/package/windows<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">MAVEN_HOME ： D:\Program Files\Apache\maven（此目录为Maven安装目录）</span><br><span class="line">Path ：%MAVEN_HOME%\bin\;</span><br><span class="line">MAVEN_OPTS : -Xms128m -Xmx512m -Duser.language&#x3D;zh -Dfile.encoding&#x3D;UTF-8</span><br><span class="line"># 配置本地仓库</span><br><span class="line">D:\Program Files\Apache\maven\conf\settings.xml</span><br><span class="line">&lt;localRepository&gt;&#x2F;path&#x2F;to&#x2F;local&#x2F;repo&lt;&#x2F;localRepository&gt;</span><br><span class="line"># 配置阿里云镜像</span><br><span class="line">&lt;mirrors&gt;  </span><br><span class="line">&lt;mirror&gt;  </span><br><span class="line">&lt;id&gt;mirrorId&lt;&#x2F;id&gt;  </span><br><span class="line">&lt;mirrorOf&gt;repositoryId&lt;&#x2F;mirrorOf&gt;  </span><br><span class="line">&lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt;  </span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;my.repository.com&#x2F;repo&#x2F;path&lt;&#x2F;url&gt;  </span><br><span class="line">&lt;&#x2F;mirror&gt;  </span><br><span class="line">&lt;mirror&gt;  </span><br><span class="line">&lt;id&gt;alimaven&lt;&#x2F;id&gt;  </span><br><span class="line">&lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt;  </span><br><span class="line">&lt;name&gt;aliyun maven&lt;&#x2F;name&gt;</span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt;  </span><br><span class="line">&lt;&#x2F;mirror&gt;  </span><br><span class="line">&lt;&#x2F;mirrors&gt; </span><br><span class="line"># Idea中配置Maven</span><br><span class="line">在setting中搜索maven</span><br><span class="line">配置Maven home directory:</span><br><span class="line">配置MAVEN_OPTS</span><br><span class="line">配置setting文件</span><br></pre></td></tr></table></figure></li></ul><h1 id="Maven命令"><a href="#Maven命令" class="headerlink" title="Maven命令"></a>Maven命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mvn clean 清除项目的生成结果</span><br><span class="line">mvn package 打包项目生成jar&#x2F;war文件</span><br><span class="line">mvn install 安装jar至本地库</span><br><span class="line">mvn compile 编译源代码</span><br><span class="line">mvn deploy 上传至私服</span><br><span class="line">mvn jar:jar 只打jar包</span><br><span class="line">mvn clean  install package -Dmaven.test.skip&#x3D;true #清理之前项目生成结果并构建然后将依赖包安装到本地仓库跳过测试</span><br><span class="line">mvn clean deploy package  -Dmaven.test.skip&#x3D;true #构建并将依赖放入私有仓库</span><br><span class="line">mvn --settings &#x2F;data&#x2F;settings.xml clean package -Dmaven.test.skip&#x3D;true #指定maven配置文件构建</span><br><span class="line">mvn install:install-file -DgroupId&#x3D;com.zebra -DartifactId&#x3D;ZSDK_CARD_API -Dversion&#x3D;v2.12.3782 -Dpackaging&#x3D;jar -Dfile&#x3D;E:\perslib\ZSDK_CARD_API.jar #安装特定jar包到仓库</span><br></pre></td></tr></table></figure><h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><ul><li>下载地址: <a href="https://www.scala-lang.org/download/" target="_blank" rel="noopener">https://www.scala-lang.org/download/</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME : D:\Program Files\scala</span><br><span class="line">Path ：%SCALA_HOME%\bin\</span><br><span class="line">Idea安装直接安装Scala插件</span><br></pre></td></tr></table></figure></li></ul><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><ul><li>winutils.ext下载地址: <a href="http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe" target="_blank" rel="noopener">http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe</a></li><li>hadoop.dll下载地址: <a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin" target="_blank" rel="noopener">https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin</a></li><li>hadoop.dll需要放在%HADOOP_HOME%\bin和C:\Windows\System32目录下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOMED:\software\hadoop2.6</span><br><span class="line">Path;%HADOOP_HOME%\bin</span><br></pre></td></tr></table></figure></li></ul><h1 id="log4j配置"><a href="#log4j配置" class="headerlink" title="log4j配置"></a>log4j配置</h1><ul><li>log4j.properties<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootCategory&#x3D;info, console</span><br><span class="line">log4j.appender.console&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.target&#x3D;System.err</span><br><span class="line">log4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern&#x3D;%p %c&#123;1&#125;: %m%n</span><br></pre></td></tr></table></figure></li></ul><h1 id="IDEA使用快捷键"><a href="#IDEA使用快捷键" class="headerlink" title="IDEA使用快捷键"></a>IDEA使用快捷键</h1><ul><li>代码自动提示<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.要保证File--&gt;Power On Save Mode是关闭状态</span><br><span class="line">2.File–&gt;Settings–&gt;Editor–&gt;General–&gt;Code Completion-&gt;Match case取消选中</span><br><span class="line">3.重启解决 问题</span><br></pre></td></tr></table></figure></li><li>打印-sout</li><li>局部变量-.var</li><li>成员变量-.field</li><li>格式化字符串-.format</li><li>判断非空-.nn,.null,.notnull</li><li>取反-.not</li><li>遍历-.for,.fori,.forr</li><li>返回值-.return</li><li>同步锁-.synchronized</li><li>Lambda表达式-.lamda</li></ul><h1 id="IDEA开发需要装的插件"><a href="#IDEA开发需要装的插件" class="headerlink" title="IDEA开发需要装的插件"></a>IDEA开发需要装的插件</h1><ul><li>Scala</li><li>Lombok(不用写实体类的get、set方法)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive窗口分析函数</title>
      <link href="/2020/02/23/Hive%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0/"/>
      <url>/2020/02/23/Hive%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive窗口分析函数"><a href="#Hive窗口分析函数" class="headerlink" title="Hive窗口分析函数"></a>Hive窗口分析函数</h1><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><blockquote><p>sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行,一般来讲聚集后的行数是要少于聚集前的行数的.<br>但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数.</p></blockquote><h3 id="主要形式"><a href="#主要形式" class="headerlink" title="主要形式"></a>主要形式</h3><p>窗口分析函数的形式是: over (….),主要有2种固定搭配</p><ul><li>over(partition by…order by…)<br>patition by是按照一个一个reduce去处理数据的，所以要使用全局排序order by</li><li>over(distribute by…sort by…)<br>distribute by是按照多个reduce去处理数据的，所以对应的排序是局部排序sort by</li></ul><h3 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h3><ul><li>默认窗口大小是从起始行到当前行</li><li>partition by …order by…rows between unbounded preceding and current row<br>从起始行到当前行</li><li>partition by …order by… rows between 3 preceding and current row<br>从当前行到之前3行</li><li>partition by …order by… rows between 3 preceding and 1 following<br>从当前行的前3行到后1行</li><li>partition by …order by… rows between current row and unbounded following<br>从当前行到最后一行</li></ul><h3 id="分析函数"><a href="#分析函数" class="headerlink" title="分析函数"></a>分析函数</h3><blockquote><p>分析函数会对窗口中的每一列输出一个结果, 查询的结果多出一列, 可以是聚合结果,可以是排序结果.</p></blockquote><ul><li>基本函数: sum(),avg(),max(),min()<br>效果和原来一样, sql示例:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select stu, class, score, sum() over(partition by class ) as sum_score;</span><br></pre></td></tr></table></figure></li><li>排名函数: row_number(), rank(), dense_rank()<br>row_number(): 没有并列，相同名次顺序排序<br>rank(): 有并列，相同名次空位(即类似于1 1 3)<br>dense_rank(): 有并列，相同名次不空位(即类似于1 1 2)</li><li>向上向下函数: lag(), lead(), first_value(), last_value()<blockquote><p>lag()<br>LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值<br>第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上n行之内，若当某一行为NULL时候，取默认值，如不指定，则为NULL）</p></blockquote><blockquote><p>lead()<br>LEAD(col,n,DEFAULT) 与LAG相反,用于统计窗口内往下第n行值<br>第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）</p></blockquote><blockquote><p>first_value()<br>取分组内排序后，截止到当前行，第一个值.<br>如果不指定ORDER BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果</p></blockquote><blockquote><p>last_value()<br>取分组内排序后，截止到当前行，最后一个值<br>如果不指定ORDER BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果</p></blockquote></li><li>分片函数: ntile(3)<br>将数据分成n片, 多的往后分, 比如: 1 1 2. 用于比如: 统计域名的流量最多的前1/3天</li><li>cume_dist(), percent_rank()<br>cume_dist(): 小于等于当前行值的行数 / 分组内的总行数<br>percent_rank(): 分组内当前行的rank值 - 1 / 分组内总行数 - 1</li></ul><h3 id="常见HQL需求分析"><a href="#常见HQL需求分析" class="headerlink" title="常见HQL需求分析"></a>常见HQL需求分析</h3>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume使用</title>
      <link href="/2020/02/20/Flume%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/02/20/Flume%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="单个采集"><a href="#单个采集" class="headerlink" title="单个采集"></a>单个采集</h1><ul><li>执行脚本<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;home&#x2F;hufei&#x2F;app&#x2F;apache-flume-1.6.0-cdh5.16.2-bin&#x2F;bin&#x2F;flume-ng agent \</span><br><span class="line">--conf $FLUME_HOME&#x2F;conf \</span><br><span class="line">--conf-file $FLUME_HOME&#x2F;script&#x2F;$1 \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure></li></ul><h2 id="avro-logger"><a href="#avro-logger" class="headerlink" title="avro-logger"></a>avro-logger</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"> </span><br><span class="line">#定制source，绑定channel、主机以及端口</span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sources.r1.bind &#x3D; hadoop002</span><br><span class="line">a1.sources.r1.port &#x3D; 4141</span><br><span class="line"> </span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"> </span><br><span class="line">#use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"> </span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h2 id="avro-hdfs"><a href="#avro-hdfs" class="headerlink" title="avro-hdfs"></a>avro-hdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent &lt;&#x3D;&#x3D; define agent</span><br><span class="line"># a1 &lt;&#x3D;&#x3D; agent name</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source &lt;&#x3D;&#x3D; define Source</span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.bind &#x3D; hadoop002</span><br><span class="line">a1.sources.r1.port &#x3D; 4141</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory  &lt;&#x3D;&#x3D; define Channel</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line"></span><br><span class="line"># Describe the sink &lt;&#x3D;&#x3D; define Sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;user&#x2F;hufei&#x2F;flume&#x2F;%Y%m%d%H</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; hive</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix &#x3D; .log</span><br><span class="line">a1.sinks.k1.hdfs.round &#x3D; true</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit &#x3D; minute</span><br><span class="line">a1.sinks.k1.hdfs.roundValue &#x3D; 1</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 102400</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 500</span><br><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 10</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel &lt;&#x3D;&#x3D; connect source,channel and sink</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h2 id="exec-hdfs"><a href="#exec-hdfs" class="headerlink" title="exec-hdfs"></a>exec-hdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r2</span><br><span class="line">a1.sinks &#x3D; k2</span><br><span class="line">a1.channels &#x3D; c2</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r2.type &#x3D; exec</span><br><span class="line">a1.sources.r2.command &#x3D; tail -F &#x2F;home&#x2F;hufei&#x2F;data&#x2F;flume.log</span><br><span class="line">a1.sources.r2.shell &#x3D; &#x2F;bin&#x2F;bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k2.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;user&#x2F;hufei&#x2F;flume&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a1.sinks.k2.hdfs.filePrefix &#x3D; logs-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a1.sinks.k2.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a1.sinks.k2.hdfs.roundValue &#x3D; 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a1.sinks.k2.hdfs.roundUnit &#x3D; hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a1.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">a1.sinks.k2.hdfs.batchSize &#x3D; 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a1.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a1.sinks.k2.hdfs.rollInterval &#x3D; 600</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a1.sinks.k2.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与 Event 数量无关</span><br><span class="line">a1.sinks.k2.hdfs.rollCount &#x3D; 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a1.sinks.k2.hdfs.minBlockReplicas &#x3D; 1</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c2.type &#x3D; memory</span><br><span class="line">a1.channels.c2.capacity &#x3D; 10000</span><br><span class="line">a1.channels.c2.transactionCapacity &#x3D; 5000</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r2.channels &#x3D; c2</span><br><span class="line">a1.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><h2 id="spooldir-hdfs"><a href="#spooldir-hdfs" class="headerlink" title="spooldir-hdfs"></a>spooldir-hdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r3</span><br><span class="line">a1.sinks &#x3D; k3</span><br><span class="line">a1.channels &#x3D; c3</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r3.type &#x3D; spooldir</span><br><span class="line">a1.sources.r3.spoolDir &#x3D; &#x2F;home&#x2F;hufei&#x2F;data</span><br><span class="line">a1.sources.r3.fileSuffix &#x3D; .COMPLETED</span><br><span class="line">a1.sources.r3.fileHeader &#x3D; true</span><br><span class="line">#忽略所有以.tmp 结尾的文件，不上传</span><br><span class="line">a1.sources.r3.ignorePattern &#x3D; ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k3.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k3.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;user&#x2F;hufei&#x2F;spooldir&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a1.sinks.k3.hdfs.filePrefix &#x3D; upload-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a1.sinks.k3.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a1.sinks.k3.hdfs.roundValue &#x3D; 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a1.sinks.k3.hdfs.roundUnit &#x3D; hour</span><br><span class="line">a1.sinks.k3.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">a1.sinks.k3.hdfs.batchSize &#x3D; 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a1.sinks.k3.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a1.sinks.k3.hdfs.rollInterval &#x3D; 600</span><br><span class="line">#设置每个文件的滚动大小大概是 128M</span><br><span class="line">a1.sinks.k3.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与 Event 数量无关</span><br><span class="line">a1.sinks.k3.hdfs.rollCount &#x3D; 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a1.sinks.k3.hdfs.minBlockReplicas &#x3D; 1</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c3.type &#x3D; memory</span><br><span class="line">a1.channels.c3.capacity &#x3D; 10000</span><br><span class="line">a1.channels.c3.transactionCapacity &#x3D; 1000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r3.channels &#x3D; c3</span><br><span class="line">a1.sinks.k3.channel &#x3D; c3</span><br></pre></td></tr></table></figure><h2 id="taildir-logger"><a href="#taildir-logger" class="headerlink" title="taildir-logger"></a>taildir-logger</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r3</span><br><span class="line">a1.sinks &#x3D; k3</span><br><span class="line">a1.channels &#x3D; c3</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r3.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r3.filegroups &#x3D; f1 f2</span><br><span class="line">a1.sources.r3.positionFile &#x3D; &#x2F;home&#x2F;hufei&#x2F;data&#x2F;taildir&#x2F;offset&#x2F;taildir_position.json</span><br><span class="line">a1.sources.r3.filegroups.f1 &#x3D; &#x2F;home&#x2F;hufei&#x2F;data&#x2F;taildir&#x2F;test1&#x2F;example.log</span><br><span class="line">a1.sources.r3.headers.f1.headerKey1 &#x3D; value1</span><br><span class="line">a1.sources.r3.filegroups.f2 &#x3D; &#x2F;home&#x2F;hufei&#x2F;data&#x2F;taildir&#x2F;test2&#x2F;.*log.*</span><br><span class="line">a1.sources.r3.headers.f2.headerKey1 &#x3D; value2</span><br><span class="line">a1.sources.r3.headers.f2.headerKey2 &#x3D; value2-2</span><br><span class="line">a1.sources.r3.fileHeader &#x3D; true</span><br><span class="line">a1.sources.r3.maxBatchCount &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k3.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Describe channel</span><br><span class="line">a1.channels.c3.type &#x3D; memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r3.channels &#x3D; c3</span><br><span class="line">a1.sinks.k3.channel &#x3D; c3</span><br></pre></td></tr></table></figure><h2 id="taildir-hdfs"><a href="#taildir-hdfs" class="headerlink" title="taildir-hdfs"></a>taildir-hdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r3</span><br><span class="line">a1.sinks &#x3D; k3</span><br><span class="line">a1.channels &#x3D; c3</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r3.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r3.filegroups &#x3D; f1</span><br><span class="line">a1.sources.r3.positionFile &#x3D; &#x2F;home&#x2F;hufei&#x2F;data&#x2F;mocklog&#x2F;offset&#x2F;taildir_position.json</span><br><span class="line">a1.sources.r3.filegroups.f1 &#x3D; &#x2F;home&#x2F;hufei&#x2F;data&#x2F;mocklog&#x2F;origin&#x2F;access.log</span><br><span class="line">a1.sources.r3.headers.f1.headerKey1 &#x3D; value1</span><br><span class="line">a1.sources.r3.fileHeader &#x3D; true</span><br><span class="line">a1.sources.r3.maxBatchCount &#x3D; 1000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k3.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k3.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;user&#x2F;hufei&#x2F;hive&#x2F;origin&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a1.sinks.k3.hdfs.filePrefix &#x3D; logs-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a1.sinks.k3.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a1.sinks.k3.hdfs.roundValue &#x3D; 6</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a1.sinks.k3.hdfs.roundUnit &#x3D; hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a1.sinks.k3.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">a1.sinks.k3.hdfs.batchSize &#x3D; 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a1.sinks.k3.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a1.sinks.k3.hdfs.rollInterval &#x3D; 1000</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a1.sinks.k3.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与 Event 数量无关</span><br><span class="line">a1.sinks.k3.hdfs.rollCount &#x3D; 0</span><br><span class="line">#最小冗余数</span><br><span class="line">a1.sinks.k3.hdfs.minBlockReplicas &#x3D; 1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c3.type &#x3D; memory</span><br><span class="line">a1.channels.c3.capacity &#x3D; 10000</span><br><span class="line">a1.channels.c3.transactionCapacity &#x3D; 5000</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r3.channels &#x3D; c3</span><br><span class="line">a1.sinks.k3.channel &#x3D; c3</span><br></pre></td></tr></table></figure><h1 id="Flume多个的串并连"><a href="#Flume多个的串并连" class="headerlink" title="Flume多个的串并连"></a>Flume多个的串并连</h1><p>##</p><p>##</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce运行解密</title>
      <link href="/2020/02/12/MapReduce%E8%BF%90%E8%A1%8C%E8%A7%A3%E5%AF%86/"/>
      <url>/2020/02/12/MapReduce%E8%BF%90%E8%A1%8C%E8%A7%A3%E5%AF%86/</url>
      
        <content type="html"><![CDATA[<h1 id="程序编写"><a href="#程序编写" class="headerlink" title="程序编写"></a>程序编写</h1><h3 id="1-编写map逻辑"><a href="#1-编写map逻辑" class="headerlink" title="1.编写map逻辑"></a>1.编写map逻辑</h3><blockquote><p>1.map方法要继承Mapper,重写map方法<br>2.类的四个参数分别是: map输入偏移量,map一次读取的值,map输出key,map输出value</p></blockquote><h3 id="2-编写reduce逻辑"><a href="#2-编写reduce逻辑" class="headerlink" title="2.编写reduce逻辑"></a>2.编写reduce逻辑</h3><blockquote><p>1.map方法要继承Reducer,重写reduce方法<br>2.类的四个参数分别是: map输出key,map输出value,reduce输出key,reduce输出value</p></blockquote><h3 id="3-编写mian方法"><a href="#3-编写mian方法" class="headerlink" title="3.编写mian方法"></a>3.编写mian方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">String input &#x3D; args[0];</span><br><span class="line">String output &#x3D; args[1];</span><br><span class="line"></span><br><span class="line">FileUtil.deleteDir(new File(output)); &#x2F;&#x2F; 删除目标文件夹</span><br><span class="line"></span><br><span class="line">Configuration conf &#x3D; new Configuration(); &#x2F;&#x2F; new conf</span><br><span class="line">Job job &#x3D; Job.getInstance(conf); &#x2F;&#x2F; new job</span><br><span class="line"></span><br><span class="line">job.setJarByClass(WordCountDriver.class); &#x2F;&#x2F; set main class</span><br><span class="line">job.setMapperClass(WordCountMapper.class); &#x2F;&#x2F; set map class</span><br><span class="line">job.setReducerClass(WordCountReducer.class); &#x2F;&#x2F; set reduce class</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; map阶段输出</span><br><span class="line">job.setMapOutputKeyClass(Text.class); &#x2F;&#x2F; map out key</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class); &#x2F;&#x2F; map out value</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; reduce阶段输出</span><br><span class="line">job.setOutputKeyClass(Text.class); &#x2F;&#x2F; reduce out key</span><br><span class="line">job.setOutputValueClass(IntWritable.class); &#x2F;&#x2F; reduce out value</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 设置输入和输出路径</span><br><span class="line">FileInputFormat.setInputPaths(job, new Path(input)); &#x2F;&#x2F; in path</span><br><span class="line">FileOutputFormat.setOutputPath(job, new Path(output)); &#x2F;&#x2F; out path</span><br><span class="line"></span><br><span class="line">boolean result &#x3D; job.waitForCompletion(true); &#x2F;&#x2F; start job</span><br><span class="line">System.exit(result ? 0 : 1); &#x2F;&#x2F; exit</span><br></pre></td></tr></table></figure><h1 id="本地和线上运行"><a href="#本地和线上运行" class="headerlink" title="本地和线上运行"></a>本地和线上运行</h1><ul><li>本地运行: 输入2个参数，直接run</li><li>线上运行: <blockquote><p>1.rz上传需要运行的jar包和包含运行命令的sh文件,shell文件内容为<br>hadoop jar xx.jar Java程序全路径名 文件输入路径 结果输出路径<br>如: hadoop jar wc.jar com.hufei.bigdata.hadoop.mapreduce.WordcountDriver /user/hufei/hdfs7 /user/hufei/hdfs8<br>其他参数说明: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">files &lt;逗号分隔的文件列表&gt;    指定要拷贝到map reduce集群的文件的逗号分隔的列表。 </span><br><span class="line">-libjars &lt;逗号分隔的jar列表&gt;    指定要包含到classpath中的jar文件的逗号分隔的列表。</span><br><span class="line">-archives &lt;逗号分隔的archive列表&gt;    指定要被解压到计算节点上的档案文件的逗号分割的列表。Archive似乎可以用来聚集大量小文件来提升运行效率</span><br></pre></td></tr></table></figure>2.类的四个参数分别是: map输出key,map输出value,reduce输出key,reduce输出value</blockquote></li></ul><h1 id="运行过程"><a href="#运行过程" class="headerlink" title="运行过程"></a>运行过程</h1><h3 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h3><blockquote><p>Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。<br>一个Map/Reduce 作业（job） 通常会把输入的数据集切分为若干独立的数据块，这个切分由 map任务（task）以完全并行的方式处理。框架会对map的输出先进行排序， 然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。<br>通常，Map/Reduce框架和分布式文件系统（ Hadoop Distributed File System，HDFS）是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。<br>Map/Reduce框架由一个单独的master ResourceManager 、每个cluster-node一个slave NodeManager、每个application一个MRAppMaster共同组成( 参考 YARN Architecture Guide）。<br>应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop的 job client提交作业（jar包/可执行程序等）和配置信息给ResourceManager，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。<br>虽然Hadoop框架是用JavaTM实现的，但Map/Reduce应用程序则不一定要用 Java来写 。<br>Hadoop Streaming是一种运行作业的实用工具，它允许用户创建和运行任何可执行程序 （例如：Shell工具）来做为mapper和reducer。<br>Hadoop Pipes是一个与SWIG兼容的C++ API （没有基于JNITM技术），它也可用于实现Map/Reduce应用程序。</p></blockquote><p>**</p><ul><li>1.MapReduce的工作方式：map任务以并行方式切分数据块，然后框架对map的输出进行排序，并作为reduce任务的输入，整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</li><li>2.M/R框架和HDFS运行在同一组节点上</li><li>3.MapReduce不是Hadoop中唯一的MR工具</li></ul><p>**</p><h3 id="MapReduce的输入和输出"><a href="#MapReduce的输入和输出" class="headerlink" title="MapReduce的输入和输出"></a>MapReduce的输入和输出</h3><blockquote><p>Map/Reduce框架运转在&lt;key, value&gt; 键值对上，也就是说， 框架把作业的输入看为是一组&lt;key, value&gt; 键值对，同样也产出一组 &lt;key, value&gt; 键值对做为作业的输出，这两组键值对的类型可能不同。<br>框架需要对key和value的类(classes)进行序列化操作， 因此，这些类需要实现 Writable接口。 另外，为了方便框架执行排序操作，key类必须实现 WritableComparable接口。<br>一个MapReduce作业的输入输出类型如下所示：<br>(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</p></blockquote><h3 id="Input-Inputformat"><a href="#Input-Inputformat" class="headerlink" title="Input-Inputformat"></a>Input-Inputformat</h3><ul><li>访问datanode中的数据反序列化数据并进行切片，为每一个切片分配一个map任务</li><li>并发的执行这些任务</li><li>通过recordReader读取切片中的每一条记录，按照记录格式读取，偏移值作为map的key，记录行作为value，当做map方法的参数<blockquote><p>input阶段主要是从节点上反序列化数据，读取后切片，供map阶段使用<br>序列化格式和inputformat格式可以自定义设置<br>只有支持分片的压缩格式可以分片<br>记录格式：如serse 用正则表达式来转换文本hive</p></blockquote></li></ul><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><ul><li>通过对输入记录的处理，转换成一个或多个中间记录</li></ul><h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.map数据会先输出到内存缓冲区中，到达默认的80%的阀值后，会像map任务本地写数据，每次写都会生成一个小文件。</span><br><span class="line">2.在写到本地的过程中，会经历分区，排序，combiner（可选）的过程</span><br><span class="line">3.当最后一个文件溢写到本地磁盘中的时候，区与区的文件就是合并，排序，压缩（可选）</span><br><span class="line">4.经过分区的排序的大文件会按照不同的分区被拷贝到相应的reduce中处理</span><br><span class="line">5.reduce端通过http network复制map端传来的数据</span><br><span class="line">6.将输入的数据排序后合并，经过2次排序后会返回一个value的迭代器</span><br><span class="line">7.分组将相同key的value放到一起作为一个value集合，作为reduce的输入</span><br></pre></td></tr></table></figure><ul><li>Partition: 将map端数据分类,默认使用hashPartitioner。可以自己定义。分区数等于reduce task数，需要将reduce task设置的和partition数相同，否则会有问题。</li><li>Sort: map端Sort是map阶段对数据的key进行字典排序，可以自定义，需要实现ComparableWrite接口。</li><li>Combiner: 在map端对相同key做聚合,也称为本地reduce。可以减轻reduce端的压力，减少网络传输,减少shuffle。不能用于求平均值等影响最终结果的场景。</li><li>Merge: 对map端过来的数据做聚合，相同key做聚合。</li><li>Sort: 对聚合的数据的key进行排序,默认是字典序。</li><li>Compress: 对数据进行压缩，常用的Sequence File、Avro DataFile、ORCFile、Parquet File</li></ul><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><ul><li>将数据写入文件系统</li></ul><h3 id="Out-Outputformat"><a href="#Out-Outputformat" class="headerlink" title="Out-Outputformat"></a>Out-Outputformat</h3><ul><li>可以自定义输出格式，按照指定条件分离数据，比如分离出不同公司的日志。</li></ul>]]></content>
      
      
      <categories>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产中Hdoop集群规划</title>
      <link href="/2019/12/31/%E7%94%9F%E4%BA%A7%E4%B8%ADHdoop%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92/"/>
      <url>/2019/12/31/%E7%94%9F%E4%BA%A7%E4%B8%ADHdoop%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h1 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h1>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS操作合集</title>
      <link href="/2019/12/23/HDFS%E6%93%8D%E4%BD%9C%E5%90%88%E9%9B%86/"/>
      <url>/2019/12/23/HDFS%E6%93%8D%E4%BD%9C%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS读写"><a href="#HDFS读写" class="headerlink" title="HDFS读写"></a>HDFS读写</h1><h3 id="HDFS读文件"><a href="#HDFS读文件" class="headerlink" title="HDFS读文件"></a>HDFS读文件</h3><ul><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址</li><li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li><li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。</li><li>客户端以packet为单位接收，先在本地缓存，然后写入目标文件。</li></ul><h3 id="HDFS写文件"><a href="#HDFS写文件" class="headerlink" title="HDFS写文件"></a>HDFS写文件</h3><ul><li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li><li>NameNode返回是否可以上传。</li><li>客户端请求第一个 block上传到哪几个datanode服务器上。</li><li>NameNode返回多个个datanode节点，分别为dn1、dn2、dn3…。</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li><li>dn1、dn2、dn3逐级应答客户端。</li><li>客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li><li>当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。</li></ul><p><strong>在传输过程中，如果dn2、dn3挂了，则NameNode在接收到到Client传输完成时，会检查副本数量是否足够，不够的话异步复制少的副本。如果dn1挂了，那么client会再次请求NameNode，再获取几个节点，NameNode将之前元数据修改，并让对应dn删除对应数据</strong></p><h1 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h1><blockquote><p>主要命令方式有：<br>hadoop fs *<br>hadoop dfs *<br>hdfs dfs *</p></blockquote><ul><li>查看帮助: hdfs dfs -help</li><li>查看当前目录信息: hdfs dfs -ls /</li><li>上传文件: hdfs dfs -put /本地路径 /hdfs路径</li><li>剪切文件: hdfs dfs -moveFromLocal a.txt /aa.txt</li><li>下载文件到本地: hdfs dfs -get /hdfs路径 /本地路径</li><li>合并下载: hdfs dfs -getmerge /hdfs路径文件夹 /合并后的文件</li><li>创建文件夹: hdfs dfs -mkdir /hello</li><li>创建多级文件夹: hdfs dfs -mkdir -p /hello/world</li><li>移动hdfs文件: hdfs dfs -mv /hdfs路径 /hdfs路径</li><li>复制hdfs文件: hdfs dfs -cp /hdfs路径 /hdfs路径</li><li>删除hdfs文件: hdfs dfs -rm /aa.txt</li><li>删除hdfs文件夹: hdfs dfs -rm -r /hello</li><li>查看hdfs中的文件内容: hdfs dfs -cat /文件 | hdfs dfs -tail -f /文件</li><li>查看hdfs的总空间:  hdfs dfs -df / | hdfs dfs -df -h /</li><li>修改副本数: hdfs dfs -setrep 1 /a.txt</li><li>查看目录中所有文件大小: hdfs dfs -du hdfs://host:port/user/hadoop/dir1</li></ul><h1 id="Java操作HDFS"><a href="#Java操作HDFS" class="headerlink" title="Java操作HDFS"></a>Java操作HDFS</h1><ul><li>初始化加载<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void setUp() throws Exception &#123;</span><br><span class="line">URI uri &#x3D; new URI(&quot;hdfs:&#x2F;&#x2F;hadoop002:9000&quot;);</span><br><span class="line">Configuration configuration &#x3D; new Configuration();</span><br><span class="line"></span><br><span class="line">configuration.set(&quot;dfs.client.use.datanode.hostname&quot;,&quot;true&quot;); </span><br><span class="line">configuration.set(&quot;dfs.replication&quot;,&quot;1&quot;); &#x2F;&#x2F; 设置副本数</span><br><span class="line">fileSystem &#x3D; FileSystem.get(uri, configuration,&quot;hadoop&quot;); &#x2F;&#x2F; 设置上传用户</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>关闭流<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public void tearDown()throws Exception &#123;</span><br><span class="line">if(null !&#x3D; fileSystem) &#123;</span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>创建文件夹<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fileSystem.mkdirs(new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs&quot;));</span><br></pre></td></tr></table></figure></li><li>上传文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Path src &#x3D; new Path(&quot;data&#x2F;word.txt&quot;);</span><br><span class="line">Path dst &#x3D; new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs&quot;);</span><br><span class="line">fileSystem.copyFromLocalFile(src, dst);</span><br></pre></td></tr></table></figure></li><li>下载文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Path src &#x3D; new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs&#x2F;word.txt&quot;);</span><br><span class="line">Path dst &#x3D; new Path(&quot;output&#x2F;word.txt&quot;);</span><br><span class="line">fileSystem.copyToLocalFile(true, src, dst);</span><br></pre></td></tr></table></figure></li><li>重命名<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Path src &#x3D; new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs&quot;);</span><br><span class="line">Path dst &#x3D; new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs2&quot;);</span><br><span class="line">fileSystem.rename(src, dst);</span><br></pre></td></tr></table></figure></li><li>删除文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fileSystem.delete(new Path(&quot;&#x2F;user&#x2F;hufei&#x2F;hdfs2&#x2F;word.txt&quot;), true);</span><br></pre></td></tr></table></figure></li><li>读取目录内的文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; files &#x3D; fileSystem.listFiles(new Path(&quot;&#x2F;&#x2F;user&#x2F;hufei&#x2F;hdfs2&quot;), true);</span><br><span class="line">while (files.hasNext())&#123;</span><br><span class="line">LocatedFileStatus fileStatus &#x3D; files.next();</span><br><span class="line"></span><br><span class="line">String path &#x3D; fileStatus.getPath().toString();</span><br><span class="line">long len &#x3D; fileStatus.getLen();</span><br><span class="line">short replication &#x3D; fileStatus.getReplication();</span><br><span class="line">FsPermission permission &#x3D; fileStatus.getPermission();</span><br><span class="line">String isDir &#x3D; fileStatus.isDirectory() ? &quot;文件夹&quot; : &quot;文件&quot;;</span><br><span class="line"></span><br><span class="line">System.out.println(path + &quot;\t&quot; + len + &quot;\t&quot; + replication + &quot;\t&quot; +permission + &quot;\t&quot; + isDir);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; block块位置</span><br><span class="line">BlockLocation[] blockLocations &#x3D; fileStatus.getBlockLocations();</span><br><span class="line">for(BlockLocation location : blockLocations) &#123;</span><br><span class="line">String[] hosts &#x3D; location.getHosts();</span><br><span class="line">for(String host : hosts) &#123;</span><br><span class="line">System.out.println(host + &quot;........&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PHP之TP5技能合集</title>
      <link href="/2019/12/22/PHP%E4%B9%8BTP5%E6%8A%80%E8%83%BD%E5%90%88%E9%9B%86/"/>
      <url>/2019/12/22/PHP%E4%B9%8BTP5%E6%8A%80%E8%83%BD%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="TP5相关解决问题"><a href="#TP5相关解决问题" class="headerlink" title="TP5相关解决问题"></a>TP5相关解决问题</h1><h3 id="1-默认入口文件public-其他文件不允许访问"><a href="#1-默认入口文件public-其他文件不允许访问" class="headerlink" title="1.默认入口文件public,其他文件不允许访问"></a>1.默认入口文件public,其他文件不允许访问</h3><h3 id="2-在application内部，后台一般放在admin内，前台代码一般放在index内，遵循MVC架构"><a href="#2-在application内部，后台一般放在admin内，前台代码一般放在index内，遵循MVC架构" class="headerlink" title="2.在application内部，后台一般放在admin内，前台代码一般放在index内，遵循MVC架构"></a>2.在application内部，后台一般放在admin内，前台代码一般放在index内，遵循MVC架构</h3><h3 id="3-application文件夹中的文件作用"><a href="#3-application文件夹中的文件作用" class="headerlink" title="3.application文件夹中的文件作用"></a>3.application文件夹中的文件作用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- command.php 命令行启动TP5框架需要读取的文件</span><br><span class="line">- common.php 常用的函数，都写在这个文件中</span><br><span class="line">- config.php 配置文件，开启什么，关闭什么，都在这设置</span><br><span class="line">- database.php 连接数据库时候读取的文件，比如用户名</span><br><span class="line">- route.php 路由文件，美化url的</span><br><span class="line">- tags.php 扩展框架的时候用到</span><br></pre></td></tr></table></figure><h3 id="4-public内文件作用"><a href="#4-public内文件作用" class="headerlink" title="4.public内文件作用"></a>4.public内文件作用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- static 这里放的是css、html之类的静态文件</span><br><span class="line">- favicon.ico 这个是网站图标，在标签栏显示的</span><br><span class="line">- index.php 网站入口文件，所有的请求都会经过这里</span><br><span class="line">- robots.txt 禁止搜索引擎爬取页面的设置</span><br><span class="line">- router.php 在没有部署网站环境的情况下，配置这个文件可以让网站运行</span><br></pre></td></tr></table></figure><h3 id="开发规范"><a href="#开发规范" class="headerlink" title="开发规范"></a>开发规范</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- 目录 只是小写字母和下划线构成</span><br><span class="line">- 类的文件名以命名空间定义，并且命名空间和类库文件所在路径一致。</span><br><span class="line">- 类的文件采用驼峰，并且首字母大写，其余文件为小写加下划线。</span><br><span class="line">- 类名和类文件名保持一致，采用驼峰命名，首字母大写。</span><br><span class="line">- 函数使用驼峰命名，首字母小写。</span><br><span class="line">- 属性名采用驼峰，首字母小写</span><br><span class="line">- 以双下划线开头的函数或方法为魔术方法。</span><br><span class="line">- 常量以大写字母和下划线命名</span><br><span class="line">- 表和字段必须以小写字母和下划线命名方式，不能以下划线开头。</span><br></pre></td></tr></table></figure><h1 id="apache配置根目录"><a href="#apache配置根目录" class="headerlink" title="apache配置根目录"></a>apache配置根目录</h1><ul><li>修改vhosts.conf<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;VirtualHost _default_:80&gt;</span><br><span class="line">DocumentRoot &quot;C:\phpStudy\PHPTutorial\WWW\public&quot;</span><br><span class="line">    ServerName localhost</span><br><span class="line">    ServerAlias localhost</span><br><span class="line">  &lt;Directory &quot;C:\phpStudy\PHPTutorial\WWW\public&quot;&gt;</span><br><span class="line">    #下面被注释的代码，用“localhost”访问时会禁止访问</span><br><span class="line">    #Options -Indexes -FollowSymLinks +ExecCGI</span><br><span class="line">    Options FollowSymLinks ExecCGI</span><br><span class="line">    AllowOverride All</span><br><span class="line">    Order allow,deny</span><br><span class="line">    Allow from all</span><br><span class="line">    Require all granted</span><br><span class="line">  &lt;&#x2F;Directory&gt;</span><br><span class="line">&lt;&#x2F;VirtualHost&gt;</span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:80&gt;</span><br><span class="line">    DocumentRoot &quot;C:\phpStudy\PHPTutorial\WWW\public&quot;</span><br><span class="line">    ServerName www.gohosts.com</span><br><span class="line">    ServerAlias gohosts.com</span><br><span class="line">  &lt;Directory &quot;C:\phpStudy\PHPTutorial\WWW\public&quot;&gt;</span><br><span class="line">      Options FollowSymLinks ExecCGI</span><br><span class="line">      AllowOverride All</span><br><span class="line">      Order allow,deny</span><br><span class="line">      Allow from all</span><br><span class="line">     Require all granted</span><br><span class="line">  &lt;&#x2F;Directory&gt;</span><br><span class="line">&lt;&#x2F;VirtualHost&gt;</span><br></pre></td></tr></table></figure></li></ul><h1 id="apache配置伪静态"><a href="#apache配置伪静态" class="headerlink" title="apache配置伪静态"></a>apache配置伪静态</h1><ul><li>httpd.conf</li><li>去掉LoadModule rewrite_module前面的#</li><li>AllowOverride None 替换为 AllowOverride All(有多处设置，需要替换)然后保存</li><li>在相应的目录下，添加.htacces文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;IfModule mod_rewrite.c&gt;</span><br><span class="line">  Options +FollowSymlinks -Multiviews</span><br><span class="line">  RewriteEngine On</span><br><span class="line"></span><br><span class="line">  RewriteCond %&#123;REQUEST_FILENAME&#125; !-d</span><br><span class="line">  RewriteCond %&#123;REQUEST_FILENAME&#125; !-f</span><br><span class="line">  RewriteRule ^(.*)$ index.php [L,E&#x3D;PATH_INFO:$1]</span><br><span class="line">&lt;&#x2F;IfModule&gt;</span><br></pre></td></tr></table></figure></li></ul><h1 id="iis配置伪静态"><a href="#iis配置伪静态" class="headerlink" title="iis配置伪静态"></a>iis配置伪静态</h1><ul><li>添加web.config文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;system.webServer&gt;</span><br><span class="line">    &lt;rewrite&gt;</span><br><span class="line">      &lt;rules&gt;</span><br><span class="line">        &lt;rule name&#x3D;&quot;cms&quot;&gt;</span><br><span class="line">          &lt;match url&#x3D;&quot;^cms&#x2F;(.*)$&quot; &#x2F;&gt;  </span><br><span class="line">          &lt;action type&#x3D;&quot;Rewrite&quot; url&#x3D;&quot;&#x2F;public&#x2F;cms&#x2F;pages&#x2F;&#123;R:1&#125;&quot; &#x2F;&gt; </span><br><span class="line">&lt;&#x2F;rule&gt;</span><br><span class="line">        &lt;rule name&#x3D;&quot;OrgPage&quot; stopProcessing&#x3D;&quot;true&quot;&gt;</span><br><span class="line">          &lt;match url&#x3D;&quot;^(.*)$&quot; &#x2F;&gt;</span><br><span class="line">          &lt;conditions logicalGrouping&#x3D;&quot;MatchAll&quot;&gt;</span><br><span class="line">            &lt;add input&#x3D;&quot;&#123;HTTP_HOST&#125;&quot; pattern&#x3D;&quot;^(.*)$&quot; &#x2F;&gt;</span><br><span class="line">            &lt;add input&#x3D;&quot;&#123;REQUEST_FILENAME&#125;&quot; matchType&#x3D;&quot;IsFile&quot; negate&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">            &lt;add input&#x3D;&quot;&#123;REQUEST_FILENAME&#125;&quot; matchType&#x3D;&quot;IsDirectory&quot; negate&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">          &lt;&#x2F;conditions&gt;</span><br><span class="line">          &lt;action type&#x3D;&quot;Rewrite&quot; url&#x3D;&quot;&#x2F;public&#x2F;index.php&#x2F;&#123;R:1&#125;&quot; &#x2F;&gt;</span><br><span class="line">        &lt;&#x2F;rule&gt;</span><br><span class="line">      &lt;&#x2F;rules&gt;</span><br><span class="line">    &lt;&#x2F;rewrite&gt;</span><br><span class="line">  &lt;&#x2F;system.webServer&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> PHP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PHP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据常用shell脚本</title>
      <link href="/2019/12/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8shell%E8%84%9A%E6%9C%AC/"/>
      <url>/2019/12/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8shell%E8%84%9A%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<h1 id="shell交互脚本"><a href="#shell交互脚本" class="headerlink" title="shell交互脚本"></a>shell交互脚本</h1><ul><li><p>zkManager.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">for host in hdp-1 hdp-2 hdp-3</span><br><span class="line">do</span><br><span class="line">echo "$&#123;host&#125;:$&#123;1&#125;ing...."</span><br><span class="line">ssh $host  "source /etc/profile;/root/apps/zookeeper-3.4.6/bin/zkServer.sh $1"</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">sleep 2</span><br><span class="line"> </span><br><span class="line">for host in hdp-1 hdp-2 hdp-3</span><br><span class="line">do</span><br><span class="line">ssh $host  "source /etc/profile;/root/apps/zookeeper-3.4.6/bin/zkServer.sh status"</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>start-kafka-cluster.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">brokers="hdp-1 hdp-2 hdp-3"</span><br><span class="line">KAFKA_HOME="/root/apps/kafka_2.12-2.2.0"</span><br><span class="line">KAFKA_NAME="kafka_2.12-2.2.0"</span><br><span class="line"> </span><br><span class="line">echo "开启kafka ..."</span><br><span class="line"> </span><br><span class="line">for broker in $brokers</span><br><span class="line">do</span><br><span class="line">  echo "INFO : Starting $&#123;KAFKA_NAME&#125; on $&#123;broker&#125; ..."</span><br><span class="line">  ssh $&#123;broker&#125; -C "source /etc/profile; sh $&#123;KAFKA_HOME&#125;/bin/kafka-server-start.sh -daemon $&#123;KAFKA_HOME&#125;/config/server.properties"</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>stop-kafka-cluster.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">brokers="hdp-1 hdp-2 hdp-3"</span><br><span class="line">KAFKA_HOME="/root/apps/kafka_2.12-2.2.0"</span><br><span class="line">KAFKA_NAME="kafka_2.12-2.2.0"</span><br><span class="line"> </span><br><span class="line">echo "关闭kafka ..."</span><br><span class="line"> </span><br><span class="line">for broker in $brokers</span><br><span class="line">do</span><br><span class="line">  echo "INFO : Shut down $&#123;KAFKA_NAME&#125; on $&#123;broker&#125; ..."</span><br><span class="line">  ssh $&#123;broker&#125; "source /etc/profile;bash $&#123;KAFKA_HOME&#125;/bin/kafka-server-stop.sh"</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>install-jdk.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">BASE_SERVER=172.16.203.100</span><br><span class="line">yum install -y wget</span><br><span class="line">wget $BASE_SERVER/soft/jdk-7u45-linux-x64.tar.gz</span><br><span class="line">tar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local</span><br><span class="line">cat &gt;&gt; /etc/profile &lt;&lt; EOF</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.7.0_45</span><br><span class="line">export PATH=\$PATH:\$JAVA_HOME/bin</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>boot.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">SERVERS="node-3.itcast.cn node-4.itcast.cn"</span><br><span class="line">PASSWORD=123456</span><br><span class="line">BASE_SERVER=172.16.203.100</span><br><span class="line"></span><br><span class="line">auto_ssh_copy_id() &#123;</span><br><span class="line">    expect -c "set timeout -1;</span><br><span class="line">        spawn ssh-copy-id $1;</span><br><span class="line">        expect &#123;</span><br><span class="line">            *(yes/no)* &#123;send -- yes\r;exp_continue;&#125;</span><br><span class="line">            *assword:* &#123;send -- $2\r;exp_continue;&#125;</span><br><span class="line">            eof        &#123;exit 0;&#125;</span><br><span class="line">        &#125;";</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_copy_id_to_all() &#123;</span><br><span class="line">    for SERVER in $SERVERS</span><br><span class="line">    do</span><br><span class="line">        auto_ssh_copy_id $SERVER $PASSWORD</span><br><span class="line">    done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssh_copy_id_to_all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for SERVER in $SERVERS</span><br><span class="line">do</span><br><span class="line">    scp install.sh root@$SERVER:/root</span><br><span class="line">    ssh root@$SERVER /root/install.sh</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux&amp;Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH搭建问题合集</title>
      <link href="/2019/12/08/CDH%E6%90%AD%E5%BB%BA%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/"/>
      <url>/2019/12/08/CDH%E6%90%AD%E5%BB%BA%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink</title>
      <link href="/2019/12/08/Flink/"/>
      <url>/2019/12/08/Flink/</url>
      
        <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>DataStream<br>    Source transformation Sink<br>部署<br>    Standalone<br>    Yarn<br>Window<br>    Window<br>    Watermark<br>    Time三兄弟<br>State<br>SQL&amp;Table API<br>CEP<br>项目<br>    拆分topic<br>    实时ETL<br>    Task 合理的配置</p><h1 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate\</span><br><span class="line">  -DarchetypeGroupId&#x3D;org.apache.flink\</span><br><span class="line">  -DarchetypeArtifactId&#x3D;flink-quickstart-scala\</span><br><span class="line">  -DarchetypeVersion&#x3D;$&#123;1:-1.10.0&#125;\</span><br><span class="line">  -DgroupId&#x3D;com.hufei.flink\</span><br><span class="line">  -DartifactId&#x3D;hufei-flink\</span><br><span class="line">  -Dversion&#x3D;0.1\</span><br><span class="line">  -DinteractiveMode&#x3D;false                              \</span><br><span class="line">  -DarchetypeCatalog&#x3D;local</span><br></pre></td></tr></table></figure><ul><li>删掉pom文件中的provided</li></ul><h1 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val flinkRdd: DataSet[String] = env.readTextFile(<span class="string">"data/wc.txt"</span>)</span><br><span class="line">flinkRdd.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    .map((_, <span class="number">1</span>))</span><br><span class="line">    .groupBy(<span class="number">0</span>)</span><br><span class="line">    .sum(<span class="number">1</span>)</span><br><span class="line">    .print()</span><br></pre></td></tr></table></figure><h1 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">val stream &#x3D; env.socketTextStream(&quot;114.67.105.94&quot;, 9999)</span><br><span class="line">stream.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">    .map((_, 1))</span><br><span class="line">    .keyBy(0)</span><br><span class="line">    .sum(1)</span><br><span class="line">    .print()</span><br><span class="line">env.execute(&quot;Flink Streaming Scala API Skeleton&quot;)</span><br></pre></td></tr></table></figure><h1 id="参数utils类"><a href="#参数utils类" class="headerlink" title="参数utils类"></a>参数utils类</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"><span class="comment">//    fromArgs(args)</span></span><br><span class="line">    fromFile(args)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">fromArgs</span><span class="params">(args: Array[String])</span>: Unit </span>=&#123;</span><br><span class="line">    val parameters = ParameterTool.fromArgs(args)</span><br><span class="line">    val groupId = parameters.get(<span class="string">"group.id"</span>, <span class="string">"hufei-default-group"</span>)</span><br><span class="line">    val topics = parameters.getRequired(<span class="string">"topics"</span>)</span><br><span class="line">    println(s<span class="string">"$groupId, $topics"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">fromFile</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val parameters = ParameterTool.fromPropertiesFile(<span class="string">"conf/flink.properties"</span>)</span><br><span class="line">    val groupId = parameters.get(<span class="string">"group.id"</span>)</span><br><span class="line">    val topics = parameters.get(<span class="string">"topics"</span>)</span><br><span class="line">    println(s<span class="string">"$groupId, $topics"</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h1 id="使用定制类排序"><a href="#使用定制类排序" class="headerlink" title="使用定制类排序"></a>使用定制类排序</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span> </span>&#123;</span><br><span class="line">    val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    val parameters = ParameterTool.fromArgs(args)</span><br><span class="line">    val hostname = parameters.get(<span class="string">"hostname"</span>)</span><br><span class="line">    val port = parameters.get(<span class="string">"port"</span>).toInt</span><br><span class="line"></span><br><span class="line">    val stream = env.socketTextStream(hostname, port)</span><br><span class="line">    stream.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(WC(_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_.word)</span><br><span class="line">      .sum(<span class="string">"count"</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">WC</span><span class="params">(word: String, count: Int)</span></span></span><br></pre></td></tr></table></figure><h1 id="线上运行"><a href="#线上运行" class="headerlink" title="线上运行"></a>线上运行</h1><ul><li>add submit，填入参数，在UI界面查看</li></ul>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建个人博客</title>
      <link href="/2019/12/06/Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/12/06/Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><blockquote><p>1.安装Git<br>2.安装Node.js<br>3.安装Hexo<br>4.GitHub创建个人仓库<br>5.生成SSH添加到GitHub<br>6.将hexo部署到GitHub<br>7.设置个人域名<br>8.发布文章</p></blockquote><h1 id="安装git，可以参考git安装那篇博客。"><a href="#安装git，可以参考git安装那篇博客。" class="headerlink" title="安装git，可以参考git安装那篇博客。"></a>安装git，可以参考git安装那篇博客。</h1><h1 id="NodeJs安装，可以参考网上博客，node-v-和-npm-v-有返回值即可。"><a href="#NodeJs安装，可以参考网上博客，node-v-和-npm-v-有返回值即可。" class="headerlink" title="NodeJs安装，可以参考网上博客，node -v 和 npm -v 有返回值即可。"></a>NodeJs安装，可以参考网上博客，node -v 和 npm -v 有返回值即可。</h1><h1 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli # hexo全局脚手架</span><br><span class="line">hexo -v # 查看hexo版本</span><br><span class="line">hexo init blog # 初始化博客</span><br><span class="line">cd blog # 进入hexo目录</span><br><span class="line">npm i # 安装依赖</span><br><span class="line">hexo c # 清除缓存</span><br><span class="line">hexo g # 生成静态页</span><br><span class="line">hexo s # 开启hexo服务，默认端口4000。可以通过localhost:4000查看</span><br><span class="line">hexo d # 部署到github或者coding</span><br></pre></td></tr></table></figure><blockquote><p>node_modules: 依赖包<br>public：存放生成的页面<br>scaffolds：生成文章的一些模板<br>source：用来存放你的文章<br>themes：主题<br>** _config.yml: 博客的配置文件**</p></blockquote><h1 id="github注册账户，参考https-my-oschina-net-chinahufei-blog-1577979"><a href="#github注册账户，参考https-my-oschina-net-chinahufei-blog-1577979" class="headerlink" title="github注册账户，参考https://my.oschina.net/chinahufei/blog/1577979"></a>github注册账户，参考<a href="https://my.oschina.net/chinahufei/blog/1577979" target="_blank" rel="noopener">https://my.oschina.net/chinahufei/blog/1577979</a></h1><h1 id="生成SSH添加到GitHub"><a href="#生成SSH添加到GitHub" class="headerlink" title="生成SSH添加到GitHub"></a>生成SSH添加到GitHub</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail&quot;</span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail&quot;</span><br><span class="line"># 把id_rsa.pub的文件内容添加到github的public key</span><br><span class="line">ssh -T git@github.com # 检查是否成功</span><br></pre></td></tr></table></figure><h1 id="将hexo部署到GitHub"><a href="#将hexo部署到GitHub" class="headerlink" title="将hexo部署到GitHub"></a>将hexo部署到GitHub</h1><ul><li>配置_config.yml文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https:&#x2F;&#x2F;github.com&#x2F;YourgithubName&#x2F;YourgithubName.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><blockquote><p>npm install hexo-deployer-git –save<br>hexo clean<br>hexo generate<br>hexo deploy</p></blockquote></li></ul><h1 id="设置个人域名"><a href="#设置个人域名" class="headerlink" title="设置个人域名"></a>设置个人域名</h1><ul><li>在域名服务商购买域名</li><li>在域名服务商账户做解析，解析到github对应的github page</li><li>在github page做域名绑定。(登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名)</li><li>也可以注册一个国内的coding，实现双域名解析，国内解析coding,国外解析github。不过这个需要域名服务商支持国内外双线路解析。</li></ul><h1 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo new newpapername</span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p><em>文章参考：<a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></em></p><h1 id="hexo-使用"><a href="#hexo-使用" class="headerlink" title="hexo 使用"></a>hexo 使用</h1><ul><li>文字说明<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% blockquote %&#125;</span><br><span class="line">1.安装Git</span><br><span class="line">2.安装Node.js</span><br><span class="line">3.安装Hexo</span><br><span class="line">4.GitHub创建个人仓库</span><br><span class="line">5.生成SSH添加到GitHub</span><br><span class="line">6.将hexo部署到GitHub</span><br><span class="line">7.设置个人域名</span><br><span class="line">8.发布文章</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure></li><li>markdown表格写法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| 水果        | 价格    |  数量  |</span><br><span class="line">| --------    | -----:  | :----: |</span><br><span class="line">| 香蕉        | $1      |   5    |</span><br><span class="line">| 苹果        | $1      |   6    |</span><br><span class="line">| 草莓        | $1      |   7    |</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16.1分布式集群搭建</title>
      <link href="/2019/12/04/CDH5.16.1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/12/04/CDH5.16.1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h1><ul><li>CM:<a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz</a></li><li>Parcel<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel</span><br><span class="line">http://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1</span><br><span class="line">http://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json</span><br></pre></td></tr></table></figure></li><li>JDK:<a href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a></li><li>Mysql:<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a></li><li>Mysql JDBC:<a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar" target="_blank" rel="noopener">http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar</a></li></ul><h1 id="二、集群初始化"><a href="#二、集群初始化" class="headerlink" title="二、集群初始化"></a>二、集群初始化</h1><ul><li>配置host<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 公⽹地址:</span></span><br><span class="line">106.15.234.222 hadoop001</span><br><span class="line">106.15.235.200 hadoop002</span><br><span class="line">106.15.234.239 hadoop003</span><br></pre></td></tr></table></figure></li><li>设置所有节点的hosts⽂件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo "172.19.7.96 hadoop001"&gt;&gt; /etc/hosts</span><br><span class="line">echo "172.19.7.98 hadoop002"&gt;&gt; /etc/hosts</span><br><span class="line">echo "172.19.7.97 hadoop003"&gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure></li><li>关闭所有节点的防⽕墙及清空规则<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure></li><li>关闭所有节点的selinux<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled</span><br><span class="line">设置后需要重启才能⽣效</span><br></pre></td></tr></table></figure></li><li>设置所有节点的时区⼀致及时钟同步<figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure></li><li>部署JDK<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/</span><br><span class="line"><span class="meta">#</span><span class="bash">切记必须修正所属⽤户及⽤户组</span></span><br><span class="line">chown -R root:root /usr/java/jdk1.8.0_45</span><br><span class="line">echo "export JAVA_HOME=/usr/java/jdk1.8.0_45" &gt;&gt; /etc/profile</span><br><span class="line">echo "export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;" &gt;&gt; /etc/profile</span><br><span class="line">source /etc/profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure></li><li>部署Mysql(见博客)</li><li>创建cm用户<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create database cmf DEFAULT CHARACTER SET utf8;</span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on cmf.* TO &#39;cmf&#39;@&#39;%&#39; IDENTIFIED BY &#39;Ruozedata123456!&#39;;</span><br><span class="line">grant all on amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;Ruozedata123456!&#39;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure></li><li>hadoop001部署Mysql JDBC<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java.jar /usr/share/java/</span><br></pre></td></tr></table></figure></li></ul><h1 id="三、部署CDH"><a href="#三、部署CDH" class="headerlink" title="三、部署CDH"></a>三、部署CDH</h1><ul><li>离线部署 cm server 以及agent<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1.1.所有节点创建⽬录及解压</span><br><span class="line">mkdir &#x2F;opt&#x2F;cloudera-manager</span><br><span class="line">tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C &#x2F;opt&#x2F;cloudera-manager&#x2F;</span><br><span class="line">1.2.所有节点修改agent的配置，指向server的节点hadoop001</span><br><span class="line">sed -i &quot;s&#x2F;server_host&#x3D;localhost&#x2F;server_host&#x3D;hadoop001&#x2F;g&quot; &#x2F;opt&#x2F;cloudera-manager&#x2F;cm5.16.1&#x2F;etc&#x2F;cloudera-scm-agent&#x2F;config.ini</span><br><span class="line">1.3.主节点修改server的配置:</span><br><span class="line">vi &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.1&#x2F;etc&#x2F;cloudera-scm-server&#x2F;db.properties</span><br><span class="line">com.cloudera.cmf.db.type&#x3D;mysql</span><br><span class="line">com.cloudera.cmf.db.host&#x3D;hadoop001</span><br><span class="line">com.cloudera.cmf.db.name&#x3D;cmf</span><br><span class="line">com.cloudera.cmf.db.user&#x3D;cmf</span><br><span class="line">com.cloudera.cmf.db.password&#x3D;Ruozedata123456!</span><br><span class="line">com.cloudera.cmf.db.setupType&#x3D;EXTERNAL</span><br><span class="line">1.4.所有节点创建⽤户</span><br><span class="line">useradd --system --home&#x3D;&#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.1&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br><span class="line">1.5.⽬录修改⽤户及⽤户组</span><br><span class="line">chown -R cloudera-scm:cloudera-scm &#x2F;opt&#x2F;cloudera-manager</span><br></pre></td></tr></table></figure></li><li>hadoop001节点部署离线parcel源<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2.1.部署离线parcel源</span><br><span class="line">mkdir -p &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line">cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;</span><br><span class="line">#切记cp时，重命名去掉1，不然在部署过程CM认为如上⽂件下载未完整，会持续下载</span><br><span class="line">cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha</span><br><span class="line">cp manifest.json &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;</span><br><span class="line">2.2.⽬录修改⽤户及⽤户组</span><br><span class="line">chown -R cloudera-scm:cloudera-scm &#x2F;opt&#x2F;cloudera&#x2F;</span><br></pre></td></tr></table></figure></li><li>所有节点创建软件安装⽬录、⽤户及⽤户组权限<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;opt&#x2F;cloudera&#x2F;parcels chown -R cloudera-scm:cloudera-scm &#x2F;opt&#x2F;cloudera&#x2F;</span><br></pre></td></tr></table></figure></li><li>hadoop001节点启动Server<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4.1.启动server</span><br><span class="line">&#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.1&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-server start</span><br><span class="line">4.2.阿⾥云web界⾯，设置该hadoop001节点防⽕墙放开7180端⼝</span><br><span class="line">4.3.等待1min，打开 http:&#x2F;&#x2F;hadoop001:7180 账号密码:admin&#x2F;admin</span><br><span class="line">4.4.假如打不开，去看server的log，根据错误仔细排查错误</span><br></pre></td></tr></table></figure></li><li>注意点<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">13.1.建议将&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness设置为最⼤值10。</span><br><span class="line">swappiness值控制操作系统尝试交换内存的积极；</span><br><span class="line">swappiness&#x3D;0：表示最⼤限度使⽤物理内存，之后才是swap空间；</span><br><span class="line">swappiness&#x3D;100：表示积极使⽤swap分区，并且把内存上的数据及时搬迁到swap空间；</span><br><span class="line">如果是混合服务器，不建议完全禁⽤swap，可以尝试降低swappiness。</span><br><span class="line">临时调整：</span><br><span class="line">sysctl vm.swappiness&#x3D;10</span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"># Adjust swappiness value</span><br><span class="line">vm.swappiness&#x3D;10</span><br><span class="line">EOF</span><br><span class="line">13.2.已启⽤透明⼤⻚⾯压缩，可能会导致重⼤性能问题，建议禁⽤此设置。</span><br><span class="line">临时调整：</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">永久调整：</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line"># Disable transparent_hugepage</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">EOF</span><br><span class="line"># centos7.x系统，需要为&quot;&#x2F;etc&#x2F;rc.d&#x2F;rc.local&quot;⽂件赋予执⾏权限</span><br><span class="line">chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产环境离线安装Mysql</title>
      <link href="/2019/12/02/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85Mysql/"/>
      <url>/2019/12/02/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85Mysql/</url>
      
        <content type="html"><![CDATA[<h1 id="Centos7-生产环境安装mysql"><a href="#Centos7-生产环境安装mysql" class="headerlink" title="Centos7 生产环境安装mysql"></a>Centos7 生产环境安装mysql</h1><h3 id="1-解压及创建目录"><a href="#1-解压及创建目录" class="headerlink" title="1.解压及创建目录"></a>1.解压及创建目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</span><br><span class="line">mv mysql-5.7.11-linux-glibc2.5-x86_64 mysql</span><br><span class="line">mv mysql /usr/local</span><br><span class="line">chown -R root:root mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在<span class="built_in">local</span>目录下</span></span><br><span class="line">mkdir mysql/arch mysql/data mysql/tmp</span><br></pre></td></tr></table></figure><h3 id="2-创建my-cnf"><a href="#2-创建my-cnf" class="headerlink" title="2.创建my.cnf"></a>2.创建my.cnf</h3><ul><li>vi /etc/my.cnf<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">port            &#x3D; 3306</span><br><span class="line">socket          &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;mysql.sock</span><br><span class="line">default-character-set&#x3D;utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port            &#x3D; 3306</span><br><span class="line">socket          &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;mysql.sock</span><br><span class="line"></span><br><span class="line">skip-slave-start</span><br><span class="line"></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size &#x3D; 256M</span><br><span class="line">sort_buffer_size &#x3D; 2M</span><br><span class="line">read_buffer_size &#x3D; 2M</span><br><span class="line">read_rnd_buffer_size &#x3D; 4M</span><br><span class="line">query_cache_size&#x3D; 32M</span><br><span class="line">max_allowed_packet &#x3D; 16M</span><br><span class="line">myisam_sort_buffer_size&#x3D;128M</span><br><span class="line">tmp_table_size&#x3D;32M</span><br><span class="line"></span><br><span class="line">table_open_cache &#x3D; 512</span><br><span class="line">thread_cache_size &#x3D; 8</span><br><span class="line">wait_timeout &#x3D; 86400</span><br><span class="line">interactive_timeout &#x3D; 86400</span><br><span class="line">max_connections &#x3D; 600</span><br><span class="line"></span><br><span class="line"># Try number of CPU&#39;s*2 for thread_concurrency</span><br><span class="line">#thread_concurrency &#x3D; 32 </span><br><span class="line"></span><br><span class="line">#isolation level and default engine </span><br><span class="line">default-storage-engine &#x3D; INNODB</span><br><span class="line">transaction-isolation &#x3D; READ-COMMITTED</span><br><span class="line"></span><br><span class="line">server-id  &#x3D; 1739</span><br><span class="line">basedir     &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql</span><br><span class="line">datadir     &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data</span><br><span class="line">pid-file     &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;hostname.pid</span><br><span class="line"></span><br><span class="line">#open performance schema</span><br><span class="line">log-warnings</span><br><span class="line">sysdate-is-now</span><br><span class="line"></span><br><span class="line">binlog_format &#x3D; ROW</span><br><span class="line">log_bin_trust_function_creators&#x3D;1</span><br><span class="line">log-error  &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;hostname.err</span><br><span class="line">log-bin &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;arch&#x2F;mysql-bin</span><br><span class="line">expire_logs_days &#x3D; 7</span><br><span class="line"></span><br><span class="line">innodb_write_io_threads&#x3D;16</span><br><span class="line"></span><br><span class="line">relay-log  &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-log</span><br><span class="line">relay-log-index &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-log.index</span><br><span class="line">relay_log_info_file&#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-log.info</span><br><span class="line"></span><br><span class="line">log_slave_updates&#x3D;1</span><br><span class="line">gtid_mode&#x3D;OFF</span><br><span class="line">enforce_gtid_consistency&#x3D;OFF</span><br><span class="line"></span><br><span class="line"># slave</span><br><span class="line">slave-parallel-type&#x3D;LOGICAL_CLOCK</span><br><span class="line">slave-parallel-workers&#x3D;4</span><br><span class="line">master_info_repository&#x3D;TABLE</span><br><span class="line">relay_log_info_repository&#x3D;TABLE</span><br><span class="line">relay_log_recovery&#x3D;ON</span><br><span class="line"></span><br><span class="line">#other logs</span><br><span class="line">#general_log &#x3D;1</span><br><span class="line">#general_log_file  &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;general_log.err</span><br><span class="line">#slow_query_log&#x3D;1</span><br><span class="line">#slow_query_log_file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow_log.err</span><br><span class="line"></span><br><span class="line">#for replication slave</span><br><span class="line">sync_binlog &#x3D; 500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#for innodb options </span><br><span class="line">innodb_data_home_dir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;</span><br><span class="line">innodb_data_file_path &#x3D; ibdata1:1G;ibdata2:1G:autoextend</span><br><span class="line"></span><br><span class="line">innodb_log_group_home_dir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;arch</span><br><span class="line">innodb_log_files_in_group &#x3D; 4</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_log_buffer_size &#x3D; 200M</span><br><span class="line"></span><br><span class="line">#根据生产需要，调整pool size </span><br><span class="line">innodb_buffer_pool_size &#x3D; 2G</span><br><span class="line">#innodb_additional_mem_pool_size &#x3D; 50M #deprecated in 5.6</span><br><span class="line">tmpdir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;tmp</span><br><span class="line"></span><br><span class="line">innodb_lock_wait_timeout &#x3D; 1000</span><br><span class="line">#innodb_thread_concurrency &#x3D; 0</span><br><span class="line">innodb_flush_log_at_trx_commit &#x3D; 2</span><br><span class="line"></span><br><span class="line">innodb_locks_unsafe_for_binlog&#x3D;1</span><br><span class="line"></span><br><span class="line">#innodb io features: add for mysql5.5.8</span><br><span class="line">performance_schema</span><br><span class="line">innodb_read_io_threads&#x3D;4</span><br><span class="line">innodb-write-io-threads&#x3D;4</span><br><span class="line">innodb-io-capacity&#x3D;200</span><br><span class="line">#purge threads change default(0) to 1 for purge</span><br><span class="line">innodb_purge_threads&#x3D;1</span><br><span class="line">innodb_use_native_aio&#x3D;on</span><br><span class="line"></span><br><span class="line">#case-sensitive file names and separate tablespace</span><br><span class="line">innodb_file_per_table &#x3D; 1</span><br><span class="line">lower_case_table_names&#x3D;1</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">max_allowed_packet &#x3D; 128M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">no-auto-rehash</span><br><span class="line">default-character-set&#x3D;utf8mb4</span><br><span class="line"></span><br><span class="line">[mysqlhotcopy]</span><br><span class="line">interactive-timeout</span><br><span class="line"></span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size &#x3D; 256M</span><br><span class="line">sort_buffer_size &#x3D; 256M</span><br><span class="line">read_buffer &#x3D; 2M</span><br><span class="line">write_buffer &#x3D; 2M</span><br></pre></td></tr></table></figure><h3 id="3-创建用户组及用户"><a href="#3-创建用户组及用户" class="headerlink" title="3.创建用户组及用户"></a>3.创建用户组及用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">groupadd -g 101 dba</span><br><span class="line">useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</span><br><span class="line">id mysqladmin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 一般不需要设置mysqladmin的密码，直接从root或者LDAP用户sudo切换</span></span><br><span class="line">passwd mysqladmin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 输入两次密码</span></span><br><span class="line">usermod -u 514 -g dba -G root -d /usr/local/mysql mysqladmin</span><br></pre></td></tr></table></figure><h3 id="4-copy-环境变量配置文件至mysqladmin用户的home目录中-为了以下步骤配置个人环境变量"><a href="#4-copy-环境变量配置文件至mysqladmin用户的home目录中-为了以下步骤配置个人环境变量" class="headerlink" title="4.copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量"></a>4.copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/skel/.* /usr/local/mysql  ###important</span><br></pre></td></tr></table></figure><h3 id="5-配置环境变量"><a href="#5-配置环境变量" class="headerlink" title="5.配置环境变量"></a>5.配置环境变量</h3></li><li>vi mysql/.bash_profile</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># .bash_profile</span><br><span class="line"># Get the aliases and functions</span><br><span class="line"></span><br><span class="line">if [ -f ~&#x2F;.bashrc ]; then</span><br><span class="line">        . ~&#x2F;.bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># User specific environment and startup programs</span><br><span class="line">export MYSQL_BASE&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql</span><br><span class="line">export PATH&#x3D;$&#123;MYSQL_BASE&#125;&#x2F;bin:$PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">unset USERNAME</span><br><span class="line"></span><br><span class="line">#stty erase ^H</span><br><span class="line">set umask to 022</span><br><span class="line">umask 022</span><br><span class="line">PS1&#x3D;&#96;uname -n&#96;&quot;:&quot;&#39;$USER&#39;&quot;:&quot;&#39;$PWD&#39;&quot;:&gt;&quot;; export PS1</span><br><span class="line"></span><br><span class="line">## end</span><br></pre></td></tr></table></figure><h3 id="6-赋权限和用户组，切换用户mysqladmin，安装"><a href="#6-赋权限和用户组，切换用户mysqladmin，安装" class="headerlink" title="6.赋权限和用户组，切换用户mysqladmin，安装"></a>6.赋权限和用户组，切换用户mysqladmin，安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chown  mysqladmin:dba /etc/my.cnf</span><br><span class="line">chmod  640 /etc/my.cnf</span><br><span class="line">chown -R mysqladmin:dba /usr/local/mysql</span><br><span class="line">chmod -R 755 /usr/local/mysql</span><br></pre></td></tr></table></figure><h3 id="7-配置服务及开机自启动"><a href="#7-配置服务及开机自启动" class="headerlink" title="7.配置服务及开机自启动"></a>7.配置服务及开机自启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将服务文件拷贝到init.d下，并重命名为mysql</span></span><br><span class="line">cp support-files/mysql.server /etc/rc.d/init.d/mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 赋予可执行权限</span></span><br><span class="line">chmod +x /etc/rc.d/init.d/mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除服务</span></span><br><span class="line">chkconfig --del mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加服务</span></span><br><span class="line">chkconfig --add mysql</span><br><span class="line">chkconfig --level 345 mysql on</span><br></pre></td></tr></table></figure><h3 id="8-安装libaio及安装mysql的初始db"><a href="#8-安装libaio及安装mysql的初始db" class="headerlink" title="8.安装libaio及安装mysql的初始db"></a>8.安装libaio及安装mysql的初始db</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">yum -y install libaio</span><br><span class="line">sudo su - mysqladmin</span><br><span class="line">bin/mysqld \</span><br><span class="line">--defaults-file=/etc/my.cnf \</span><br><span class="line">--user=mysqladmin \</span><br><span class="line">--basedir=/usr/local/mysql/ \</span><br><span class="line">--datadir=/usr/local/mysql/data/ \</span><br><span class="line">--initialize</span><br><span class="line"><span class="meta">#</span><span class="bash">　在初始化时如果加上 –initial-insecure，则会创建空密码的 root@localhost 账号，否则会创建带密码的 root@localhost 账号，密码直接写在 <span class="built_in">log</span>-error 日志文件中</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  （在5.6版本中是放在 ~/.mysql_secret 文件里，更加隐蔽，不熟悉的话可能会无所适从）</span></span><br><span class="line">cd data</span><br><span class="line">cat hostname.err |grep password # 查看临时密码，复制保存</span><br><span class="line">cd ..</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动mysql，2次回车</span></span><br><span class="line">/usr/local/mysql/bin/mysqld_safe --defaults-file=/etc/my.cnf &amp;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line">service mysql status</span><br></pre></td></tr></table></figure><h3 id="9-查看mysql是否成功启动"><a href="#9-查看mysql是否成功启动" class="headerlink" title="9.查看mysql是否成功启动"></a>9.查看mysql是否成功启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">exit; # 退出</span><br><span class="line">ps -ef | grep mysql  # 查看mysql端口</span><br><span class="line">netstat -nlp | grep 12022 # 查看端口对应的服务，能看到3306端口</span><br><span class="line">su - mysqladmin</span><br></pre></td></tr></table></figure><h3 id="10-登录及修改用户密码-重启"><a href="#10-登录及修改用户密码-重启" class="headerlink" title="10.登录及修改用户密码, 重启"></a>10.登录及修改用户密码, 重启</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p'*****' # *****是复制的临时密码</span><br><span class="line">alter user root@localhost identified by 'hufei123456';</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'hufei123456' ;</span><br><span class="line">flush privileges;</span><br><span class="line">exit;</span><br><span class="line">service mysql restart</span><br><span class="line">mysql -uroot -hufei123456</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows中大数据开发环境及工具配置</title>
      <link href="/2019/12/01/Windows%E4%B8%AD%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/"/>
      <url>/2019/12/01/Windows%E4%B8%AD%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h1><ul><li>下载地址: hk服务器/package/windows<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME ： C:\Program Files (x86)\Java\jdk1.8.0_60（此目录为jdk主目录）</span><br><span class="line">Path ：%JAVA_HOME%\bin</span><br><span class="line">classPath ： .;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar（别错过前面的点）</span><br><span class="line">Idea中添加JDK : File--&gt;Project Structure--&gt;SDKS--&gt;添加</span><br></pre></td></tr></table></figure></li></ul><h1 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h1><ul><li>下载地址: hk服务器/package/windows<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">MAVEN_HOME ： D:\Program Files\Apache\maven（此目录为Maven安装目录）</span><br><span class="line">Path ：%MAVEN_HOME%\bin\;</span><br><span class="line">MAVEN_OPTS : -Xms128m -Xmx512m -Duser.language&#x3D;zh -Dfile.encoding&#x3D;UTF-8</span><br><span class="line"># 配置本地仓库</span><br><span class="line">D:\Program Files\Apache\maven\conf\settings.xml</span><br><span class="line">&lt;localRepository&gt;&#x2F;path&#x2F;to&#x2F;local&#x2F;repo&lt;&#x2F;localRepository&gt;</span><br><span class="line"># 配置阿里云镜像</span><br><span class="line">&lt;mirrors&gt;  </span><br><span class="line">&lt;mirror&gt;  </span><br><span class="line">&lt;id&gt;mirrorId&lt;&#x2F;id&gt;  </span><br><span class="line">&lt;mirrorOf&gt;repositoryId&lt;&#x2F;mirrorOf&gt;  </span><br><span class="line">&lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt;  </span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;my.repository.com&#x2F;repo&#x2F;path&lt;&#x2F;url&gt;  </span><br><span class="line">&lt;&#x2F;mirror&gt;  </span><br><span class="line">&lt;mirror&gt;  </span><br><span class="line">&lt;id&gt;alimaven&lt;&#x2F;id&gt;  </span><br><span class="line">&lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt;  </span><br><span class="line">&lt;name&gt;aliyun maven&lt;&#x2F;name&gt;</span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt;  </span><br><span class="line">&lt;&#x2F;mirror&gt;  </span><br><span class="line">&lt;&#x2F;mirrors&gt; </span><br><span class="line"># Idea中配置Maven</span><br><span class="line">在setting中搜索maven</span><br><span class="line">配置Maven home directory:</span><br><span class="line">配置MAVEN_OPTS</span><br><span class="line">配置setting文件</span><br></pre></td></tr></table></figure></li></ul><h1 id="Maven命令"><a href="#Maven命令" class="headerlink" title="Maven命令"></a>Maven命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mvn clean 清除项目的生成结果</span><br><span class="line">mvn package 打包项目生成jar&#x2F;war文件</span><br><span class="line">mvn install 安装jar至本地库</span><br><span class="line">mvn compile 编译源代码</span><br><span class="line">mvn deploy 上传至私服</span><br><span class="line">mvn jar:jar 只打jar包</span><br><span class="line">mvn clean  install package -Dmaven.test.skip&#x3D;true #清理之前项目生成结果并构建然后将依赖包安装到本地仓库跳过测试</span><br><span class="line">mvn clean deploy package  -Dmaven.test.skip&#x3D;true #构建并将依赖放入私有仓库</span><br><span class="line">mvn --settings &#x2F;data&#x2F;settings.xml clean package -Dmaven.test.skip&#x3D;true #指定maven配置文件构建</span><br><span class="line">mvn install:install-file -DgroupId&#x3D;com.zebra -DartifactId&#x3D;ZSDK_CARD_API -Dversion&#x3D;v2.12.3782 -Dpackaging&#x3D;jar -Dfile&#x3D;E:\perslib\ZSDK_CARD_API.jar #安装特定jar包到仓库</span><br></pre></td></tr></table></figure><h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><ul><li>下载地址: <a href="https://www.scala-lang.org/download/" target="_blank" rel="noopener">https://www.scala-lang.org/download/</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME : D:\Program Files\scala</span><br><span class="line">Path ：%SCALA_HOME%\bin\</span><br><span class="line">Idea安装直接安装Scala插件</span><br></pre></td></tr></table></figure></li></ul><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><ul><li>winutils.ext下载地址: <a href="http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe" target="_blank" rel="noopener">http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe</a></li><li>hadoop.dll下载地址: <a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin" target="_blank" rel="noopener">https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin</a></li><li>hadoop.dll需要放在%HADOOP_HOME%\bin和C:\Windows\System32目录下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOMED:\software\hadoop2.6</span><br><span class="line">Path;%HADOOP_HOME%\bin</span><br></pre></td></tr></table></figure></li></ul><h1 id="log4j配置"><a href="#log4j配置" class="headerlink" title="log4j配置"></a>log4j配置</h1><ul><li>log4j.properties<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootCategory&#x3D;info, console</span><br><span class="line">log4j.appender.console&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.target&#x3D;System.err</span><br><span class="line">log4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern&#x3D;%p %c&#123;1&#125;: %m%n</span><br></pre></td></tr></table></figure></li></ul><h1 id="IDEA使用快捷键"><a href="#IDEA使用快捷键" class="headerlink" title="IDEA使用快捷键"></a>IDEA使用快捷键</h1><ul><li>代码自动提示<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.要保证File--&gt;Power On Save Mode是关闭状态</span><br><span class="line">2.File–&gt;Settings–&gt;Editor–&gt;General–&gt;Code Completion-&gt;Match case取消选中</span><br><span class="line">3.重启解决 问题</span><br></pre></td></tr></table></figure></li><li>打印-sout</li><li>局部变量-.var</li><li>成员变量-.field</li><li>格式化字符串-.format</li><li>判断非空-.nn,.null,.notnull</li><li>取反-.not</li><li>遍历-.for,.fori,.forr</li><li>返回值-.return</li><li>同步锁-.synchronized</li><li>Lambda表达式-.lamda</li></ul><h1 id="IDEA开发需要装的插件"><a href="#IDEA开发需要装的插件" class="headerlink" title="IDEA开发需要装的插件"></a>IDEA开发需要装的插件</h1><ul><li>Scala</li><li>Lombok(不用写实体类的get、set方法)</li></ul><h1 id="IDEA激活方法"><a href="#IDEA激活方法" class="headerlink" title="IDEA激活方法"></a>IDEA激活方法</h1><ul><li>Help–&gt;Edit Custom VM Option –&gt;添加一下内容<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2019.2.4\punk\jetbrains-agent.jar</span><br></pre></td></tr></table></figure></li><li>Help -&gt; Register–&gt;填入验证码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A82DEE284F-eyJsaWNlbnNlSWQiOiJBODJERUUyODRGIiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiJVbmxpbWl0ZWQgbGljZW5zZSB0aWxsIGVuZCBvZiB0aGUgY2VudHVyeS4iLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRE0iLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJBQyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiODkwNzA3MC8wIiwiZ3JhY2VQZXJpb2REYXlzIjowLCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-5epo90Xs7KIIBb8ckoxnB&#x2F;AZQ8Ev7rFrNqwFhBAsQYsQyhvqf1FcYdmlecFWJBHSWZU9b41kvsN4bwAHT5PiznOTmfvGv1MuOzMO0VOXZlc+edepemgpt+t3GUHvfGtzWFYeKeyCk+CLA9BqUzHRTgl2uBoIMNqh5izlDmejIwUHLl39QOyzHiTYNehnVN7GW5+QUeimTr&#x2F;koVUgK8xofu59Tv8rcdiwIXwTo71LcU2z2P+T3R81fwKkt34evy7kRch4NIQUQUno&#x2F;&#x2F;Pl3V0rInm3B2oFq9YBygPUdBUbdH&#x2F;KHROyohZRD8SaZJO6kUT0BNvtDPKF4mCT1saWM38jkw&#x3D;&#x3D;-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG&#x2F;PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg&#x2F;nYV31HLF7fJUAplI&#x2F;1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl&#x2F;GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4&#x2F;G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd&#x2F;GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt&#x2F;wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59&#x2F;THOT7NJQhr6AyLkhhJCdkzE2cob&#x2F;KouVp4ivV7Q3Fc6HX7eepHAAF&#x2F;DpxwgOrg9smX6coXLgfp0b1RU2u&#x2F;tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB&#x2F;40BjpMUrDRCeKuiBahC0DCoU&#x2F;4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV&#x2F;g&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3AGXEJXFK9-eyJsaWNlbnNlSWQiOiIzQUdYRUpYRks5IiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9XSwiaGFzaCI6IjEyNzk2ODc3LzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0&#x3D;-WGTHs6XpDhr+uumvbwQPOdlxWnQwgnGaL4eRnlpGKApEEkJyYvNEuPWBSrQkPmVpim&#x2F;8Sab6HV04Dw3IzkJT0yTc29sPEXBf69+7y6Jv718FaJu4MWfsAk&#x2F;ZGtNIUOczUQ0iGKKnSSsfQ&#x2F;3UoMv0q&#x2F;yJcfvj+me5Zd&#x2F;gfaisCCMUaGjB&#x2F;lWIPpEPzblDtVJbRexB1MALrLCEoDv3ujcPAZ7xWb54DiZwjYhQvQ+CvpNNF2jeTku7lbm5v+BoDsdeRq7YBt9ANLUKPr2DahcaZ4gctpHZXhG96IyKx232jYq9jQrFDbQMtVr3E+GsCekMEWSD&#x2F;&#x2F;dLT+HuZdc1sAIYrw&#x3D;&#x3D;-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG&#x2F;PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg&#x2F;nYV31HLF7fJUAplI&#x2F;1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl&#x2F;GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4&#x2F;G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd&#x2F;GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt&#x2F;wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59&#x2F;THOT7NJQhr6AyLkhhJCdkzE2cob&#x2F;KouVp4ivV7Q3Fc6HX7eepHAAF&#x2F;DpxwgOrg9smX6coXLgfp0b1RU2u&#x2F;tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB&#x2F;40BjpMUrDRCeKuiBahC0DCoU&#x2F;4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV&#x2F;g&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KNBB2QUUR1-eyJsaWNlbnNlSWQiOiJLTkJCMlFVVVIxIiwibGljZW5zZWVOYW1lIjoiZ2hib2tlIiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IiIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiQUMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRFBOIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBTIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRNIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkNMIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJTMCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSTSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJXUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSU1UiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiMTI3OTY4NzcvMCIsImdyYWNlUGVyaW9kRGF5cyI6NywiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ&#x3D;&#x3D;-1iV7BA&#x2F;baNqv0Q5yUnAphUmh66QhkDRX+qPL09ICuEicBqiPOBxmVLLCVUpkxhrNyfmOtat2LcHwcX&#x2F;NHkYXdoW+6aS0S388xe1PV2oodiPBhFlEaOac42UQLgP4EidfGQSvKwC9tR1zL5b2CJPQKZ7iiHh&#x2F;iKBQxP6OBMUP1T7j3Fe1rlxfYPc92HRZf6cO+C0+buJP5ERZkyIn5ZrVM4TEnWrRHbpL8SVNq4yqfc+NwoRzRSNC++81VDS3AXv9c91YeZJz6JXO7AokIk54wltr42FLNuKbozvB&#x2F;HCxV9PA5vIiM+kZY1K0w5ytgxEYKqA87adA7R5xL&#x2F;crpaMxHQ&#x3D;&#x3D;-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG&#x2F;PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg&#x2F;nYV31HLF7fJUAplI&#x2F;1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl&#x2F;GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4&#x2F;G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd&#x2F;GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt&#x2F;wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59&#x2F;THOT7NJQhr6AyLkhhJCdkzE2cob&#x2F;KouVp4ivV7Q3Fc6HX7eepHAAF&#x2F;DpxwgOrg9smX6coXLgfp0b1RU2u&#x2F;tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB&#x2F;40BjpMUrDRCeKuiBahC0DCoU&#x2F;4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV&#x2F;g&#x3D;&#x3D;</span><br></pre></td></tr></table></figure></li><li>然后重启就可以使用了</li></ul><h1 id="IDEA集成JDK、Maven"><a href="#IDEA集成JDK、Maven" class="headerlink" title="IDEA集成JDK、Maven"></a>IDEA集成JDK、Maven</h1><ul><li>setting–&gt;搜索java–&gt;Java 1.8</li><li>setting–&gt;搜索maven–&gt;User Seting File –&gt;选择maven的setting目录，选择即可</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>企业用自建Git服务器</title>
      <link href="/2019/12/01/%E4%BC%81%E4%B8%9A%E7%94%A8%E8%87%AA%E5%BB%BAGit%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2019/12/01/%E4%BC%81%E4%B8%9A%E7%94%A8%E8%87%AA%E5%BB%BAGit%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Centos7-git安装"><a href="#Centos7-git安装" class="headerlink" title="Centos7 git安装"></a>Centos7 git安装</h1><h3 id="一-安装"><a href="#一-安装" class="headerlink" title="一.安装"></a>一.安装</h3><ul><li>检查是否安装过:rpm -qa | grep git  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如果安装过，卸载</span></span><br><span class="line">rpm -e --nodeps git</span><br></pre></td></tr></table></figure></li><li>安装git:yum install git</li><li>git –version</li></ul><h3 id="二-配置ssh-key"><a href="#二-配置ssh-key" class="headerlink" title="二.配置ssh key"></a>二.配置ssh key</h3><ul><li>初始化ssk-key，回车  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C "hufei1459@163.com"</span><br></pre></td></tr></table></figure></li><li>eval “$(ssh-agent -s)”</li><li>ssh-add ~/.ssh/id_rsa<h3 id="在网站添加你的ssk-key"><a href="#在网站添加你的ssk-key" class="headerlink" title="在网站添加你的ssk-key"></a>在网站添加你的ssk-key</h3></li><li>/root/.ssh/id_rsa.pub<h3 id="测试连接"><a href="#测试连接" class="headerlink" title="测试连接"></a>测试连接</h3></li><li>ssh -T <a href="mailto:git@github.com">git@github.com</a></li></ul><hr><h1 id="使用Gogs搭建git服务器"><a href="#使用Gogs搭建git服务器" class="headerlink" title="使用Gogs搭建git服务器"></a>使用Gogs搭建git服务器</h1><h2 id="1-配置Gogs所需的环境"><a href="#1-配置Gogs所需的环境" class="headerlink" title="1.配置Gogs所需的环境"></a>1.配置Gogs所需的环境</h2><ul><li>安装nginx<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nginx</span><br></pre></td></tr></table></figure></li><li>安装git<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure></li><li>安装MySQL<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mysql-server # 安装mysql</span><br><span class="line">mysql -u root -p # 连接数据库</span><br><span class="line">SET GLOBAL storage_engine = 'InnoDB';  # 设置数据库模式为InnoDB</span><br><span class="line">CREATE DATABASE gogs CHARACTER SET utf8 COLLATE utf8_bin; # 创建数据库名字为gogs</span><br><span class="line">GRANT ALL PRIVILEGES ON gogs.* TO ‘root’@‘localhost’ IDENTIFIED BY 'YourPassword'; # 给数据库gogs赋权限</span><br><span class="line">FLUSH PRIVILEGES;  # 刷新</span><br><span class="line">QUIT； # 退出</span><br></pre></td></tr></table></figure></li><li>为Gogs创建单独的用户<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser git  # 创建用户git</span><br><span class="line">su git # 切换到git用户</span><br><span class="line">cd ~  # 切换到home目录</span><br><span class="line">wget https://dl.gogs.io/0.11.4/linux_amd64.zip # 下载gogs</span><br><span class="line">unzip linux_amd64.zip # 解压</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-配置与运行Gogs"><a href="#2-配置与运行Gogs" class="headerlink" title="2.配置与运行Gogs"></a>2.配置与运行Gogs</h2><ul><li>修改Gogs service配置文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /home/git/gogs/scripts/init/centos/gogs</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PATH=/sbin:/usr/sbin:/bin:/usr/bin</span><br><span class="line">DESC="Go Git Service"</span><br><span class="line">NAME=gogs</span><br><span class="line">SERVICEVERBOSE=yes</span><br><span class="line">PIDFILE=/var/run/$NAME.pid</span><br><span class="line">SCRIPTNAME=/etc/init.d/$NAME</span><br><span class="line">WORKINGDIR=/home/git/gogs #这个根据自己的目录修改</span><br><span class="line">DAEMON=$WORKINGDIR/$NAME</span><br><span class="line">DAEMON_ARGS="web"</span><br><span class="line">USER=git  #如果运行gogs不是用的这个用户，修改对应用户</span><br></pre></td></tr></table></figure></li><li>切换到root账户然后复制到/etc/init.d/<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /home/git/gogs/scripts/init/centos/gogs /etc/init.d/</span><br></pre></td></tr></table></figure></li><li>增加执行权限<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x /etc/init.d/gogs</span><br></pre></td></tr></table></figure></li><li>复制service<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /home/git/gogs/scripts/systemd/gogs.service /etc/systemd/system/</span><br></pre></td></tr></table></figure></li><li>启动Gogs<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service gogs start</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-浏览器配置gogs"><a href="#3-浏览器配置gogs" class="headerlink" title="3.浏览器配置gogs"></a>3.浏览器配置gogs</h2><ul><li>打开浏览器3000端口<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://*******:3000/install # 星号部分换成ip地址</span><br></pre></td></tr></table></figure></li><li>配置gogs.相关资料：<a href="https://gogs.io/docs/advanced/configuration_cheat_sheet" target="_blank" rel="noopener" title="gogs配置手册">gogs配置手册</a></li><li>gogs配置文件：/home/git/gogs/custom/conf/app.ini</li></ul><h2 id="4-nginx-反向代理"><a href="#4-nginx-反向代理" class="headerlink" title="4.nginx 反向代理"></a>4.nginx 反向代理</h2><ul><li>创建相应的配置文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/nginx/sites-enabled/gogs.conf</span><br></pre></td></tr></table></figure></li><li>添加<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">        server_name  code.chinahufei.com;</span><br><span class="line">        location / &#123;</span><br><span class="line">                proxy_pass http://127.0.0.1:3000/;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>如此，注册创建账号，登录即可。</li></ul><h3 id="如何在局域网搭建git服务器"><a href="#如何在局域网搭建git服务器" class="headerlink" title="如何在局域网搭建git服务器"></a>如何在局域网搭建git服务器</h3><ul><li><a href="https://www.cnblogs.com/hujunzheng/p/4970411.html" target="_blank" rel="noopener" title="https://www.cnblogs.com/hujunzheng/p/4970411.html">https://www.cnblogs.com/hujunzheng/p/4970411.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据Linux重要命令</title>
      <link href="/2019/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AELinux%E9%87%8D%E8%A6%81%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AELinux%E9%87%8D%E8%A6%81%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux-Basic-Command"><a href="#Linux-Basic-Command" class="headerlink" title="Linux Basic Command"></a>Linux Basic Command</h1><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ul><li><p>1.解压到特定目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf file.tar.gz -C &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line">tar -zcvf afterName.tar beforeName</span><br></pre></td></tr></table></figure></li><li><p>2.更改目录所属权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chown username fileName</span><br><span class="line">sudo chgrp userName fileName</span><br></pre></td></tr></table></figure></li><li><p>3.切换用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su - mysqladmin</span><br><span class="line">usermod -g groupName userName</span><br></pre></td></tr></table></figure></li><li><p>4.发送和下载文件到特定机器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r &#x2F;root&#x2F;demo root@123.25.23.108 &#x2F;root&#x2F;opt</span><br><span class="line">scp -P 22 -r root@123.25.23.108:&#x2F;home&#x2F;opt&#x2F; &#x2F;home&#x2F;my&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>5.登录特定机器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 22 root@ip192.168.0.3</span><br></pre></td></tr></table></figure></li><li><p>6.禁用IPv6地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo &#39;net.ipv6.conf.all.disable_ipv6 &#x3D; 1&#39; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sudo sysctl -p &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"># file: &#x2F;etc&#x2F;hosts</span><br><span class="line">#::1     localhost ip6-localhost ip6-loopback</span><br></pre></td></tr></table></figure></li><li><p>7.查看文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tail -f access.log     # 实时查看文件</span><br><span class="line">tail -200f access.log  # 查看最后200行</span><br></pre></td></tr></table></figure></li><li><p>8.创建软连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s rootLocation targetLocation</span><br></pre></td></tr></table></figure></li><li><p>9.搜索文本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find &#x2F;tmp -size +4M -size -5M &#39;*.log&#39; #在tmp目录下查找大于4M小于5M的日志文件</span><br><span class="line">grep -n &#39;.c.&#39; test.txt #在test.txt中搜索包含c的文本</span><br></pre></td></tr></table></figure></li><li><p>10.端口和进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep nginx#查看进程pid</span><br><span class="line">netstat -nap | grep 进程pid#通过pid查看端口</span><br><span class="line">netstat -nap | grep 端口号#通过端口查看进程</span><br><span class="line">lsof -i:端口号#通过端口查看进程</span><br><span class="line">telnet 10.20.66.37 8090         # ping端口</span><br><span class="line">ping 192.168.0.2                # ping ip</span><br></pre></td></tr></table></figure></li><li><p>11.进程相关</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">last&#x2F;lastlog: 查看用户最近登录情况</span><br><span class="line">df: 查看硬盘使用情况</span><br><span class="line">du: 查看文件大小</span><br><span class="line">free: 查看内存使用情况</span><br><span class="line">&#x2F;proc: 查看文件系统</span><br><span class="line">ls &#x2F;var&#x2F;log&#x2F;: 查看日志</span><br><span class="line">tail &#x2F;var&#x2F;log&#x2F;messages: 查看系统报错日志</span><br><span class="line">top: 查看进程</span><br><span class="line">kill 1234&#x2F;kill -9 4333: 结束进程</span><br></pre></td></tr></table></figure></li><li><p>12.上传下载文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install lrzsz -y # 安装</span><br><span class="line">sz filename #下载文件到本地</span><br><span class="line">rz 上传文件</span><br></pre></td></tr></table></figure></li></ul><h3 id="防火墙"><a href="#防火墙" class="headerlink" title="防火墙"></a>防火墙</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1.通用</span><br><span class="line">/sbin/iptables -I INPUT -p tcp --dport 27017 -j ACCEPT  </span><br><span class="line">/etc/rc.d/init.d/iptables save</span><br><span class="line">2.centos7 临时操作</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">注：如果centos7下需要使用iptables</span><br><span class="line">yum install iptables-services,systemctl enable iptables,systemctl start iptables</span><br><span class="line">3.centos6 临时操作</span><br><span class="line">service iptables stop/start/status/restart</span><br><span class="line">4.查看某端口的防火墙状态</span><br><span class="line">firewall-cmd --query-port=666/tcp</span><br><span class="line">5.查看防火墙开放的端口</span><br><span class="line">iptables -L -n</span><br><span class="line"><span class="meta">#</span><span class="bash"> 其他参考</span></span><br><span class="line">1. 查看已打开的端口 # netstat -anp</span><br><span class="line">2. 查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp</span><br><span class="line">若此提示 FirewallD is not running</span><br><span class="line">表示为不可知的防火墙 需要查看状态并开启防火墙</span><br><span class="line"></span><br><span class="line">3. 查看防火墙状态 # systemctl status firewalld</span><br><span class="line">running 状态即防火墙已经开启</span><br><span class="line">dead 状态即防火墙未开启</span><br><span class="line">4. 开启防火墙，# systemctl start firewalld 没有任何提示即开启成功</span><br><span class="line">5. 开启防火墙 # service firewalld start</span><br><span class="line">关闭防火墙 # systemctl stop firewalld</span><br><span class="line">centos7.3 上述方式可能无法开启，可以先#systemctl unmask firewalld.service 然后 # systemctl start firewalld.service</span><br><span class="line"></span><br><span class="line">6. 查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp 提示no表示未开</span><br><span class="line">7. 开永久端口号 firewall-cmd --add-port=666/tcp --permanent 提示 success 表示成功</span><br><span class="line">8. 重新载入配置 # firewall-cmd --reload 比如添加规则之后，需要执行此命令</span><br><span class="line">9. 再次查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp 提示yes表示成功</span><br><span class="line">10. 若移除端口 # firewall-cmd --permanent --remove-port=666/tcp</span><br><span class="line"></span><br><span class="line">11. 修改iptables 有些版本需要安装iptables-services # yum install iptables-services 然后修改进目录 /etc/sysconfig/iptables 修改内容</span><br></pre></td></tr></table></figure><h3 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1.搜索进程</span><br><span class="line">ps -ef | grep java</span><br><span class="line">2.根据用户过滤进程</span><br><span class="line">ps -u root</span><br><span class="line">3.根据cpu来筛选进程</span><br><span class="line">ps -aux --sort -pcpu | less</span><br><span class="line">4.根据内存来筛选 进程</span><br><span class="line">ps -aux --sort -pmem | less</span><br><span class="line">ps -aux --sort -pcpu,+pmem | head -n 10</span><br><span class="line">5.通过进程名和PID过滤</span><br><span class="line">ps -C java</span><br><span class="line">6.根据线程过滤</span><br><span class="line">ps -L 1213</span><br><span class="line">7.查看详细信息</span><br><span class="line">ps -f -C java</span><br><span class="line">8.显示为树形</span><br><span class="line">pstree</span><br><span class="line">9.显示安全信息(登录服务器记录)</span><br><span class="line">ps -eo pid,user,args</span><br><span class="line">10.实时监控进程状态</span><br><span class="line">watch -n 1 ‘ps -aux --sort -pmem, -pcpu’</span><br><span class="line">watch -n 1 ‘ps -aux --sort -pmem, -pcpu | head 20’</span><br><span class="line">11.查看前10个进程</span><br><span class="line">ps -aux |head -n 10</span><br><span class="line">12.查看后10个进程</span><br><span class="line">ps -aux |tail -n 10</span><br><span class="line">13.查看内存使用的前10个进程</span><br><span class="line">ps -aux |sort -nk3|head -n 10</span><br></pre></td></tr></table></figure><h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><h4 id="前五行是统计信息区"><a href="#前五行是统计信息区" class="headerlink" title="前五行是统计信息区"></a>前五行是统计信息区</h4><ul><li>第一行(任务队列信息)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">01:06:48 : 当前时间</span><br><span class="line">up 1:22 : 系统运行时间，格式为时:分</span><br><span class="line">1 user : 当前登录用户数</span><br><span class="line">load average: 0.06, 0.60, 0.48 : 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。</span><br></pre></td></tr></table></figure></li><li>第二、三行(进程和CPU)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Tasks: 29 total  : 总进程数</span><br><span class="line">1 running        : 正在运行的进程数</span><br><span class="line">28 sleeping      : 睡眠的进程数</span><br><span class="line">0 stopped        : 停止的进程数</span><br><span class="line">0 zombie         : 僵尸进程数</span><br><span class="line">Cpu(s): 0.3% us  : 用户空间占用CPU百分比</span><br><span class="line">1.0% sy         : 内核空间占用CPU百分比</span><br><span class="line">0.0% ni         : 用户进程空间内改变过优先级的进程占用CPU百分比</span><br><span class="line">98.7% id         : 空闲CPU百分比</span><br><span class="line">0.0% wa         : 等待输入输出的CPU时间百分比</span><br></pre></td></tr></table></figure></li><li>第四、五行(内存信息)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Mem: 191272k total :物理内存总量</span><br><span class="line">173656k used   :    使用的物理内存总量</span><br><span class="line">17616k free       :    空闲内存总量</span><br><span class="line">22052k buffers   :    用作内核缓存的内存量</span><br><span class="line">Swap: 192772k total:交换区总量</span><br><span class="line">0k used           :    使用的交换区总量</span><br><span class="line">192772k free   :    空闲交换区总量</span><br><span class="line">123988k cached   :    缓冲的交换区总量。</span><br><span class="line">内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，</span><br><span class="line">该数值即为这些内容已存在于内存中的交换区的大小。</span><br><span class="line">相应的内存再次被换出时可不必再对交换区写入。</span><br></pre></td></tr></table></figure><h4 id="列表是进程信息区"><a href="#列表是进程信息区" class="headerlink" title="列表是进程信息区"></a>列表是进程信息区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">PID    :进程id</span><br><span class="line">PPID:父进程id</span><br><span class="line">RUSER:Real user name</span><br><span class="line">UID    :进程所有者的用户id</span><br><span class="line">USER:进程所有者的用户名</span><br><span class="line">GROUP:进程所有者的组名</span><br><span class="line">TTY    :启动进程的终端名。不是从终端启动的进程则显示为</span><br><span class="line">PR    :优先级</span><br><span class="line">NI    :nice值。负值表示高优先级，正值表示低优先级</span><br><span class="line">P    :最后使用的CPU，仅在多CPU环境下有意义</span><br><span class="line">%CPU:上次更新到现在的CPU时间占用百分比</span><br><span class="line">TIME:进程使用的CPU时间总计，单位秒</span><br><span class="line">TIME+:进程使用的CPU时间总计，单位1&#x2F;100秒</span><br><span class="line">%MEM:进程使用的物理内存百分比</span><br><span class="line">VIRT:进程使用的虚拟内存总量，单位kb。VIRT&#x3D;SWAP+RES</span><br><span class="line">SWAP:进程使用的虚拟内存中，被换出的大小，单位kb。</span><br><span class="line">RES    :进程使用的、未被换出的物理内存大小，单位kb。RES&#x3D;CODE+DATA</span><br><span class="line">CODE:可执行代码占用的物理内存大小，单位kb</span><br><span class="line">DATA:可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb</span><br><span class="line">SHR    :共享内存大小，单位kb</span><br><span class="line">nFLT:页面错误次数</span><br><span class="line">nDRT:最后一次写入到现在，被修改过的页面数。</span><br><span class="line">S    :进程状态。D&#x3D;不可中断的睡眠状态 R&#x3D;运行 S&#x3D;睡眠 T&#x3D;跟踪&#x2F;停止 Z&#x3D;僵尸进程</span><br><span class="line">COMMAND:命令名&#x2F;命令行</span><br><span class="line">WCHAN:若该进程在睡眠，则显示睡眠中的系统函数名</span><br><span class="line">Flags:任务标志，参考 sched.h</span><br></pre></td></tr></table></figure></li></ul><h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.查找当前目录以及子目录以.c结尾的文件</span><br><span class="line">find . -name &#39;*.c&#39;</span><br><span class="line">2.列出当前目录及其子目录中的所有一般文件</span><br><span class="line">find . -type f</span><br><span class="line">3.列出当前目录及其子目录下所有最近20天内更新过的文件</span><br><span class="line">find . -ctime -20</span><br><span class="line">4.查找&#x2F;var&#x2F;log目录中更改时间在7日以前的普通文件，并在删除之前询问它们</span><br><span class="line">find &#x2F;var&#x2F;log -type f -mtime +7 ok rm &#123;&#125; \</span><br><span class="line">5.查找当前目录中文件是644组合的文件列表</span><br><span class="line">find . -type f -perm 644 -exec ls -l &#123;&#125; \;</span><br><span class="line">6.查找系统中所有文件长度为0的为普通文件，并列出它们的完整路径</span><br><span class="line">find &#x2F; -type f –size 0 –exec ls –l &#123;&#125; \</span><br></pre></td></tr></table></figure><h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.查找包含match_pattern的所有行</span><br><span class="line">grep match_pattern file_name</span><br><span class="line">2.多个文件查找</span><br><span class="line">grep match_pattern file1 file2 file3</span><br><span class="line">3.输出除之外的所有行</span><br><span class="line">grep -v “match_pattern” file_name</span><br><span class="line">4.使用正则表达式</span><br><span class="line">grep -E &#39;[1-9]+&#39;</span><br><span class="line">5.只输出文件中匹配的部分</span><br><span class="line">echo this is a test line. | grep -o -E &#39;[a-z]+\.&#39;</span><br><span class="line">6.统计文本中包含匹配字符串的行数</span><br><span class="line">grep -c “text” file_name</span><br><span class="line">7.在多级目录中对文本进行递归搜索</span><br><span class="line">grep “text” . -r -n</span><br></pre></td></tr></table></figure><h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><ul><li>动作说明: a-新增，c-取代，d-删除，i-插入，p-打印，s-取代</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1.添加-在testfile文件的第四行后添加一行，并将结果输出到标准输出</span><br><span class="line">sed -e 4a\newLine testfile</span><br><span class="line">2.删除-将&#x2F;etc&#x2F;passwd 的内容列出并且列印行号，同时，请将第 2~5 行删除</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;2,5d&#39;</span><br><span class="line">3.删除第 3 到最后一行</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;3,$d&#39;</span><br><span class="line">4.在第二行后(亦即是加在第三行)加上『drink tea?』字样</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;2a drink tea&#39;</span><br><span class="line">5.在第二行前</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;2i drink tea&#39;</span><br><span class="line">6.要增加两行以上，在第二行后面加入两行字，例如 Drink tea or ..... 与 drink beer?</span><br><span class="line"> nl &#x2F;etc&#x2F;passwd | sed &#39;2a Drink tea or ......\</span><br><span class="line">&gt; drink beer ?&#39;</span><br><span class="line">7.将第2-5行的内容取代成为『No 2-5 number』</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;2,5c No 2-5 number&#39;</span><br><span class="line">8.仅列出 &#x2F;etc&#x2F;passwd 文件内的第 5-7 行</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed -n &#39;5,7p&#39;</span><br><span class="line">9.搜索 &#x2F;etc&#x2F;passwd有root关键字的行</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed &#39;&#x2F;root&#x2F;p&#39;</span><br><span class="line">10.删除&#x2F;etc&#x2F;passwd所有包含root的行，其他行输出</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed  &#39;&#x2F;root&#x2F;d&#39;</span><br><span class="line">11.搜索&#x2F;etc&#x2F;passwd,找到root对应的行，执行后面花括号中的一组命令，每个命令之间用分号分隔，这里把bash替换为blueshell，再输出这行</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed -n &#39;&#x2F;root&#x2F;&#123;s&#x2F;bash&#x2F;blueshell&#x2F;;p;q&#125;&#39; </span><br><span class="line">12.将 IP 前面的部分予以删除</span><br><span class="line">&#x2F;sbin&#x2F;ifconfig eth0 | grep &#39;inet addr&#39; | sed &#39;s&#x2F;^.*addr:&#x2F;&#x2F;g&#39;</span><br><span class="line">13.将 IP 后面的部分予以删除</span><br><span class="line">&#x2F;sbin&#x2F;ifconfig eth0 | grep &#39;inet addr&#39; | sed &#39;s&#x2F;^.*addr:&#x2F;&#x2F;g&#39; | sed &#39;s&#x2F;Bcast.*$&#x2F;&#x2F;g&#39;</span><br><span class="line">14.一条sed命令，删除&#x2F;etc&#x2F;passwd第三行到末尾的数据，并把bash替换为blueshell</span><br><span class="line">nl &#x2F;etc&#x2F;passwd | sed -e &#39;3,$d&#39; -e &#39;s&#x2F;bash&#x2F;blueshell&#x2F;&#39;</span><br><span class="line">15.利用 sed 将 regular_express.txt 内每一行结尾若为 . 则换成 !</span><br><span class="line">sed -i &#39;s&#x2F;\.$&#x2F;\!&#x2F;g&#39; regular_express.txt</span><br></pre></td></tr></table></figure><h3 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1.搜索&#x2F;etc&#x2F;passwd有root关键字的所有行</span><br><span class="line">awk  &#39;&#x2F;root&#x2F;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">2.搜索&#x2F;etc&#x2F;passwd有root关键字的所有行，并显示对应的shell</span><br><span class="line">awk -F: &#39;&#x2F;root&#x2F; &#123;print $7&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">3.统计&#x2F;etc&#x2F;passwd:文件名，每行的行号，每行的列数，对应的完整行内容</span><br><span class="line">awk -F: &#39;&#123;printf (&quot;filename:%10s, linenumber:%3s,column:%3s,content:%3f\n&quot;,FILENAME,NR,NF,$0)&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">4.打印&#x2F;etc&#x2F;passwd&#x2F;的第二行信息</span><br><span class="line">awk -F: &#39;NR&#x3D;&#x3D;2&#123;print &quot;filename: &quot;FILENAME, $0&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">5.指定特定的分隔符，查询第一列</span><br><span class="line">awk -F &quot;:&quot; &#39;&#123;print $1&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">6.指定特定的分隔符，查询最后一列</span><br><span class="line">awk -F &quot;:&quot; &#39;&#123;print $NF&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">7.指定特定的分隔符，查询倒数第二列</span><br><span class="line">awk -F &quot;:&quot; &#39;&#123;print $NF-1&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">8.获取第12到31行的第一列的信息</span><br><span class="line">awk -F &quot;:&quot;  &#39;&#123;if(NR&lt;31 &amp;&amp; NR &gt;12) print $1&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">9.查看最近登录最多的IP信息</span><br><span class="line">last | awk &#39;&#123;S[$3]++&#125; END&#123;for(a in S ) &#123;print S[a],a&#125;&#125;&#39; |uniq| sort -rh</span><br><span class="line">10.利用正则过滤多个空格</span><br><span class="line">ifconfig |grep eth* | awk -F &#39;[ ]+&#39; &#39;&#123;print $1&#125;&#39;&lt;br&gt;&lt;br&gt;</span><br><span class="line">10.统计某个文件夹下的大于100k文件的数量和总和</span><br><span class="line">ls -l|awk &#39;&#123;if($5&gt;100)&#123;count++; sum+&#x3D;$5&#125;&#125; &#123;print &quot;Count:&quot; count,&quot;Sum: &quot; sum&#125;&#39;</span><br><span class="line">11.统计显示&#x2F;etc&#x2F;passwd的账户</span><br><span class="line">awk -F: &#39;&#123;count++;&#125; END&#123;print count&#125;&#39; &#x2F;etc&#x2F;passwd        </span><br><span class="line">cat &#x2F;etc&#x2F;passwd|wc -l</span><br><span class="line">awk -F &#39;:&#39; &#39;BEGIN &#123;count&#x3D;0;&#125; &#123;name[count] &#x3D; $1;count++;&#125;; END&#123;for (i &#x3D; 0; i &lt; NR; i++) print i, name[i]&#125;&#39; &#x2F;etc&#x2F;passwd</span><br></pre></td></tr></table></figure><h3 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.获取有多少个字符</span><br><span class="line">wc functest.sh -c</span><br><span class="line">2.使用&#39; &#39;进行分割，获取第一个参数</span><br><span class="line">wc functest.sh -c | cut -d &#39; &#39; -f 1</span><br><span class="line">3.将PATH变量取出， 我要找出第5个变量</span><br><span class="line">echo $PATH | cut -d &#39;:&#39; -f 5</span><br><span class="line">4.将PATH变量取出， 我要找出第3和5个变量</span><br><span class="line">echo $PATH | cut -d &#39;:&#39; -f 3,5</span><br><span class="line">5.将PATH变量取出， 我要找出第3到最后一个</span><br><span class="line">echo $PATH | cut -d &#39;:&#39; -f 3-</span><br><span class="line">6.将PATH变量取出， 我要找出第3到第5个</span><br><span class="line">echo $PATH | cut -d &#39;:&#39; -f 3-5</span><br><span class="line">7.只显示&#x2F;etc&#x2F;passwd的用户和shell</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | cut -d &#39;:&#39; -f 1,7</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.对&#x2F;etc&#x2F;passwd 的账号进行排序</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort</span><br><span class="line">2.对&#x2F;etc&#x2F;passwd 按照第三列进行排序(默认安装字母顺序排序)</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort -t &#39;:&#39; -k 3</span><br><span class="line">3.对&#x2F;etc&#x2F;passwd 按照第三列进行排序（按照数字排序)</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort -t &#39;:&#39; -k 3n  # 升序</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort -t &#39;:&#39; -k 3nr # 倒序</span><br><span class="line">4.对&#x2F;etc&#x2F;passwd  先按照第六个域的第2个字符到第4个字符排序，再按照第一个域倒序</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort -t &#39;:&#39; -k 6.2,6.4 -k 1r</span><br><span class="line">5.对&#x2F;etc&#x2F;passwd 按照第3个域排序后去重</span><br><span class="line">cat &#x2F;etc&#x2F;passwd | sort -t &#39;:&#39; -k 3 -u</span><br></pre></td></tr></table></figure><h3 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat word.txt | sort | uniq</span><br></pre></td></tr></table></figure><h3 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.统计行数</span><br><span class="line">wc -l &#x2F;etc&#x2F;passwd</span><br><span class="line">2.统计单词书</span><br><span class="line">wc -w &#x2F;etc&#x2F;passwd</span><br><span class="line">3.统计字符数</span><br><span class="line">wc -m &#x2F;etc&#x2F;passwd</span><br></pre></td></tr></table></figure><h3 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h3><ul><li>curl参数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. -X --request    [GET|POST|PUT|DELETE|…]    使用指定的HTTP method 发出指定的request</span><br><span class="line">2. -H --header    “XX:XXX”    设定request的header</span><br><span class="line">curl -i -H &quot;Content-Type: application&#x2F;json&quot; http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br><span class="line">-i --include    显示response的header</span><br><span class="line">3. -d --data    “XX&#x3D;XXX”    设定HTTP parameters</span><br><span class="line">curl -i -X POST -d “param1&#x3D;value1&amp;m2&#x3D;value2”</span><br><span class="line">4. -v --verbose    输出比较多的信息</span><br><span class="line">5. -u --user    “XX:XXX”    使用者帐密</span><br><span class="line">curl -i --user username:password http:&#x2F;&#x2F;www.rest.com&#x2F;api&#x2F;foo&#39;</span><br><span class="line">6. -b --cookie    cookie文件路径   使用cookie</span><br><span class="line">curl -i --header &quot;Accept:application&#x2F;json&quot; -X GET -b ~&#x2F;cookie.txt http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br><span class="line">7.session认证</span><br><span class="line">curl -X GET &#39;http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1&#39; --header &#39;sessionid:1234567890987654321&#39;</span><br><span class="line">8. 文件上传</span><br><span class="line">curl -i -X POST -F &#39;file&#x3D;@&#x2F;User&#x2F;my_file.txt&#39; -F &#39;name&#x3D;file_name&#39;</span><br></pre></td></tr></table></figure></li><li>wget获取http资源<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget  --post-data&#x3D;&quot;xx&#x3D;xxx&quot; http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br></pre></td></tr></table></figure></li><li>curl实例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 发送 json格式</span><br><span class="line">curl -H &#39;content-type: application&#x2F;json&#39; -X POST -d &#39;&#123;&quot;name&quot;:&quot;shfbjsf&quot;&#125;&#39; http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br><span class="line">2. 发送json文件</span><br><span class="line">curl -X POST -H &#39;content-type: application&#x2F;json&#39;  -d @&#x2F;apps&#x2F;jsonfile.json http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br><span class="line">3. 发送xml</span><br><span class="line">curl -H &#39;content-type: application&#x2F;xml&#39; -X POST -d &#39;&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;name&gt;aaa&lt;&#x2F;name&gt;&#39; http:&#x2F;&#x2F;chinahufei.com&#x2F;api&#x2F;v1&#x2F;banner&#x2F;1</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux&amp;Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop编程问题合集</title>
      <link href="/2019/01/13/hadoop%E7%BC%96%E7%A8%8B%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/"/>
      <url>/2019/01/13/hadoop%E7%BC%96%E7%A8%8B%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<ul><li>winutils</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark社区博客合集</title>
      <link href="/2019/01/06/Spark%E7%A4%BE%E5%8C%BA%E5%8D%9A%E5%AE%A2%E5%90%88%E9%9B%86/"/>
      <url>/2019/01/06/Spark%E7%A4%BE%E5%8C%BA%E5%8D%9A%E5%AE%A2%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark社区博客合集"><a href="#Spark社区博客合集" class="headerlink" title="Spark社区博客合集"></a>Spark社区博客合集</h1><ul><li><ol><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483881&idx=1&sn=b2e826162de4d94dcf0119556e2d7f50&chksm=cef37c68f984f57e1c672a72e90bdd09a2a9cf2239a9d472fe9b9caebcc381e8a47c2b17b9f8&scene=21#wechat_redirect" target="_blank" rel="noopener" title="浅谈 Spark 的多语言支持">浅谈 Spark 的多语言支持</a></li></ol></li><li><ol start="2"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483911&idx=1&sn=454c75b8a75a2cc539ede95f8d6e816d&chksm=cef37f86f984f6909696851320b3ecbc9fcf1acf41d49719ae93ba992dea514eb32dad0e0f63&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Apache Spark3.0什么样？一文读懂Apache Spark最新技术发展与展望">Apache Spark3.0什么样？一文读懂Apache Spark最新技术发展与展望</a></li></ol></li><li><ol start="3"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483668&idx=1&sn=cb7517ba0ef98654ab0b1cf2ab484b8e&chksm=cef37c95f984f583df8a4550e9885800f444325e82ce249257f353ad1c7e650032ef2791653f&scene=21#wechat_redirect" target="_blank" rel="noopener" title="基于Spark SQL实现对HDFS操作的实时监控报警">基于Spark SQL实现对HDFS操作的实时监控报警</a></li></ol></li><li><ol start="4"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483689&idx=1&sn=c706af287c3f729bfb00c7b143021fbd&chksm=cef37ca8f984f5be39eb2bcae63b16d4c826227a34a2af4d6ed295f67a21796f9588bedb8912&scene=21#wechat_redirect" target="_blank" rel="noopener" title="通过Spark SQL实时归档SLS数据">通过Spark SQL实时归档SLS数据</a></li></ol></li><li><ol start="5"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483704&idx=1&sn=76e442e7f0a03c85abf2cc0ecb73b8e6&chksm=cef37cb9f984f5afd0aff253dcd1cb33ec48a1a610add176ffa236a4f8050bbdb9414333ccd5&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用Spark SQL进行流式机器学习计算（上）">使用Spark SQL进行流式机器学习计算（上）</a></li></ol></li><li><ol start="6"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483715&idx=1&sn=4f1b94a5d4b45e51f45b5ea04e1b5754&chksm=cef37cc2f984f5d434752b8c74c9657d46488fbefff3e5dc1868d44832d38b29413bcdfc4843&scene=21#wechat_redirect" target="_blank" rel="noopener" title="通过WebUI查看Structured Streaming作业统计信息">通过WebUI查看Structured Streaming作业统计信息</a></li></ol></li><li><ol start="7"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484099&idx=1&sn=90d9144115b857ff27cb2fa09f1cd232&chksm=cef37f42f984f65493868fee2a09f571a8215bd6f0bf1efd06b0cf8c443308d392f1b86a74f7&scene=21#wechat_redirect" target="_blank" rel="noopener" title="现代流式计算的基石：Google DataFlow">现代流式计算的基石：Google DataFlow</a></li></ol></li><li><ol start="8"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483675&idx=1&sn=b09c5123162a046268a6657d9ca7b42c&chksm=cef37c9af984f58c0581aaf1fa11c58d0e402b9716dd3c463d46176424c5678cdd93a92110de&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark Streaming 框架在 5G 中的应用">Spark Streaming 框架在 5G 中的应用</a></li></ol></li><li><ol start="9"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484033&idx=1&sn=1d923e1be4dc45e5c9da75b79dffd8b4&chksm=cef37f00f984f61602812facac0b24170963a947588a3dc4aa798d3f2600b912de5ba9847868&scene=21#wechat_redirect" target="_blank" rel="noopener" title="是时候放弃 Spark Streaming, 转向 Structured Streaming 了">是时候放弃 Spark Streaming, 转向 Structured Streaming 了</a></li></ol></li><li><ol start="10"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483761&idx=1&sn=db22e6a7378fe8ac0d40864816fa424e&chksm=cef37cf0f984f5e6f31ff140ca7de7b82f2a30cbae198eb7bc9138e4ef2e19ba344d83d5fa12&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用Spark Streaming SQL基于时间窗口进行数据统计">使用Spark Streaming SQL基于时间窗口进行数据统计</a></li></ol></li></ul><hr><ul><li><ol start="11"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484390&idx=1&sn=8abd8c464fac305f50962d58331ae7d7&chksm=cef37e67f984f771ea3ef605f85a96a3b37653d55288b6a79aa01b43441c38ef3c8e3cbe9a78&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark-StructuredStreaming checkpointLocation分析、优化耗时">Spark-StructuredStreaming checkpointLocation分析、优化耗时</a></li></ol></li><li><ol start="12"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484253&idx=1&sn=9d3fac47921def314049ab5644e2f785&chksm=cef37edcf984f7ca034a1a9074d752afa9b62756d3dbedb0c2bdb21f2f026e7a5e005ec1db03&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用Spark Streaming SQL进行PV/UV统计">使用Spark Streaming SQL进行PV/UV统计</a></li></ol></li><li><ol start="13"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484515&idx=1&sn=eee77f3d2d1a20ed77c3edd3639ad275&chksm=cef379e2f984f0f498277fcc9858c715deef84f39f5b31b01325a9aef73e89568eb222887f13&scene=21#wechat_redirect" target="_blank" rel="noopener" title="通过Spark Streaming作业处理Kafka数据">通过Spark Streaming作业处理Kafka数据</a></li></ol></li><li><ol start="14"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484537&idx=1&sn=e0319f5942e6768cf17e0e21ad669ead&chksm=cef379f8f984f0eea9a5137be8460ea39d6e77f5dcacc81e60ac7c974e55efabf5f3eb057b04&scene=21#wechat_redirect" target="_blank" rel="noopener" title="通过Kafka Connect进行数据迁移">通过Kafka Connect进行数据迁移</a></li></ol></li><li><ol start="15"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483695&idx=1&sn=4e66800691d381c2fe26d9574b470207&chksm=cef37caef984f5b8f02201ffb3522557c44991bc20b6b868e1b82355623bf3a2dc8c009196ca&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark内置图像数据源初探">Spark内置图像数据源初探</a></li></ol></li><li><ol start="16"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483744&idx=1&sn=d4a5ebefd352762d6bc4cbd60841a9aa&chksm=cef37ce1f984f5f785ad59165446e2164e4993d564b757fb18375ab8cfa0e9f9a51ca9ee9730&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】Spark-Alchemy：HyperLogLog的使用介绍">【译】Spark-Alchemy：HyperLogLog的使用介绍</a></li></ol></li><li><ol start="17"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483754&idx=1&sn=681659dc5e273bde4ea2cedb97753d76&chksm=cef37cebf984f5fdbd636fc296e7b6be6363881305e629ecb8078b70643cd646ea1d0f38b393&scene=21#wechat_redirect" target="_blank" rel="noopener" title="EMR Spark Runtime Filter性能优化">EMR Spark Runtime Filter性能优化</a></li></ol></li><li><ol start="18"><li><a href="http://baidu.com" target="_blank" rel="noopener" title="EMR Spark Relational Cache如何支持雪花模型中的关联匹配">EMR Spark Relational Cache如何支持雪花模型中的关联匹配</a></li></ol></li><li><ol start="19"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483662&idx=1&sn=de1f7670d3a502b18db675c1047fa057&chksm=cef37c8ff984f59984e52713b5781857ddbb44bdb1a266e724ed661ca859616e554aafc76ebc&scene=21#wechat_redirect" target="_blank" rel="noopener" title="EMR Spark Relational Cache的执行计划重写">EMR Spark Relational Cache的执行计划重写</a></li></ol></li><li><ol start="20"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483722&idx=1&sn=bda51fb092e1ccdd4a8bfc9f8fd5e046&chksm=cef37ccbf984f5ddac3d8ba638abdcd315c8c617ab46cb9861172883597cd41ea03eebac6d2b&scene=21#wechat_redirect" target="_blank" rel="noopener" title="EMR Spark Relational Cache 利用数据预组织加速查询">EMR Spark Relational Cache 利用数据预组织加速查询</a></li></ol></li></ul><hr><ul><li><ol start="21"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483815&idx=1&sn=d58da251890d9ed6cf914cdee478ad00&chksm=cef37c26f984f5302abff1ae5ae01ae947b6105988f74a586134b427c76431d3a63d77f20e4a&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用Relational Cache加速EMR Spark数据分析">使用Relational Cache加速EMR Spark数据分析</a></li></ol></li><li><ol start="22"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483821&idx=1&sn=975cfdca7315202d3ca9af717edd6d03&chksm=cef37c2cf984f53a560bf6050879d18641267117702dddaab62cc2510150d4118a37366088b3&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用EMR Spark Relational Cache跨集群同步数据">使用EMR Spark Relational Cache跨集群同步数据</a></li></ol></li><li><ol start="23"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484191&idx=1&sn=079fd10f5cf2a67059dffbb64fe269ba&chksm=cef37e9ef984f7883572869eb7b4096295d52fb7ceac092f3fb692292bec293dae7fbc1b426d&scene=21#wechat_redirect" target="_blank" rel="noopener" title="2019杭州云栖大会回顾之Spark Relational Cache实现亚秒级响应的交互式分析">2019杭州云栖大会回顾之Spark Relational Cache实现亚秒级响应的交互式分析</a></li></ol></li><li><ol start="24"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483766&idx=1&sn=0c11d9c3c56193837dbb683ddf41168f&chksm=cef37cf7f984f5e19a567ba1a86199ae0780a33448e3711f2a3a08beee5d24224f3d9a69cffa&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】数据湖正在成为新的数据仓库">【译】数据湖正在成为新的数据仓库</a></li></ol></li><li><ol start="25"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484135&idx=1&sn=8f2f44846a9e2c4e99b9e51cb9456b1f&chksm=cef37f66f984f670875bfe17fce1e3ca8f8255a09e3a0c9bbd102e8231646a0df1cfcdb4efb2&scene=21#wechat_redirect" target="_blank" rel="noopener" title="深入剖析 Delta Lake：详解事务日志">深入剖析 Delta Lake：详解事务日志</a></li></ol></li><li><ol start="26"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484422&idx=1&sn=04c6d12dbba9f7a3a54cdd040a55e1e9&chksm=cef37987f984f091e2506235eeba2e4f846845c2dda8e0e59f3d1733588b2d8dac727c8b5809&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Delta元数据解析">Delta元数据解析</a></li></ol></li><li><ol start="27"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484289&idx=1&sn=c0fba9596c7d92b30efffdf602d81c41&chksm=cef37e00f984f7164c83530a361caca2d80ddf98cd4be5dbfc0de7c0bc99b5808acbab5d73d1&scene=21#wechat_redirect" target="_blank" rel="noopener" title="开源生态的新发展：Apache Spark 3.0、Koala和Delta Lake">开源生态的新发展：Apache Spark 3.0、Koala和Delta Lake</a></li></ol></li><li><ol start="28"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484239&idx=1&sn=e829e3c4ec5b153512e00f501f7897cd&chksm=cef37ecef984f7d83a76d8d2e839d8f13a0962bd3082f1a43e869034362c567bdfb907bd9554&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】Delta Lake 0.4.0 新特性演示：使用 Python API 就地转换与处理 Delta Lake 表">【译】Delta Lake 0.4.0 新特性演示：使用 Python API 就地转换与处理 Delta Lake 表</a></li></ol></li><li><ol start="29"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483772&idx=1&sn=3df58d14be76f5e6a5c3be347a6b0c0f&chksm=cef37cfdf984f5eba135fa1803110621524980ce262f6bcb7c845b2d578638f2ef2fa3e2d527&scene=21#wechat_redirect" target="_blank" rel="noopener" title="漫谈分布式计算框架">漫谈分布式计算框架</a></li></ol></li><li><ol start="30"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484089&idx=1&sn=33bc88b4c742a5db2457a546abc242fc&chksm=cef37f38f984f62ed6097cc51dacd79853c791de59e1b15530fd6e48d7b2f131ffae59399cec&scene=21#wechat_redirect" target="_blank" rel="noopener" title="分布式快照算法: Chandy-Lamport">分布式快照算法: Chandy-Lamport</a></li></ol></li></ul><hr><ul><li><ol start="31"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484386&idx=1&sn=8f3b221eac11e7ffd5b17c0f763e3f53&chksm=cef37e63f984f77596a930607c2abd4e904cfd8ebaca7132a3a4ad5918ffa6e67210a279c8ed&scene=21#wechat_redirect" target="_blank" rel="noopener" title="海量小文件的的根源">海量小文件的的根源</a></li></ol></li><li><ol start="32"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484436&idx=1&sn=8d98e389861f17f9aadaede61a7317de&chksm=cef37995f984f0831ca93c8220dca5bfa896f5a60f1313220deb56e14c33a049241619adcac1&scene=21#wechat_redirect" target="_blank" rel="noopener" title="是时候改变你数仓的增量同步方案了">是时候改变你数仓的增量同步方案了</a></li></ol></li><li><ol start="33"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483782&idx=1&sn=07208d07cc965deb9f1cea3f8835e12a&chksm=cef37c07f984f51155fbbb6b59f265b9a14bf728e9e79c84f715ad165c3f578a357b7adb7269&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】Spark NLP使用入门">【译】Spark NLP使用入门</a></li></ol></li><li><ol start="34"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483794&idx=1&sn=47a825732ac760727a74820c3f699d70&chksm=cef37c13f984f505359b507edbed95bf4910048ac2693f1b1c3c7d16c243aabe7b5b5aaaea72&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】使用Spark SQL 运行大规模基因组工作流">【译】使用Spark SQL 运行大规模基因组工作流</a></li></ol></li><li><ol start="35"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483808&idx=1&sn=753003e5b7935abfb4ee297a20ff15d3&chksm=cef37c21f984f53735c14836f5c081a4a3a0bbb8c630476aa193b281aa0311ee78c91bdaf5bd&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】用SQL统一所有：一种有效的、语法惯用的流和表管理方法">【译】用SQL统一所有：一种有效的、语法惯用的流和表管理方法</a></li></ol></li><li><ol start="36"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483828&idx=1&sn=509db7520dd7f4de308e73e3ee696a03&chksm=cef37c35f984f523383a369725a8e047e0d0f7f9c7eb6306766fab92c0451c2f0f427950519b&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用Apache Arrow助力PySpark数据处理">使用Apache Arrow助力PySpark数据处理</a></li></ol></li><li><ol start="37"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483849&idx=1&sn=45a14c21cb7eba6b962761844495207c&chksm=cef37c48f984f55ee32ac526fade0b80804869b9fa891c9bbd4af295543d981563c50cea6a2a&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark on Kubernetes原生支持浅析">Spark on Kubernetes原生支持浅析</a></li></ol></li><li><ol start="38"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483989&idx=1&sn=86cdf3234cdf4975f33ab2de2472f1af&chksm=cef37fd4f984f6c2b5ee059bd859658182f70b5653f15accec53e780d8b7ef6eb3a760187dd3&scene=21#wechat_redirect" target="_blank" rel="noopener" title="列式存储系列（一）C-Store">列式存储系列（一）C-Store</a></li></ol></li><li><ol start="39"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484050&idx=1&sn=c2a62ea715e40d69d8e24a3c93a3ee2f&chksm=cef37f13f984f60599e308323902fd74bbfb60f8a0d2ae51f7f9e349ab509061465f48613509&scene=21#wechat_redirect" target="_blank" rel="noopener" title="列式存储系列（二）: Vertica">列式存储系列（二）: Vertica</a></li></ol></li><li><ol start="40"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483916&idx=1&sn=5edb12e5c4c4ca85338a17a1d87fae19&chksm=cef37f8df984f69bc249700d592f55a64e3e3aa6328fe8bec7cbbab10f6de15c68312a6b8bcc&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark on Kubernetes 的现状与挑战">Spark on Kubernetes 的现状与挑战</a></li></ol></li></ul><hr><ul><li><ol start="41"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483855&idx=1&sn=af287d5cd33baab52b947d69d97af72a&chksm=cef37c4ef984f5580b8602b47413f7a80e684c80e84c14b389c1cbf33e9c3a2c3198c23d5f8f&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Koalas：让 pandas 轻松切换 Apache Spark">Koalas：让 pandas 轻松切换 Apache Spark</a></li></ol></li><li><ol start="42"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483976&idx=1&sn=aa39393aecdafdd1453fb632c938b895&chksm=cef37fc9f984f6dffe6fd1f2451226918242a01ae7d2e307de0bea71bfbcfab4875cfc577596&scene=21#wechat_redirect" target="_blank" rel="noopener" title="使用spark-redis组件访问云数据库Redis">使用spark-redis组件访问云数据库Redis</a></li></ol></li><li><ol start="43"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484065&idx=1&sn=44f5e82b7505b85d9326ca80f8e876ce&chksm=cef37f20f984f6362a7d839d50bbe1a475693ec197d92b7e1a9c4572e44b03d804953efd364f&scene=21#wechat_redirect" target="_blank" rel="noopener" title="玩转阿里云EMR三部曲-高级篇 交互式查询及统一数据源">玩转阿里云EMR三部曲-高级篇 交互式查询及统一数据源</a></li></ol></li><li><ol start="44"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484079&idx=1&sn=dc83ea5ef255a5f6e7042b276ad26572&chksm=cef37f2ef984f63896367dff68264abfe296c0782b52b8ea84c12582c5ea9d1f41f2a19600d9&scene=21#wechat_redirect" target="_blank" rel="noopener" title="HIVE优化浅谈">HIVE优化浅谈</a></li></ol></li><li><ol start="45"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247483971&idx=1&sn=38cbea5fac93d6c63145de1dba8379d8&chksm=cef37fc2f984f6d41809bfa99c7f624adea2f67f7f72b6778f3d0f7cec1886944a5aa48b4e2c&scene=21#wechat_redirect" target="_blank" rel="noopener" title="HIVE TopN shuffle 原理">HIVE TopN shuffle 原理</a></li></ol></li><li><ol start="46"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484125&idx=1&sn=6ae156fd32cd99b99f3e2478baf29bf0&chksm=cef37f5cf984f64a554ce6ad9a2f764f2be52efd12b12a9c13f907e482550ef6bbe7a92495c3&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Kerberos使用OpenLDAP作为backend">Kerberos使用OpenLDAP作为backend</a></li></ol></li><li><ol start="47"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484140&idx=1&sn=b95fbe4b5d129e1db0a56bbdb576a19b&chksm=cef37f6df984f67b83ac3982b984a267698b0e98332f30887bf76b8fc729650fab86e0481af3&scene=21#wechat_redirect" target="_blank" rel="noopener" title="在 Apache Spark 中利用 HyperLogLog 函数实现高级分析">在 Apache Spark 中利用 HyperLogLog 函数实现高级分析</a></li></ol></li><li><ol start="48"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484145&idx=1&sn=3556e991ee6a78e6cf7ccede81031c8b&chksm=cef37f70f984f666b9c53bed558ba41154839d3cd93247d568bb0f4b2f9ed6b458138efa2664&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】Hadoop发生了什么？我们该如何做？">【译】Hadoop发生了什么？我们该如何做？</a></li></ol></li><li><ol start="49"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484159&idx=1&sn=640e5c5d0740ba6383eaec9279dc9391&chksm=cef37f7ef984f668d317d4a5616adcf288b8982832c9ca0d4d46f2ac6c1eb09c001642f2a04e&scene=21#wechat_redirect" target="_blank" rel="noopener" title="实时 OLAP 系统 Druid">实时 OLAP 系统 Druid</a></li></ol></li><li><ol start="50"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484371&idx=1&sn=f03697a4d2e4c3a34de458338b529a0a&chksm=cef37e52f984f744f8555f630f16772312bcbed08b77b329d5d1eeec7847a6ecc8aec2d64c07&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark Operator浅析">Spark Operator浅析</a></li></ol></li></ul><hr><ul><li><ol start="51"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484381&idx=1&sn=43faa5a979811eb520fac6f3bf1ece91&chksm=cef37e5cf984f74ad4cfbab984fc294108e4aa32c73dc6f15bb1af2e4ae98f995018b727ad60&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark Codegen浅析">Spark Codegen浅析</a></li></ol></li><li><ol start="52"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484475&idx=1&sn=ed1e96a9588276ca19c8cfe4418d2ed9&chksm=cef379baf984f0acbeb0ae1449a8c99bb439dad677624a5c77bd94c76b468e0fd5fdf723d837&scene=21#wechat_redirect" target="_blank" rel="noopener" title="深入分析Spark UDF的性能">深入分析Spark UDF的性能</a></li></ol></li><li><ol start="53"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484485&idx=1&sn=89751ffb18da71e67fd6bafd1daf5c81&chksm=cef379c4f984f0d29ad2a5ee7f542d3e7b207a91ed2e830574406081abab426e8f822f3b3bd4&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Spark整合Ray思路漫谈">Spark整合Ray思路漫谈</a></li></ol></li><li><ol start="54"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484460&idx=1&sn=91d559469766528d1dbb92b9b839a03d&chksm=cef379adf984f0bbda673769f54680da311cc04cbbb0f7abec647bd2abcb062068f1331966fc&scene=21#wechat_redirect" target="_blank" rel="noopener" title="Tablestore结合Spark的流批一体SQL实战">Tablestore结合Spark的流批一体SQL实战</a></li></ol></li><li><ol start="55"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484315&idx=1&sn=ff545922c7e6fe1d0d82d1d1c6259e10&chksm=cef37e1af984f70c966742aaac1ef8fe75684068c966323513c27f4f3e9e3d3bb2803197235d&scene=21#wechat_redirect" target="_blank" rel="noopener" title="助力云上开源生态 - 阿里云开源大数据平台的发展">助力云上开源生态 - 阿里云开源大数据平台的发展</a></li></ol></li><li><ol start="56"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484198&idx=1&sn=9f164807c71aa97a6a7aaf09c2096978&chksm=cef37ea7f984f7b1b4dddeef3ce29428e20d11f5510582120de547e13de97ebdefa6a11fcac9&scene=21#wechat_redirect" target="_blank" rel="noopener" title="JindoFS概述：云原生的大数据计算存储分离方案">JindoFS概述：云原生的大数据计算存储分离方案</a></li></ol></li><li><ol start="57"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484219&idx=1&sn=d04e016a8ddfba3e48d6d8697fb4498e&chksm=cef37ebaf984f7ac2db3c7308bc345bf9fc61f1c9e930747c368187944fe5f76afa9b0ec3c6c&scene=21#wechat_redirect" target="_blank" rel="noopener" title="JindoFS解析 - 云上大数据高性能数据湖存储方案">JindoFS解析 - 云上大数据高性能数据湖存储方案</a></li></ol></li><li><ol start="58"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484341&idx=1&sn=0eba127af5ad2cbfbf4b1aa7b8f43743&chksm=cef37e34f984f72294d73170186961d378d83ecc2ff34c0311d738e0098fe8621aa5bfac0e60&scene=21#wechat_redirect" target="_blank" rel="noopener" title="EMR打造高效云原生数据分析引擎">EMR 打造高效云原生数据分析引擎</a></li></ol></li><li><ol start="59"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484259&idx=1&sn=479254c306d4ed3a9f3a91d6e38f5676&chksm=cef37ee2f984f7f438a5c04e09dabc89935952cde4a4deac454faa5464b20a8f8de35966304a&scene=21#wechat_redirect" target="_blank" rel="noopener" title="5分钟迅速搭建云上Lambda大数据分析架构">5分钟迅速搭建云上Lambda大数据分析架构</a></li></ol></li><li><ol start="60"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484294&idx=1&sn=002b8f59d7af075f545f696ca0495452&chksm=cef37e07f984f711c7a014aba75facd49ff79b39f19b9c772ca0d90f4eb78dbb4e1da63f13d2&scene=21#wechat_redirect" target="_blank" rel="noopener" title="如何在Spark中实现Count Distinct重聚合">如何在Spark中实现Count Distinct重聚合</a></li></ol></li><li><ol start="61"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484320&idx=1&sn=6f367d84c63082cec63f985fb0076d1a&chksm=cef37e21f984f737a3aa19a38079cc4075c3ae344db219f1c8469c758e022d2d0138cbf6344a&scene=21#wechat_redirect" target="_blank" rel="noopener" title="基于Spark和TensorFlow 的机器学习实践">基于 Spark 和 TensorFlow 的机器学习实践</a></li></ol></li><li><ol start="62"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484470&idx=1&sn=0c22ce21702fb737e8a79a56c38b2e5f&chksm=cef379b7f984f0a1100e45bc8b20519fb328cdf01b6cc8d1c004889546d6c5d6111ae29b3c6f&scene=21#wechat_redirect" target="_blank" rel="noopener" title="如何用Apache Spark和LightGBM构建机器学习模型来预测信用卡欺诈">如何用Apache Spark和LightGBM构建机器学习模型来预测信用卡欺诈</a></li></ol></li><li><ol start="63"><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjI0NjUxMA==&mid=2247484494&idx=1&sn=da104a0df150c62b76576309f735b792&chksm=cef379cff984f0d99647708fe6be8d2a82c4c36b18837c43cbe00d1189b42a7e84e8367c698d&scene=21#wechat_redirect" target="_blank" rel="noopener" title="【译】Apache Spark 数据建模之时间维度（一）">【译】Apache Spark 数据建模之时间维度（一）</a></li></ol></li></ul><h1 id="Spark社区历次直播视频"><a href="#Spark社区历次直播视频" class="headerlink" title="Spark社区历次直播视频"></a>Spark社区历次直播视频</h1><h3 id="12月11日-【实时数仓建设以及典型场景应用】"><a href="#12月11日-【实时数仓建设以及典型场景应用】" class="headerlink" title="12月11日 【实时数仓建设以及典型场景应用】"></a>12月11日 <a href="https://developer.aliyun.com/live/1758?spm=a2c6h.12873639.0.0.3cb3219eS0rUBN" target="_blank" rel="noopener">【实时数仓建设以及典型场景应用】</a></h3><blockquote><p>本次分享会介绍实时数仓的思路以及一些实践，包括SparkStreaming SQL引擎，以及对Delta/Kudu/Druid/阿里云多种存储组件的深度整合；同时会在这个基础上介绍一些典型案例应用</p></blockquote><h3 id="12月5日-【是时候改变你数仓的增量同步方案了-】"><a href="#12月5日-【是时候改变你数仓的增量同步方案了-】" class="headerlink" title="12月5日 【是时候改变你数仓的增量同步方案了 】"></a>12月5日 <a href="https://developer.aliyun.com/live/1758?spm=a2c6h.12873639.0.0.3cb3219enHW37w" target="_blank" rel="noopener">【是时候改变你数仓的增量同步方案了 】</a></h3><blockquote><p>本次分享会介绍实时数仓的思路以及一些实践，包括SparkStreaming SQL引擎，以及对Delta/Kudu/Druid/阿里云多种存储组件的深度整合；同时会在这个基础上介绍一些典型案例应用</p></blockquote><h3 id="11月28日-【Tablestore结合Spark的云上流批一体大数据架构-】"><a href="#11月28日-【Tablestore结合Spark的云上流批一体大数据架构-】" class="headerlink" title="11月28日 【Tablestore结合Spark的云上流批一体大数据架构 】"></a>11月28日 <a href="https://developer.aliyun.com/live/1716?spm=a2c6h.12873639.0.0.3cb3219enHW37w" target="_blank" rel="noopener">【Tablestore结合Spark的云上流批一体大数据架构 】</a></h3><blockquote><p>传统Lambda架构组件多运维复杂，如何使用一套存储和一套计算来实现流批架构充分享受技术红利？以Delta Lake为代表的新型数据湖方案越来越流行，传统的Lambda架构如何向数据湖架构进行扩展？以及结构化数据结合Delta Lake的最佳解决方案是什么。本次分享将会结合理论讲解和实际场景为您一一解答。</p></blockquote><h3 id="11月16日【阿里云大数据-AI技术沙龙上海站】"><a href="#11月16日【阿里云大数据-AI技术沙龙上海站】" class="headerlink" title="11月16日【阿里云大数据+AI技术沙龙上海站】"></a>11月16日【阿里云大数据+AI技术沙龙上海站】</h3><ul><li><a href="https://developer.aliyun.com/live/1712?spm=a2c6h.12873581.0.0.270f1566XWpLUS&groupCode=apachespark" target="_blank" rel="noopener">基于 Spark 打造高效云原生数据分析引擎</a><blockquote><p>由阿里巴巴 EMR 团队提交的 TPC-DS 成绩在九月份的榜单中取得了排名第一的成绩。这个成绩背后离不开 EMR 团队对 Spark 执行引擎持续不断的优化。<br>本次分享将选取一些有代表性的优化点，深入到技术细节做详细介绍，包括但不限于动态过滤、CBO增强、TopK排序等等。</p></blockquote></li><li><a href="https://developer.aliyun.com/live/1713?spm=a2c6h.12873581.0.0.270f1566XWpLUS&groupCode=apachespark" target="_blank" rel="noopener">使用分布式自动机器学习进行时间序列分析</a><blockquote><p>对于时间序列预测搭建机器学习应用的过程非常繁琐且需要大量经验。为了提供一个简单易用的时间序列预测工具，我们将自动机器学习应用于时间序列预测，将特征生成，模型选择和超参数调优等过程实现自动化。我们的工具基于Ray（UC Berkeley RISELab开源的针对高级AI 应用的分布式框架，并作为Analytics zoo（由intel开源的统一的大数据分析和人工智能平台）的一部分功能提供给用户。</p></blockquote></li><li><a href="https://developer.aliyun.com/live/1714?spm=a2c6h.12873581.0.0.270f1566XWpLUS&groupCode=apachespark" target="_blank" rel="noopener">云上大数据的存储方案设计和选择</a><blockquote><p>上云拐点已来，开源大数据上云是业界共识。如何满足在云上低成本存储海量数据的同时又实现高效率弹性计算的潜在需求？放眼业界，都有哪些成熟存储方案和选择？各自适用的存储和计算场景是什么？背后的技术关键和考虑因素都有哪些？欢迎大数据技术爱好者面对面交流和探讨！</p></blockquote></li><li><a href="https://developer.aliyun.com/live/1715?spm=a2c6h.12873581.0.0.270f1566XWpLUS&groupCode=apachespark" target="_blank" rel="noopener">从Python 到Java ，Pyboot加速大数据和AI的融合</a><blockquote><p>Python 代表机器学习生态，而以 Hadoop/Spark 为核心的开源大数据则以 Java 为主。前者拥有数不清的算法库和程序，后者承载着海量数据和大量的企业应用。除了 SQL 这个标准方式和各种五花八门的协议接口，还有没有更高效的一手数据通道，将两个生态对接起来，乃至深度融合？Pyboot 是我们在这个方向上的探索。有兴趣的同学欢迎现场观摩演示和技术交流。</p></blockquote></li></ul><h3 id="11月14日-【-Spark-on-Kubernetes-amp-YARN】"><a href="#11月14日-【-Spark-on-Kubernetes-amp-YARN】" class="headerlink" title="11月14日 【 Spark on Kubernetes &amp; YARN】"></a>11月14日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41115" target="_blank" rel="noopener">【 Spark on Kubernetes &amp; YARN】</a></h3><blockquote><p>以Kubernetes为代表的云原生技术越来越流行起来，spark是如何跑在Kubernetes之上来享受云原生技术的红利？<br>Spark跑在Kubernetes之上和跑在Hadoop YARN上又有什么区别？以及Kubernetes 和YARN的差异点是什么。</p></blockquote><h3 id="10月17日-【Tablestore-Spark-Streaming-Connector-–-海量结构化数据的实时计算和处理-】"><a href="#10月17日-【Tablestore-Spark-Streaming-Connector-–-海量结构化数据的实时计算和处理-】" class="headerlink" title="10月17日 【Tablestore Spark Streaming Connector – 海量结构化数据的实时计算和处理 】"></a>10月17日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41103" target="_blank" rel="noopener">【Tablestore Spark Streaming Connector – 海量结构化数据的实时计算和处理 】</a></h3><blockquote><p>简介： Tablestore是阿里云自研的云原生结构化大数据存储服务，本议题会详细介绍如何基于Tablestore的CDC技术，将大表内实时数据更新对接Spark Streaming来实现数据的实时计算和处理。最新版本的Connector会随着EMR下个版本的SDK一起开源，场景环节会结合阿里内部的业务介绍用户如何结合Tablestore和Spark来实现实时数据处理。</p></blockquote><h3 id="9月26日-【New-Developments-in-the-Open-Source-Ecosystem-Apache-Spark-3-0-and-Koalas】"><a href="#9月26日-【New-Developments-in-the-Open-Source-Ecosystem-Apache-Spark-3-0-and-Koalas】" class="headerlink" title="9月26日 【New Developments in the Open Source Ecosystem: Apache Spark 3.0 and Koalas】"></a>9月26日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41096" target="_blank" rel="noopener">【New Developments in the Open Source Ecosystem: Apache Spark 3.0 and Koalas】</a></h3><blockquote><p>Apache Spark 3.0 and Koalas的最新进展</p></blockquote><h3 id="9月27日-【助力云上开源生态-阿里云开源大数据平台的发展】"><a href="#9月27日-【助力云上开源生态-阿里云开源大数据平台的发展】" class="headerlink" title="9月27日 【助力云上开源生态 - 阿里云开源大数据平台的发展】"></a>9月27日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41097" target="_blank" rel="noopener">【助力云上开源生态 - 阿里云开源大数据平台的发展】</a></h3><blockquote><p>介绍阿里云上开源生态的发展，阿里云如何更好的支持和融合开源生态，以及未来的发展。</p></blockquote><h3 id="9月27日-【EMR打造高效云原生数据分析引擎】"><a href="#9月27日-【EMR打造高效云原生数据分析引擎】" class="headerlink" title="9月27日 【EMR打造高效云原生数据分析引擎】"></a>9月27日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41098" target="_blank" rel="noopener">【EMR打造高效云原生数据分析引擎】</a></h3><blockquote><p>MR-Jindo 是 EMR 推出的云原生 OLAP 引擎。凭借该引擎，EMR 成为第一个云上 TPC-DS 成绩提交者。经过持续不断地内核优化，目前基于最新 EMR-Jindo 引擎的 TPC-DS 成绩又有了大幅提高，达到了3615071，成本降低到 0.76 CNY。本次分享将介绍 EMR-Jindo 引擎背后的相关技术以及以 EMR-Jindo 为核心的云上大数据架构方案。</p></blockquote><h3 id="9月27日-【云上大数据的一种高性能数据湖存储方案】"><a href="#9月27日-【云上大数据的一种高性能数据湖存储方案】" class="headerlink" title="9月27日 【云上大数据的一种高性能数据湖存储方案】"></a>9月27日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41099" target="_blank" rel="noopener">【云上大数据的一种高性能数据湖存储方案】</a></h3><blockquote><p>大数据上云是业界普遍共识，存储和计算分离的趋势日益显著，如何为云上蓬勃发展的大数据处理和分析引擎提供坚实的存储基础？这个 session 会主要讨论 EMR 技术团队重磅推出的一种新型混合存储解决方案，该方案基于云平台和云存储，面向新的存储硬件和计算发展趋势，为 EMR 弹性计算量身打造，在成本，弹性和性能上追求极佳平衡。技术上是如何实现的？性能如何？覆盖了哪些典型场景，最佳实践是什么？</p></blockquote><h3 id="9月27日-【基于Spark与TensorFlow的机器学习实践】"><a href="#9月27日-【基于Spark与TensorFlow的机器学习实践】" class="headerlink" title="9月27日 【基于Spark与TensorFlow的机器学习实践】"></a>9月27日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41100" target="_blank" rel="noopener">【基于Spark与TensorFlow的机器学习实践】</a></h3><blockquote><p>Apache Spark是目前最火热的计算框架，而TensorFlow是目前最火热的机器学习框架，当他们2个碰撞到一起的时候，也会产生巨大的能量。本议题会介绍EMR和PAI在这个上面的实践。</p></blockquote><h3 id="9月27日-【Spark-Relational-Cache实现亚秒级响应的交互式分析】"><a href="#9月27日-【Spark-Relational-Cache实现亚秒级响应的交互式分析】" class="headerlink" title="9月27日 【Spark Relational Cache实现亚秒级响应的交互式分析】"></a>9月27日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41101" target="_blank" rel="noopener">【Spark Relational Cache实现亚秒级响应的交互式分析】</a></h3><blockquote><p>2019杭州云栖大会大数据生态专场中的分享《Spark Relational Cache实现亚秒级响应的交互式分析》<br>Apache Spark被广泛用于超大规模的数据分析处理，在交互式分析等时间敏感的场景中，超大规模数据量的处理时间可能无法满足用户快速响应的需求。通过数据的预组织和预计算，将频繁访问的数据和计算提前执行并保存在Relational Cache中，优化后续特定模式的查询，可以显著提高查询速度，实现亚秒级的响应。本议题主要介绍Spark Relational Cache的实现原理和使用场景。</p></blockquote><h3 id="9月18日-【阿里巴巴大数据产品最新特性介绍—E-MapReduce】"><a href="#9月18日-【阿里巴巴大数据产品最新特性介绍—E-MapReduce】" class="headerlink" title="9月18日 【阿里巴巴大数据产品最新特性介绍—E-MapReduce】"></a>9月18日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41093" target="_blank" rel="noopener">【阿里巴巴大数据产品最新特性介绍—E-MapReduce】</a></h3><blockquote><p>本次直播将为您介绍E-MapReduce近期发布最新feature，涵盖集群队列管理，弹性伸缩等场景产品的使用。帮助您更快的上手云上开源大数据体系。</p></blockquote><h3 id="8月28日-【Spark-Streaming-SQL流式处理简介】"><a href="#8月28日-【Spark-Streaming-SQL流式处理简介】" class="headerlink" title="8月28日 【Spark Streaming SQL流式处理简介】"></a>8月28日 <a href="https://tianchi.aliyun.com/course/video?liveId=41084" target="_blank" rel="noopener">【Spark Streaming SQL流式处理简介】</a></h3><blockquote><p>本次直播将简要介绍EMR Spark Streaming SQL，主要包含Streaming SQL的语法和使用，最后做demo演示</p></blockquote><h3 id="8月14日-【Spark-Shuffle-优化】"><a href="#8月14日-【Spark-Shuffle-优化】" class="headerlink" title="8月14日 【Spark Shuffle 优化】"></a>8月14日 <a href="https://tianchi.aliyun.com/course/video?liveId=41076" target="_blank" rel="noopener">【Spark Shuffle 优化】</a></h3><blockquote><p>本次直播介绍EMR Spark 在shuffle方面的相关优化工作，主要包含shuffle 优化的背景以及shuffle 优化的设计方案，最后会介绍Spark shuffle 在 TPC-DS测试中的性能数据</p></blockquote><h3 id="7月31日-【Apache-Spark-在存储计算分离趋势下的数据缓存】"><a href="#7月31日-【Apache-Spark-在存储计算分离趋势下的数据缓存】" class="headerlink" title="7月31日 【Apache Spark 在存储计算分离趋势下的数据缓存】"></a>7月31日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41073" target="_blank" rel="noopener">【Apache Spark 在存储计算分离趋势下的数据缓存】</a></h3><blockquote><p>在数据上云的大背景下，存储计算分离逐渐成为了大数据处理的一大趋势，计算引擎需要通过网络读写远端的数据，很多情况下 IO 成为了整个计算任务的瓶颈，因而数据缓存成为此类场景下的一个重要的优化手段。本次分享将介绍 Spark 在数据缓存上的一些做法，并将介绍 EMR 自研的 Jindo 存储系统在数据缓存上的应用。</p></blockquote><h3 id="7月24日-【Apache-Spark-基于-Apache-Arrow-的列式存储优化】"><a href="#7月24日-【Apache-Spark-基于-Apache-Arrow-的列式存储优化】" class="headerlink" title="7月24日 【Apache Spark 基于 Apache Arrow 的列式存储优化】"></a>7月24日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41070" target="_blank" rel="noopener">【Apache Spark 基于 Apache Arrow 的列式存储优化】</a></h3><blockquote><p>Apache Arrow 是一个基于内存的列式存储标准，旨在解决数据交换和传输过程中，序列化和反序列化带来的开销。目前，Apache Spark 社区的一些重要优化都在围绕 Apache Arrow 展开，本次分享会介绍 Apache Arrow 并分析通过 Arrow 将给 Spark 带来哪些特性。</p></blockquote><h3 id="7月3日-【Koalas-介绍】"><a href="#7月3日-【Koalas-介绍】" class="headerlink" title="7月3日 【Koalas 介绍】"></a>7月3日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41058" target="_blank" rel="noopener">【Koalas 介绍】</a></h3><blockquote><p>Koalas是Spark社区推出的新项目，旨在为Spark提供与pandas完全兼容的接口，在降低pandas用户的学习和迁移成本的同时，充分利用Spark强大的分布式处理能力。本次分享介绍Koalas的基本用法和原理。</p></blockquote><h3 id="6月26日-【Spark-Relational-Cache-原理和实践】"><a href="#6月26日-【Spark-Relational-Cache-原理和实践】" class="headerlink" title="6月26日 【Spark Relational Cache 原理和实践】"></a>6月26日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41050" target="_blank" rel="noopener">【Spark Relational Cache 原理和实践】</a></h3><blockquote><p>主要介绍Relational Cache/物化视图的历史和背景，以及EMR Spark基于Relational Cache加速Spark查询的技术方案，及如何通过基于Relational Cache的数据预计算和预组织，使用Spark支持亚秒级响应的交互式分析使用场景。</p></blockquote><h3 id="6与19日-【MLFlow和spark在机器学习方面的进展、Project-Hydrogen和spark在深度学习方面的进展-】"><a href="#6与19日-【MLFlow和spark在机器学习方面的进展、Project-Hydrogen和spark在深度学习方面的进展-】" class="headerlink" title="6与19日 【MLFlow和spark在机器学习方面的进展、Project Hydrogen和spark在深度学习方面的进展 】"></a>6与19日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41008" target="_blank" rel="noopener">【MLFlow和spark在机器学习方面的进展、Project Hydrogen和spark在深度学习方面的进展 】</a></h3><blockquote><p>mlflow为企业提供一套开源的机器学习端到端工具，同时，project hydrogen项目旨在将AI框架与Spark更好的结合。本次直播介绍mlflow的场景和使用方式，project hydrogen的进展以及我们如何通过project hydrogen提供的能力更好的将Spark与AI结合。</p></blockquote><h3 id="6月6日-【Structured-Steaming的进阶与实践-】"><a href="#6月6日-【Structured-Steaming的进阶与实践-】" class="headerlink" title="6月6日 【Structured Steaming的进阶与实践 】"></a>6月6日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41051" target="_blank" rel="noopener">【Structured Steaming的进阶与实践 】</a></h3><blockquote><p>structured steaming因其低时延和提供的SQL API等特性被越来越多的企业所使用，作为实时计算的首选。<br>本次分享structured steaming的使用，包含spark 2.4 structured streaming的新特性，API原理和使用场景等的介绍。</p></blockquote><h3 id="5月29日-【Migration-to-Apache-Spark】"><a href="#5月29日-【Migration-to-Apache-Spark】" class="headerlink" title="5月29日 【Migration to Apache Spark】"></a>5月29日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41034" target="_blank" rel="noopener">【Migration to Apache Spark】</a></h3><blockquote><p>Spark因其统一引擎、性能、易用性等特点备受青睐，将大数据处理引擎迁移到Spark已经成为一种趋势(比如将Hive迁移到SparkSQL)，很多大公司也正在实践。<br>本次分享将围绕Hive迁移到SparkSQL进行展开，内容包括介绍大公司迁移流程、遇到的问题以及对Spark做的一些反馈优化。</p></blockquote><h3 id="5月23日-【基于Spark实现的MLSQL如何帮助企业构建数据中台】"><a href="#5月23日-【基于Spark实现的MLSQL如何帮助企业构建数据中台】" class="headerlink" title="5月23日 【基于Spark实现的MLSQL如何帮助企业构建数据中台】"></a>5月23日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41033" target="_blank" rel="noopener">【基于Spark实现的MLSQL如何帮助企业构建数据中台】</a></h3><blockquote><p>本次分享中，分享者会阐述他心目中的数据中台的样子，并且介绍如何基于MLSQL完成数据中台的构建。<br>此外，分享者会也会介绍MLSQL是如何基于Spark来完成这些扩展的，重要的技术点有：<br>1.如何扩展Spark SQL使其成为一个数据专用的语言MLSQL.<br>2.如何实现对各种数据源譬如HDFS/ES/MySQL/MongoDB等细化到列的权限控制。<br>3.如何构建二层RPC通讯强化对Executor的控制，实现对机器学习更好的支持。<br>4.如何支持兼容多版本Spark<br>5.如何避免机器学习中预测阶段无法复用训练时的代码和数据<br>另外，我们也会简单探讨下Databricks公司新开元项目Delta对于数据和机器学习的意义。</p></blockquote><h3 id="5月15日-【Delta-Lake：一种新型的数据湖方案】"><a href="#5月15日-【Delta-Lake：一种新型的数据湖方案】" class="headerlink" title="5月15日 【Delta Lake：一种新型的数据湖方案】"></a>5月15日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219enHW37w&liveId=41027" target="_blank" rel="noopener">【Delta Lake：一种新型的数据湖方案】</a></h3><blockquote><p>Delta Lake 是 Databricks 推出的一种新型的数据湖方案，解决了传统数据湖方案中的诸多痛点。其中的核心组件 Delta 也于近期开源。本次分享将围绕 Delta Lake 和 Delta 的诸多细节展开，如 Delta Lake 的适用场景、技术优势，Delta 的原理实现以及一些高级特性等，并就现有解决方案做横向对比。</p></blockquote><h3 id="4月29日-【Spark-AI-北美峰会参会分享】"><a href="#4月29日-【Spark-AI-北美峰会参会分享】" class="headerlink" title="4月29日 【Spark + AI 北美峰会参会分享】"></a>4月29日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg&liveId=41029" target="_blank" rel="noopener">【Spark + AI 北美峰会参会分享】</a></h3><blockquote><p>Spark + AI 北美峰会 2019 盛况依然，这两天正如火如荼。大会的主题是 Build，Unify，Scale，对此如何理解？砖厂这次有哪些重磅消息和重要发布，并作如何解读？Spark 过去几年发展的基调和线索是什么，从这次峰会上又如何看出 Spark 在未来几年的发展端倪？</p></blockquote><h3 id="1月10日-【微软Azure平台利用Intel-Analytics-Zoo构建AI客服支持实践】"><a href="#1月10日-【微软Azure平台利用Intel-Analytics-Zoo构建AI客服支持实践】" class="headerlink" title="1月10日 【微软Azure平台利用Intel Analytics Zoo构建AI客服支持实践】"></a>1月10日 <a href="https://tianchi.aliyun.com/course/video?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg&liveId=41028" target="_blank" rel="noopener">【微软Azure平台利用Intel Analytics Zoo构建AI客服支持实践】</a></h3><blockquote><p>本次分享将为大家介绍Intel的Analytics Zoo工具包，并分享微软Azure智能客服平台使用Intel Analytics Zoo的实践经验。</p></blockquote><h3 id="12月26日-【大数据列式存储之-Parquet-ORC】"><a href="#12月26日-【大数据列式存储之-Parquet-ORC】" class="headerlink" title="12月26日 【大数据列式存储之 Parquet/ORC】"></a>12月26日 <a href="https://yq.aliyun.com/live/785?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg" target="_blank" rel="noopener">【大数据列式存储之 Parquet/ORC】</a></h3><blockquote><p>Parquet 和 ORC 是大数据生态里最常用到的两个列式存储引擎，这两者在实现上有什异同，哪个效率更好，哪个性能更优，本次分享将和您一起探索两大列式存储。</p></blockquote><h3 id="12月21日-【What’s-New-in-Apache-Spark-2-4-】"><a href="#12月21日-【What’s-New-in-Apache-Spark-2-4-】" class="headerlink" title="12月21日 【What’s New in Apache Spark 2.4?】"></a>12月21日 <a href="https://yq.aliyun.com/live/775?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg" target="_blank" rel="noopener">【What’s New in Apache Spark 2.4?】</a></h3><blockquote><p>This talk will provide an overview of the major features and enhancements in Spark 2.4 release and the upcoming releases and will be followed by a Q&amp;A session.<br>The Apache Spark 2.4 comes packed with a lot of new functionalities: new barrier execution mode, flexible streaming sink, the native AVRO data source, PySpark’s eager evaluation mode, Kubernetes support, higher-order functions, Scala 2.12 support and a lot of other improvements.</p></blockquote><h3 id="12月13日-【Spark-RDD编程入门】"><a href="#12月13日-【Spark-RDD编程入门】" class="headerlink" title="12月13日 【Spark RDD编程入门】"></a>12月13日 <a href="https://yq.aliyun.com/live/720?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg" target="_blank" rel="noopener">【Spark RDD编程入门】</a></h3><blockquote><p>1.Spark、RDD简介<br>2.RDD API简介<br>3.打包与spark-submit<br>4.性能分析与调优基础</p></blockquote><h3 id="12月6日-【机器学习介绍与Spark-MLlib实践】"><a href="#12月6日-【机器学习介绍与Spark-MLlib实践】" class="headerlink" title="12月6日 【机器学习介绍与Spark MLlib实践】"></a>12月6日 <a href="https://yq.aliyun.com/live/693?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg" target="_blank" rel="noopener">【机器学习介绍与Spark MLlib实践】</a></h3><blockquote><p>本次讲座主要面对的是机器学习的入门者，以及想要使用Spark来进行机器学习的用户。我们会介绍一下机器学习相关领域的基础知识，以及机器学习在spark上面的实践，同时给出我们的一些使用建议。</p></blockquote><h3 id="11月27日-【Spark-SQL-实践与优化】"><a href="#11月27日-【Spark-SQL-实践与优化】" class="headerlink" title="11月27日 【Spark SQL 实践与优化】"></a>11月27日 <a href="https://yunqivedio.alicdn.com/od/Kf8Rb1543482700458.mp4?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg&file=Kf8Rb1543482700458.mp4" target="_blank" rel="noopener">【Spark SQL 实践与优化】</a></h3><blockquote><p>1.基本原理<br>2.支持的DataSource介绍<br>3.Hue/Zepplin/Livy周边跟SparkSQL的集成使用等<br>4.SparkSQL优化<br>5.SparkSQL Catalyst优化<br>6.AE优化<br>7.Shuffle优化</p></blockquote><h3 id="12月4日-【从-Spark-Streaming-到-Structured-Streaming"><a href="#12月4日-【从-Spark-Streaming-到-Structured-Streaming" class="headerlink" title="12月4日 【从 Spark Streaming 到 Structured Streaming"></a>12月4日 <a href="https://yq.aliyun.com/live/689?spm=a2c6h.12873639.0.0.3cb3219eoJoJgg" target="_blank" rel="noopener">【从 Spark Streaming 到 Structured Streaming</a></h3><blockquote><p>1.Spark Streaming<br>2.Google Dataow<br>3.Structured Streaming<br>4.Reference</p></blockquote><p><em>参考地址：<a href="https://developer.aliyun.com/article/718783?groupCode=aliyunemr" target="_blank" rel="noopener">https://developer.aliyun.com/article/718783?groupCode=aliyunemr</a></em><br><em>参考地址：<a href="https://mp.weixin.qq.com/s/d4FnXXCJS9SuztUDA2KvTw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/d4FnXXCJS9SuztUDA2KvTw</a></em></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据技术官方文档</title>
      <link href="/2019/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/"/>
      <url>/2019/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="cloudera"><a href="#cloudera" class="headerlink" title="cloudera"></a>cloudera</h1><ul><li>cloudera源码包下载：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a></li></ul><h1 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h1><ul><li>hadoop SingleNode文档: <a href="http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.2.1/single_node_setup.html</a></li><li>hadoop伪分布式文档: <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a></li><li>hadoop Cluster模式文档: <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></li></ul><h1 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h1><ul><li>mapreduce文档: <a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a></li></ul><h1 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h1><ul><li>伪分布式hive配置: <a href="https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/cdh_ig_hive_metastore_configure.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/cdh_ig_hive_metastore_configure.html</a></li><li>hive窗口分析函数: <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li></ul><h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><ul><li>scala安装包: <a href="https://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">https://www.scala-lang.org/download/all.html</a></li></ul><h1 id="saprk程序pom依赖"><a href="#saprk程序pom依赖" class="headerlink" title="saprk程序pom依赖"></a>saprk程序pom依赖</h1><ul><li>saprk依赖地址: <a href="https://mvnrepository.com/artifact/org.apache.spark" target="_blank" rel="noopener">https://mvnrepository.com/artifact/org.apache.spark</a></li><li>saprk案例: <a href="https://spark.apache.org/examples.html" target="_blank" rel="noopener">https://spark.apache.org/examples.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.12单台伪分布式搭建</title>
      <link href="/2019/01/05/CDH5-12%E5%8D%95%E5%8F%B0%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/01/05/CDH5-12%E5%8D%95%E5%8F%B0%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h1><blockquote><p>hadoop: 2.6.0  连接端口:9000(core-site),webUI: 50070,  yarn webUI: 18088(yarn-site), job history: 19888<br>cdh: 5.12.0<br>mysql: 5.7.11<br>hive: 1.1.0<br>scala: 2.12.0<br>spark: 2.4.4  webUI:8080  端口：7017<br>azkaban: 3.81.0 webUI: 8081<br>flink: 1.10.0 webUI: 18081</p></blockquote><h1 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h1><blockquote><p>1.安装包准备<br>2.安装JDK<br>3.安装mysql<br>4.hadoop部署<br>5.hive部署<br>6.客户端连接hive</p></blockquote><h1 id="cloudera官网下载hadoop-hive-hbase安装包"><a href="#cloudera官网下载hadoop-hive-hbase安装包" class="headerlink" title="cloudera官网下载hadoop,hive,hbase安装包"></a>cloudera官网下载hadoop,hive,hbase安装包</h1><ul><li>在官网下载以cdh5.12结尾的hadoop,hive安装包, 地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a></li><li>下载JDK, 下载mysql5.11.7,mysql连接jar包</li><li>准备dbeaver的hadoop-common，hive-jdbc-standalone的jar包</li></ul><h1 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;usr&#x2F;java</span><br><span class="line">tar -xzvf jdk-8u45-linux-x64.tar.gz -C &#x2F;usr&#x2F;java&#x2F;</span><br><span class="line">#切记必须修正所属⽤户及⽤户组</span><br><span class="line">chown -R root:root &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_45</span><br><span class="line">echo &quot;export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_45&quot; &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">echo &quot;export PATH&#x3D;$&#123;JAVA_HOME&#125;&#x2F;bin:$&#123;PATH&#125;&quot; &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">which java</span><br></pre></td></tr></table></figure><h1 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h1><ul><li>参考: <a href="https://blog.chinahufei.com/2019/12/02/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85mysql/" target="_blank" rel="noopener">https://blog.chinahufei.com/2019/12/02/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85mysql/</a></li></ul><h1 id="hadoop部署"><a href="#hadoop部署" class="headerlink" title="hadoop部署"></a>hadoop部署</h1><h3 id="解压hadoop和hive"><a href="#解压hadoop和hive" class="headerlink" title="解压hadoop和hive"></a>解压hadoop和hive</h3><h3 id="配置-bashrc"><a href="#配置-bashrc" class="headerlink" title="配置.bashrc"></a>配置.bashrc</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> .bashrc</span><br><span class="line"></span><br><span class="line"># Source global definitions</span><br><span class="line">if [ -f &#x2F;etc&#x2F;bashrc ]; then</span><br><span class="line">        . &#x2F;etc&#x2F;bashrc</span><br><span class="line">fi</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;ruoze&#x2F;app&#x2F;hadoop</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;home&#x2F;ruoze&#x2F;app&#x2F;hive</span><br><span class="line"></span><br><span class="line">export PATH&#x3D;$&#123;HADOOP_HOME&#125;&#x2F;bin:$&#123;HADOOP_HOME&#125;&#x2F;sbin:$&#123;HIVE_HOME&#125;&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><ul><li>source .bashrc</li><li>which hdfs</li><li>which hive</li></ul><h3 id="配置信任关系"><a href="#配置信任关系" class="headerlink" title="配置信任关系"></a>配置信任关系</h3><ul><li>ssh-keygen -t rsa -P ‘’ -f ~/.ssh/id_rsa</li><li>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</li><li>chmod 0600 ~/.ssh/authorized_keys</li><li>ssh hadoop001 date</li></ul><h3 id="配置文件及-NN-SNN-DN-RM-NM都以hadoop001启动"><a href="#配置文件及-NN-SNN-DN-RM-NM都以hadoop001启动" class="headerlink" title="配置文件及 NN SNN DN RM NM都以hadoop001启动"></a>配置文件及 NN SNN DN RM NM都以hadoop001启动</h3><ul><li>vi core-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;home&#x2F;hufei&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li><li>vi hdfs-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop:9868&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.https-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop:9869&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li><li>vi slaves<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br></pre></td></tr></table></figure></li><li>vi mapred-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li><li>vi yarn-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop:18088&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li><li>vi hadoop-env.sh<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_45</span><br></pre></td></tr></table></figure></li></ul><h3 id="格式化启动"><a href="#格式化启动" class="headerlink" title="格式化启动"></a>格式化启动</h3><ul><li>hdfs namenode -format</li><li>start-dfs.sh</li><li>start-yarn.sh</li><li>jps</li></ul><h3 id="打开网址"><a href="#打开网址" class="headerlink" title="打开网址"></a>打开网址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;hadoop001:50070</span><br><span class="line">http:&#x2F;&#x2F;hadoop001:18088</span><br><span class="line">云主机 就开启，防火墙端口号</span><br></pre></td></tr></table></figure><h1 id="hive部署"><a href="#hive部署" class="headerlink" title="hive部署"></a>hive部署</h1><h3 id="部署mysql及赋予权限"><a href="#部署mysql及赋予权限" class="headerlink" title="部署mysql及赋予权限"></a>部署mysql及赋予权限</h3><ul><li>su - mysqladmin</li><li>grant all privileges on <em>.</em> to hufei@’%’ identified by ‘hufei123456’;</li><li>flush privileges;</li></ul><h3 id="配置hive-site-xml"><a href="#配置hive-site-xml" class="headerlink" title="配置hive-site.xml"></a>配置hive-site.xml</h3><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;hadoop:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;the URL of the MySQL database&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;hive&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;hufei123456&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></li><li>传入mysql的驱动jar包</li></ul><h3 id="启动metastore-hiveserver2服务"><a href="#启动metastore-hiveserver2服务" class="headerlink" title="启动metastore + hiveserver2服务"></a>启动metastore + hiveserver2服务</h3><ul><li>nohup hive –service  metastore &gt; ~/logs/metastore.log 2&gt;&amp;1 &amp;</li><li>nohup  hiveserver2  &gt; ~/log/hiveserver2.log 2&gt;&amp;1 &amp;</li></ul><h3 id="测试hiveserver2服务是否ok"><a href="#测试hiveserver2服务是否ok" class="headerlink" title="测试hiveserver2服务是否ok"></a>测试hiveserver2服务是否ok</h3><ul><li>第一种：beeline<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beeline</span><br><span class="line">!connect jdbc:hive2:&#x2F;&#x2F;hadoop:10000&#x2F;default</span><br></pre></td></tr></table></figure></li><li>第二种：hive<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure></li></ul><h1 id="客户端连接hive"><a href="#客户端连接hive" class="headerlink" title="客户端连接hive"></a>客户端连接hive</h1><ul><li>下载dbeaver</li><li>点击连接，导入hadoop-common，hive-jdbc-standalone的jar包</li><li>图形化界面操作书写hive sql</li></ul><h1 id="安装scala"><a href="#安装scala" class="headerlink" title="安装scala"></a>安装scala</h1><ul><li>切换到root用户(su)</li><li>解压scala安装包到/usr/local<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf scala-2.12.0.tgz -C &#x2F;usr&#x2F;local&#x2F;</span><br></pre></td></tr></table></figure></li><li>添加环境变量 vim /etc/profile<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Scala env</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;scala-2.12.0</span><br><span class="line">export PATH&#x3D;$SCALA_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure></li><li>配置立即生效(source /etc/profile)</li></ul><h1 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h1><ul><li>切换到hufei用户(su - hufei)</li><li>解压安装包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-2.4.4-bin-hadoop2.6.tgz -C ~&#x2F;app&#x2F;</span><br></pre></td></tr></table></figure></li><li>创建软连接<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~&#x2F;app&#x2F;</span><br><span class="line">ln -s spark-2.4.4-bin-hadoop2.6 spark</span><br></pre></td></tr></table></figure></li><li>配置spark<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd spark-2.4.4-bin-hadoop2.6&#x2F;conf</span><br><span class="line">cp spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_45</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;scala-2.12.0</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;hufei&#x2F;app&#x2F;hadoop</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;home&#x2F;hufei&#x2F;app&#x2F;hadoop&#x2F;etc&#x2F;hadoop</span><br><span class="line">export SPARK_MASTER_IP&#x3D;hadoop002</span><br><span class="line">export SPARK_MASTER_PORT&#x3D;7077</span><br></pre></td></tr></table></figure></li><li>配置环境变量(vim ~/.bashrc)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#SPARK_HOME</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;apps&#x2F;spark</span><br><span class="line">export PATH&#x3D;$PATH:$SPARK_HOME&#x2F;bin</span><br></pre></td></tr></table></figure></li><li>立即生效(source ~/.bashrc)</li><li>启动spark<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~&#x2F;apps&#x2F;spark&#x2F;sbin&#x2F;start-all.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure></li><li>访问8080端口</li></ul><h1 id="安装azakaban"><a href="#安装azakaban" class="headerlink" title="安装azakaban"></a>安装azakaban</h1><ul><li>下载安装包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;azkaban&#x2F;azkaban&#x2F;releases</span><br></pre></td></tr></table></figure></li><li>解压<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf 3.81.0.tar.gz</span><br></pre></td></tr></table></figure></li><li>手动配置gradle-4.6-all.zip<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp gradle-4.6-all.zip azkaban-3.81.0&#x2F;gradle&#x2F;wrapper&#x2F;</span><br><span class="line">cd azkaban-3.81.0&#x2F;gradle&#x2F;wrapper&#x2F;</span><br><span class="line">vi gradle-wrapper.properties</span><br><span class="line"></span><br><span class="line">#distributionUrl&#x3D;https\:&#x2F;&#x2F;services.gradle.org&#x2F;distributions&#x2F;gradle-4.6-all.zip</span><br><span class="line">distributionUrl&#x3D;gradle-4.6-all.zip</span><br></pre></td></tr></table></figure></li><li>在根目录下执行编译<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;gradlew build -x test</span><br></pre></td></tr></table></figure></li><li>安装solo-server<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz -C  ~&#x2F;app&#x2F;</span><br><span class="line">cd ~&#x2F;app&#x2F;azkaban-solo-server-0.1.0-SNAPSHOT</span><br><span class="line">bin&#x2F;start-solo.sh  #!!!必须在根目录下执行，默认端口是8081</span><br></pre></td></tr></table></figure></li><li>打开webUI,修改配置在conf中</li></ul><h1 id="安装sqoop"><a href="#安装sqoop" class="headerlink" title="安装sqoop"></a>安装sqoop</h1><ul><li>下载安装包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F;sqoop-1.4.6-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure></li><li>解压<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf sqoop-1.4.6-cdh5.16.2.tar.gz -C ~&#x2F;app&#x2F;</span><br></pre></td></tr></table></figure></li><li>修改conf/sqoop-env.sh文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME&#x3D;&#x2F;home&#x2F;hufei&#x2F;app&#x2F;hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;&#x2F;home&#x2F;hufei&#x2F;app&#x2F;hadoop</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;home&#x2F;hufei&#x2F;app&#x2F;hive</span><br><span class="line">export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper-3.4.10</span><br><span class="line">export ZOOCFGDIR&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper-3.4.10</span><br><span class="line">export HBASE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hbase</span><br></pre></td></tr></table></figure></li><li>拷贝jdbc驱动和java-json的jar包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-5.1.27-bin.jar ~&#x2F;app&#x2F;sqoop&#x2F;lib</span><br><span class="line">cp java-json.jar ~&#x2F;app&#x2F;sqoop&#x2F;lib</span><br></pre></td></tr></table></figure></li><li>启动sqoop</li></ul><h1 id="安装Flink"><a href="#安装Flink" class="headerlink" title="安装Flink"></a>安装Flink</h1><ul><li>下载安装包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;mirror.bit.edu.cn&#x2F;apache&#x2F;flink&#x2F;flink-1.10.0&#x2F;flink-1.10.0-bin-scala_2.12.tgz</span><br></pre></td></tr></table></figure></li><li>解压<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf flink-1.10.0-bin-scala_2.12.tar.gz -C ~&#x2F;app&#x2F;</span><br></pre></td></tr></table></figure></li><li>修改flink-conf.yaml文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: hadoop</span><br><span class="line">rest.port: 18081   # 防止冲突和挖矿</span><br><span class="line">taskmanager.numberOfTaskSlots: 4</span><br></pre></td></tr></table></figure></li><li>启动<br>start-cluster.sh</li></ul><h1 id="安装hue"><a href="#安装hue" class="headerlink" title="安装hue"></a>安装hue</h1><ul><li><p>下载安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F;hue-3.9.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>安装依赖包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi gcc gcc-c++ krb5-devel libtidy libxml2-devel libxslt-devel openldap-devel python-devel sqlite-devel openssl-devel mysql-devel gmp-devel</span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~&#x2F;app&#x2F;hue-3.9.0-cdh5.16.2</span><br><span class="line">make apps</span><br></pre></td></tr></table></figure></li><li><p>Mysql创建hue用户，赋予权限</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user 'hue' identified by 'hue';</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* to 'hue'@'%' IDENTIFIED BY 'hue' WITH GRANT OPTION;   //将权限授予host为%即所有主机的hue用户</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* to 'hue'@'hadoop002' IDENTIFIED BY 'hue' WITH GRANT OPTION;  //将权限授予host为master的hue用户</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* to 'hue'@'localhost' IDENTIFIED BY 'hue' WITH GRANT OPTION; //将权限授予host为localhost的hue用户（其实这一步可以不配）</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* to 'hue'@'bigdatamaster' IDENTIFIED BY 'hue' WITH GRANT OPTION;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure></li><li><p>初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  ~&#x2F;app&#x2F;hue-3.9.0-cdh5.5.4&#x2F;build&#x2F;env</span><br><span class="line">bin&#x2F;hue syncdb</span><br><span class="line">bin&#x2F;hue migrate</span><br></pre></td></tr></table></figure></li><li><p>基本配置<br>vi desktop/conf/hue.ini</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[desktop]</span><br><span class="line">  # 安全秘钥，存储session的加密处理 30-60位</span><br><span class="line">  secret_key&#x3D;dfsahjfhflsajdhfljahl</span><br><span class="line">  # Time zone name</span><br><span class="line">  time_zone&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">  # Enable or disable debug mode.</span><br><span class="line">  django_debug_mode&#x3D;false</span><br><span class="line">  # Enable or disable backtrace for server error</span><br><span class="line">  http_500_debug_mode&#x3D;false</span><br><span class="line">  # This should be the hadoop cluster admin</span><br><span class="line">  ## default_hdfs_superuser&#x3D;hdfs</span><br><span class="line">  default_hdfs_superuser&#x3D;root</span><br><span class="line">  # 不启用的模块</span><br><span class="line">           </span><br><span class="line">  #app_blacklist&#x3D;impala,security,rdbms,jobsub,pig,hbase,sqoop,zookeeper,metastore,indexer</span><br><span class="line"> </span><br><span class="line">[[database]]</span><br><span class="line">  # 数据库引擎类型</span><br><span class="line">  engine&#x3D;mysql</span><br><span class="line">  # 数据库主机地址</span><br><span class="line">  host&#x3D;hadoop002</span><br><span class="line">  # 数据库端口</span><br><span class="line">  port&#x3D;3306</span><br><span class="line">  # 数据库用户名</span><br><span class="line">  user&#x3D;hue</span><br><span class="line">  # 数据库密码</span><br><span class="line">  password&#x3D;hue</span><br><span class="line">  # 数据库库名</span><br><span class="line">  name&#x3D;hue</span><br></pre></td></tr></table></figure></li><li><p>启动hue</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">build&#x2F;env&#x2F;bin&#x2F;supervisor</span><br><span class="line">用户名和密码，在数据库查，默认登陆端口8888</span><br><span class="line">select id, password, username from auth_user;</span><br><span class="line">update auth_user set password &#x3D; md5(&quot;123456&quot;) where username &#x3D; &#39;y&#39;;</span><br></pre></td></tr></table></figure></li><li><p>HUE整合hadoop<br>配置hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>HUE整合MapReduce/Yarn<br>开启JobHistoryServer，配置mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreudce.jobhistory.intermediate.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/history/done/done_intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mr-jobhistory-daemon.sh start historyserver</p></li><li><p>HUE 整合MySQL<br>搜索librdbms 引擎改为mysql，填入host,port,username,password，然后重启</p></li><li><p>HUE整合Hive<br>搜索beeswax, 修改相应的参数<br>hive_server_host,hive_server_port=10000,hive_conf_dir<br>一定要先启动hiveserver2</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
